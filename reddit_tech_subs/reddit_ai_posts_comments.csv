,source,subreddit,id,created_utc,text,score,num_comments
0,post,MachineLearning,1of17nx,1761320666.0,"[D] What‚Äôs an AI tool you wanted to love but it keeps missing the mark?
There‚Äôs this weird gap between AI tools that wow you for a day and those that actually stay in your workflow.

I‚Äôm curious what‚Äôs an AI powered app or plugin that you have tried but ended up being more frustrating than helpful?

Was it accuracy , lack of control, bad UX, too generic or something else?

Trying to understand what ‚ÄúAI done wrong‚Äù looks like for real creators and engineers.",0,6.0
1,post,MachineLearning,1oez1oi,1761315664.0,"[P] I built an app that converts any text into high-quality audio. It works with PDFs, blog posts, Substack and Medium links, and even photos of text.
I‚Äôm excited to share a project I‚Äôve been working on over the past few months!

It‚Äôs a mobile app that turns any text into high-quality audio. Whether it‚Äôs a webpage, a Substack or Medium article, a PDF, or just copied text‚Äîit converts it into clear, natural-sounding speech. You can listen to it like a podcast or audiobook, even with the app running in the background.

The app is privacy-friendly and doesn‚Äôt request any permissions by default. It only asks for access if you choose to share files from your device for audio conversion.

You can also take or upload a photo of any text, and the app will extract and read it aloud.

Thanks for your support, I‚Äôd love to hear what you think!

[Free iPhone app](https://apps.apple.com/us/app/frateca-text-to-speech-audio/id6741859465),

[Free Android app on Google Play](https://play.google.com/store/apps/details?id=ai.texttospeech.app)",45,3.0
2,post,MachineLearning,1oemepr,1761273529.0,"[R] Signal Processing for AI ‚Äî A New Way to Think About LLMs and ANN Search
We have been exploring how signal processing principles, traditionally used in communication systems to extract meaningful information from noisy data, can be applied to AI models and embedding spaces to make them more efficient and accurate.

We're presenting this work in collaboration with **Prof. Gunnar Carlsson** (Stanford Mathematics Emeritus, pioneer in topological data analysis), showing how signal processing can complement modern AI architectures.


üìç **Event details:** https://luma.com/rzscj8q6

&nbsp;

As a first application to ANN search, we achieved 10x faster vector search than current solutions. If vector databases interest you, here's the technical note and video:

[**Traversal is Killing Vector Search ‚Äî How Signal Processing is the Future**](https://www.linkedin.com/pulse/traversal-killing-vector-search-how-signal-processing-yusuf-motiwala-s4tic/)

&nbsp;

If this interests you and you are in the Bay Area, we'd love to have you join the event and discuss how signal processing could shape the next wave of AI systems. We had some great discussions at PyTorch Conference over the last two days.

We'll also be at **TechCrunch Disrupt 2025** if you'd like to meet and brainstorm there.",6,6.0
3,post,MachineLearning,1oedumd,1761250444.0,"Deepseek OCR : High Compression Focus, But Is the Core Idea New? + A Thought on LLM Context Compression[D]
**The paper highlights** its ""Contexts Optical Compression"" module, which compresses visual tokens between the vision encoder and the MoE language decoder. They show impressive results, like 97% OCR precision even with <10x compression (original vision tokens vs. compressed ones) and \~60% at 20x.  


**My take \[D\]:** The compression of visual tokens in the latent space is not a new thing it is was done in the VLMs previously. I guess back than the compression was not the main focus, in this paper the focus was on 10x compression. And this gave the AI community idea to compress the input context of LLMs by representing it in image and compressing the image in latent space which could be much more dense as compared to text where the structure is constraint by tokens as the lowest compressed form.

But can't we just compress the text tokens by training an autoencoder and using the encoder to generate the latent space lower dimensional embeddings.

**Would love to hear what others think**

**Paper link:** [**https://www.arxiv.org/pdf/2510.18234**](https://www.arxiv.org/pdf/2510.18234)",7,2.0
4,post,MachineLearning,1oe6ywk,1761234775.0,"[R] Continuous latent interpolation breaks geometric constraints in 3D generation
Working with text-to-3D models and hitting a fundamental issue that's confusing me. Interpolating between different objects in latent space produces geometrically impossible results.

Take ""wooden chair"" to ""metal beam"". The interpolated mesh has vertices that simultaneously satisfy chair curvature constraints and beam linearity constraints. Mathematically the topology is sound but physically it's nonsense.

This suggests something wrong with how these models represent 3D space. We're applying continuous diffusion processes designed for pixel grids to discrete geometric structures with hard constraints.

Is this because 3D training data lacks intermediate geometric forms? Or is forcing geometric objects through continuous latent mappings fundamentally flawed? The chair-to-beam path should arguably have zero probability mass in real space.

Testing with batch generations of 50+ models consistently reproduces this. Same interpolation paths yield same impossible geometry patterns.

This feels like the 3D equivalent of the ""half-dog half-cat"" problem in normalizing flows but I can't find papers addressing it directly.",50,15.0
5,post,MachineLearning,1odzd16,1761214072.0,"[R] Un-LOCC (Universal Lossy Optical Context Compression), Achieve Up To 3√ó context compression with 93.65% Accuracy.
**TL;DR**: I compress LLM context into **images** instead of text, and let a **vision-language model** (VLM) ‚Äúdecompress‚Äù it by reading the image. In my tests, this yields up to **\~2.8:1 token compression at 93.65% accuracy** on *Gemini 2.5-Flash-Lite (Exp 56)*, and **99.26% at 1.7:1** on *Qwen2.5-VL-72B-Instruct (Exp 34)*. Full code, experiments, and replication steps are open-source.

**Repo (please ‚≠ê if useful):** [https://github.com/MaxDevv/Un-LOCC](https://github.com/MaxDevv/Un-LOCC)

# What this is:

**Un-LOCC (Universal Lossy Optical Context Compression)**: a simple, general method to **encode long text context into compact images**, then **decode with a VLM**. Think of the VLM as an OCR-plus semantic decompressor.

* I render text into a fixed-size PNG (e.g., **324√ó324**, Atkinson Hyperlegible \~**13px**), pass that image to a VLM, and ask it to reproduce the original text.
* **Accuracy** = normalized Levenshtein similarity (%).
* **Compression ratio** = *text tokens √∑ image tokens*.

# Key results (linked to experiments in the repo):

* **Gemini 2.5-Flash-Lite**: **100% @ 1.3:1** *(Exp 46)* and **\~93.65% @ 2.8:1** *(Exp 56)*.
* **Qwen2.5-VL-72B-Instruct**: **99.26% @ 1.7:1** *(Exp 34)*; **\~75.56% @ 2.3:1** *(Exp 41)*.
* **Qwen3-VL-235B-a22b-Instruct**: **95.24% @ 2.2:1** *(Exp 50)*; **\~82.22% @ 2.8:1** *(Exp 90)*.
* **Phi-4-Multimodal**: **94.44% @ 1.1:1** *(Exps 59, 85)*; **\~73.55% @ 2.3:1** *(Exp 61)*.
* **UI-TARS-1.5-7B**: **95.24% @ 1.7:1** *(Exp 72)*; **\~79.71% @ 1.7:1** *(Exp 88)*.
* **LLaMA-4-Scout**: **86.57% @ 1.3:1** *(Exp 53)*.

>Details, prompts, fonts, and measurement code are in the README. I cite each claim with **(Exp XX)** so you can verify quickly.

# Why this matters:

* **Cheaper context**: replace expensive text tokens with ‚Äúimage tokens‚Äù when a capable VLM sits in the loop.
* **Architecturally simple**: no model modifications are needed, you can use rendering + a VLM you already have.
* **Composable**: combine with retrieval, chunking, or multimodal workflows.

# What I need help with:

* **A better algorithm:** The O-NIH algorithm is okay for checking if models can see the text, however I'm not sure how to easily determine the model's full comprehension of the text.
* **Model coverage**: more open VLMs; local runs welcome.
* **Edge cases**: math, code blocks, long tables, multilingual.
* **Repro/PRs**: if you get better ratios or accuracy, please open an issue/PR.

**Repo again (and yes, stars genuinely help discoverability):** [https://github.com/MaxDevv/Un-LOCC](https://github.com/MaxDevv/Un-LOCC)",1,4.0
6,post,MachineLearning,1oddm2g,1761151725.0,"[R] Are you working on a code-related ML research project? I want to help with your dataset
I‚Äôve been digging into how researchers build datasets for code-focused AI work ‚Äî things like program synthesis, code reasoning, SWE-bench-style evals, DPO/RLHF. It seems many still rely on manual curation or synthetic generation pipelines that lack strong quality control.

**I‚Äôm part of a small initiative supporting researchers who need custom, high-quality datasets for code-related experiments ‚Äî at no cost. Seriously, it's free.**

If you‚Äôre working on something in this space and could use help with data collection, annotation, or evaluation design, I‚Äôd be happy to share more details via DM.

Drop a comment with your research focus or current project area if you‚Äôd like to learn more ‚Äî I‚Äôd love to connect.",0,8.0
7,post,MachineLearning,1od8zpw,1761141412.0,"[N] Open AI just released Atlas browser. It's just accruing architectural debt
The web wasn't built for AI agents. It was built for humans with eyes, mice, and 25 years of muscle memory navigating dropdown menus.  
  
Most AI companies are solving this with browser automation, playwright scripts, Selenium wrappers, headless Chrome instances that click, scroll, and scrape like a human would.

It's a workaround and it's temporary.

These systems are slow, fragile, and expensive. They burn compute mimicking human behavior that AI doesn't need. They break when websites update. They get blocked by bot detection. They're architectural debt pretending to be infrastructure etc.

The real solution is to build web access designed for how AI actually works instead of teaching AI to use human interfaces.¬†

A few companies are taking this seriously. Exa or Linkup are rebuilding search from the ground up for semantic / vector-based retrieval Linkup provides structured, AI-native access to web data. Jina AI is building reader APIs for clean content extraction. Shopify in a way tried to address this by exposing its APIs for some partners (e.g., Perplexity)

The web needs an API layer, not better puppeteering.

As AI agents become the primary consumers of web content, infrastructure built on human-imitation patterns will collapse under its own complexity‚Ä¶",146,85.0
8,post,MachineLearning,1od3j63,1761125120.0,"[N] Pondering how many of the papers at AI conferences are just AI generated garbage.
[https://www.scmp.com/tech/tech-trends/article/3328966/ai-powered-fraud-chinese-paper-mills-are-mass-producing-fake-academic-research](https://www.scmp.com/tech/tech-trends/article/3328966/ai-powered-fraud-chinese-paper-mills-are-mass-producing-fake-academic-research)

A new CCTV investigation found that paper mills in mainland China are using generative AI to mass-produce forged scientific papers, with some workers reportedly ‚Äúwriting‚Äù more than 30 academic articles per week using chatbots.  
  
These operations advertise on e-commerce and social media platforms as ‚Äúacademic editing‚Äù services. Behind the scenes, they use AI to fabricate data, text, and figures, selling co-authorships and ghostwritten papers for a few hundred to several thousand dollars each.  
  
One agency processed over 40,000 orders a year, with workers forging papers far beyond their expertise. A follow-up commentary in The Beijing News noted that ‚Äúvarious AI tools now work together, some for thinking, others for searching, others for editing, expanding the scale and industrialization of paper mill fraud.‚Äù",157,49.0
9,post,MachineLearning,1od0v4o,1761114801.0,"[P] Getting purely curiosity driven agents to complete Doom E1M1
Quick context: I'm training a playable DOOM world model where you can prompt like ""spawn cyberdemon left"" or ""harder"" to change game events in real time. I wanted to take DeepMind's playable Doom world model in [Diffusion Models are Real-Time Game Engiens](https://arxiv.org/abs/2408.14837), and add text conditioning to make game events promptable.

**To train this I need \~100 hours of action-labeled DOOM gameplay data.**

I could have scraped DOOM data from YouTube, or paid contractors, but thought it would be fun to train a curious RL agent that explores the map. I thought this would be a solved problem, since I saw RL papers from 2018 about ""curiosity-driven"" learning.

I couldn't have been more wrong! Training agents to be ""curious"" is far from a solved problem. Here's what I tried and what happened so far:

**1. Implemented the original** [**curiosity-driven exploration**](https://arxiv.org/abs/1705.05363) **paper(Pathak et al., 2018) ‚Üí hit the Noisy TV Problem**

The Noisy TV Problem is where the agent gets stuck staring at a random process in the game. This is a known problem with defining the curiosity bonus as prediction error, since noise is not learnable. The specific ""Noisy TV"" the agent converges to is getting transfixed by the pistol's muzzle smoke against a high-contrast white background.

**2. Implemented** [Learning Progress Monitoring](https://arxiv.org/pdf/2509.25438v1) **(2025) ‚Üí agent converged to taking no action.**

The paper defined curiosity bonus as learning progress: difference between past prediction error of next state and current prediction error of next state. Sounds good on paper, but in practice you have to get a lot right to guarantee past prediction error > current prediction error for learnable (non-random) states. I couldn't figure this out, and past and present prediction error became roughly equal during training, causing agent to take no action due to lack of reward.

**3. Implemented OpenAI** [Random Network Distillation](https://arxiv.org/abs/1810.12894) **‚Üí agent learns  but not because of curiosity**

The agent learned, but only because of extrinsic rewards (kills, room discovery, etc), not curiosity bonus rewards. After many iterations, curiosity bonus rewards shrank to zero as well, similar to LPM. The agent acts greedily to kill enemies and discover rooms, with little to no variety in its actions.

More details here in my repo, where all three implementations work out-of-box: [https://github.com/pythonlearner1025/BoredDoomGuy](https://github.com/pythonlearner1025/BoredDoomGuy)

At this point, I reminded myself training a curious RL agent is a side quest, and I have to get back on the main quest. But if you've trained an agent to complete Doom E1M1 purely from curiosity, I'm curious to hear how you did it!

For now, I'm falling back to collecting training data from human players. You can help by playing doom in your browser at [playdoom.win](https://www.playdoom.win) your fun is my training data: your game viewport and actions will be logged!",7,2.0
10,post,MachineLearning,1od0fw8,1761113209.0,"[R] rBridge: Predicting LLM Reasoning Performance with Small Proxy Models (100√ó Compute Reduction)
We present rBridge, a method that enables small proxy models (‚â§1B parameters) to effectively predict large-model reasoning performance, addressing the emergence problem in reasoning capabilities.

**Paper:** [https://www.arxiv.org/abs/2509.21013](https://www.arxiv.org/abs/2509.21013)

**Abstract/TL;DR:** Given the prohibitive cost of pre-training large language models, leveraging smaller proxy models to optimize datasets before scaling up is essential. However, reasoning capabilities exhibit emergent behavior only at larger scales (typically >7B parameters), making traditional proxy approaches ineffective. rBridge solves this by aligning evaluation with both (1) the pre-training objective and (2) the target task through weighted negative log-likelihood using frontier model reasoning traces.

**Key Contributions:**

1. **Theoretical insight:** We identify that proxy evaluation schemes must align with both pre-training objectives and target tasks for effective reasoning prediction
2. **Novel method:** rBridge weights NLL by task-alignment using frontier model confidence scores, handling tokenizer mismatches at letter-level
3. **Empirical validation:**
   * 100.2√ó compute reduction for dataset ranking (80.8% decision accuracy across 25 datasets)
   * Strong proxy-target correlations: R¬≤ = 0.826-0.874 across 6 benchmarks (GSM8K, MATH500, ARC-C, MMLU Pro, CQA, HumanEval)
   * Zero-shot transfer of fitted functions across pre-training datasets

**Experimental Setup:**

* Proxy scales: 100M to 1B
* Target scales: 7B to 32B
* Training corpus: 250B to 3.75T tokens
* Evaluation: 5-fold cross-validation

**Practical Impact:** This enables compute-constrained researchers to explore pre-training design choices at dramatically reduced costs. A single 7B training run can exceed $50K; our method reduces exploration costs by 100√ó+ while maintaining predictive accuracy.

Code will be released soon.",13,1.0
11,post,MachineLearning,1ocyruz,1761107343.0,"[D] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation
[https://arxiv.org/abs/2402.09267](https://arxiv.org/abs/2402.09267)

Very interesting paper I found about how to make LLMS keep themselves in check when it comes to factuality and how to mitigate and reduce hallucinations without the need of human intervention.

I think this framework could contribute and give LLMs huge benefits, especially in fields where high factuality confidence and low (or ideally none) hallucinations are needed.

Summary: In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. ",11,2.0
12,post,MachineLearning,1obnz7i,1760980536.0,"GPU 101 and Triton kernels
Dear fellow ML people,

LLMs need trillions of tokens to be trained, which makes optimization and speed key of current ML pipeline. When I wrote a [GPT2 implementation from scratch](https://github.com/Bornlex/GPT2), I iteratively improved it by adding a few features such as Multi-head self attention, grouped query self attention, kv cache...

Then I asked myself : can I make training faster ?

I wrote this blog article¬†[Make GPU go brrr](https://bornlex.github.io/posts/triton1/)¬†a few days ago and would be very happy to know :

1. **How useful is it to you ?**¬†I try to write articles to compile multiple sources online so that readers get a 0 to 1 resource. It helps me clear my mind, serialize my knowledge somewhere, and hopefully land a big AI company job someday !
2. **How can I improve it ?**¬†Feel free to share feedback about the quality of the writing, if something is not clear, if the drawings are too cryptic...
3. **What topic should I focus on next ?**¬†This one is purely for me to improve even more thanks to you guys.

During this journey of writing articles, I find myself digging deeper and deeper into technical stuff, which is very exciting. This Triton part of ML is lovely and allows me to make converge 2 sides of computer science that I love : AI and low level programming. I will iterate on this with an implementation of FlashAttention.

Have a great week.

Cheers.",40,13.0
13,post,MachineLearning,1obf5bw,1760951055.0,"Minimizing Mode Collapse in CycleGAN [D]
Any steps that have worked for you in the past will work. My generator loss is around 2-3 range (with identity and cyclic components), while discriminator loss has flat lined at 0.005-0.02. Sample outputs look extremely different from what is required. After a certain epoch, I implemented 2x Gen step for each disc, higher gen loss, lowered cyclic and identity components, but 2-3 epoch later, even if the gen loss is less, there isnt any change in disc loss

[](https://www.reddit.com/submit/?post_id=t3_1obf0ky)",1,3.0
14,post,MachineLearning,1obbp7m,1760938132.0,"[P] Built a searchable gallery of ML paper plots with copy-paste replication code
Hey everyone,

I got tired of seeing interesting plots in papers and then spending 30+ minutes hunting through GitHub repos or trying to reverse-engineer the visualization code, so I built a tool to fix that.

**What it does:**

* Browse a searchable gallery of plots from ML papers (loss curves, attention maps, ablation studies, etc.)
* Click any plot to get the exact Python code that generated it
* Copy-paste the code and run it immediately - all dependencies listed
* Filter by model architecture, or visualization type and find source papers by visualization

The code snippets are self-contained and include sample data generation where needed, so you can actually run them and adapt them to your own use case using LLM agents as well.

[Be an early user :)](https://ml-builder.vercel.app/)

Right now it has \~80 plots from popular papers (attention mechanisms, transformer visualizations, RL training curves, etc.) but I'm adding more weekly. If there's a specific paper visualization you always wanted to replicate, drop it in the comments and I'll prioritize it.

Happy to answer questions about implementation or take suggestions for improvements!",45,12.0
15,post,MachineLearning,1ob5yuv,1760920503.0,"My experience deploying an ML-driven trading system [P]
Years back, after finishing my CS degree, I got into algorithmic trading as a personal project. It felt like the perfect arena to push my skills in coding, data science, and, most importantly, data engineering. After a long road of development, I recently deployed my first fully automated, ML-driven system.

The trading results aren't the point of this post. I'm here to talk about the steps I've taken to solve the fundamental problem of getting a machine learning model to perform in a live environment exactly as it did during historical testing.

A live production environment is hostile to determinism. Unlike a sterile backtest where all data is known, a live system deals with a relentless, ordered stream of events. This introduces two critical failure modes:

* **Lookahead Bias:**¬†The risk of accidentally using information from the future to make a decision in the past. A live system must be architected to be a strict ""tape reader,"" ensuring it only ever acts on information that has already occurred.
* **State Drift:**¬†A more insidious problem where the system's internal ""memory""‚Äîits representation of the world, built from the stream of incoming data‚Äîslowly but surely drifts away from the ground truth of the historical environment. The live model ends up seeing a distorted reality compared to the one it was trained on, rendering its predictions meaningless.

It's important to note that training a model on features containing lookahead bias will often¬†*cause*¬†state drift, but not all state drift is caused by lookahead bias. My entire development process was engineered to prevent both.

My first principle was to enforce a strict, row-by-row processing model for all historical data. There are countless ways lookahead bias can creep into a feature engineering pipeline, but the most tempting source I found was from trying to optimize for performance. Using vectorized pandas operations or multi-threading is standard practice, but for a stateful, sequential problem, it's a minefield. While I'm sure there are pandas wizards who can vectorize my preprocessing without causing leaks, I'm not one of them. I chose to make a deliberate trade-off: I sacrificed raw performance for provable correctness.

My solution is a ""golden master"" script that uses the¬†*exact same stateful classes*¬†the live bot will use. It feeds the entire historical dataset through these classes one row at a time, simulating a live ""tape reader."" At the end of its run, it saves the final state of every component into a single file. While this is much slower than a vectorized approach, it's the cornerstone of the system's determinism.

The live bot's startup process is now brutally simple: it loads the state file from the golden master. It doesn't build its own state; it¬†*restores*¬†it. It only has to process the short data gap between the end of the golden master's run and the current moment. This makes the live system easier to debug and guarantees a perfect, deterministic handover from the historical environment.

Finally, I have the validator. This tool also starts from the same ""golden master"" state and re-processes the exact same raw data the live bot saw during its run. The goal is a Pearson correlation of 1.0 between the live bot's predictions and the validator's predictions. Anything less than a perfect correlation indicates a logical divergence that must be found and fixed.

This project has been an incredible learning experience, but the biggest lesson was in humility. The most complex challenges weren't in model architecture but in the meticulous data engineering required to create a provably consistent bridge between the historical and the live environments.

While my actual trading models are private, I have a lower-frequency version of the system that posts market updates and predictions. After running live for over three weeks, it maintained a >0.9999 correlation with its validator - shown in the attached picture. It's currently offline for some upgrades but will be back online in a few days. You can see it here:

[https://x.com/ZtenlEssej](https://x.com/ZtenlEssej)

Thanks for reading. I have high hopes for my trading system, but it will take time. For now my skills are very much for hire. Feel free to reach out if you think I could be a fit for your project!",0,6.0
16,post,MachineLearning,1oajofr,1760859664.0,"Are MLE roles being commoditized and squeezed? Are the jobs moving to AI engineering? [D]
A couple quotes from Gemini and Claude

""While still in high demand, some of the model-specific work is becoming more democratized or abstracted by automated tools and APIs.""

""""""

The ML engineering that remains valuable:

* Research-level work at frontier labs (extremely competitive, requires PhD + exceptional talent)
* Highly specialized domains (medical imaging, robotics, etc.) where you need domain expertise + ML
* Infrastructure/systems work (distributed training, optimization, serving at scale)
* Novel applications where APIs don't exist yet

The ML engineering that's being commoditized:

* Standard computer vision tasks
* Basic NLP fine-tuning
* Hyperparameter optimization
* Model selection for common tasks
* Data preprocessing pipelines

""""""

Is the job landscape bifurcating toward: (1) research + frontier labs, (2) applying off-the-shelf models to business verticals

My background:

I left a computer vision role several years ago because I felt like it was plateauing, where all I was doing was dataset gathering and fine-tuning on new applications. It wasn't at a particularly stellar company.

I went to a more general data science & engineering type role, more forecasting and churn focused.

I'm debating whether to try to upskill and foray into AI engineering, building RAG systems.

What are y'all's thoughts? How does one go about doing that jump? Maybe the MLE roles are still stable and available, and I just need to improve.",55,44.0
17,post,MachineLearning,1oa8qu4,1760824908.0,"Numerical Analysis [D]
i have the option to take a numerical analysis class next semester, and I wanted to ask, what are some cool applications of machine learning and deep learning with numerical analysis? And what jobs combine ML and numerical analysis techniques?",9,7.0
18,post,MachineLearning,1oa7bb2,1760821399.0,"[D] What are some trendy or emerging topics in AI/ML research beyond LLMs and NLP?
Hi everyone,

I‚Äôve noticed that most discussions lately revolve around LLMs and NLP, but I‚Äôm curious about what other areas in AI/ML are currently getting attention in research.

What topics or fields do you think are becoming exciting right now?",76,53.0
19,post,MachineLearning,1o9yuxv,1760801488.0,"[P] Open-Source Implementation of ""Agentic Context Engineering"" Paper - Agents that improve by learning from their own execution feedback
We implemented Stanford's recent ""Agentic Context Engineering"" paper (https://arxiv.org/abs/2510.04618) and open-sourced it. 

Instead of fine-tuning, agents curate their own context by learning from execution feedback. Three-agent system (Generator, Reflector, Curator) builds a ""playbook"" of strategies autonomously. 

GitHub: https://github.com/kayba-ai/agentic-context-engine 

Interested in feedback from the community on the approach and implementation!",33,5.0
20,post,MachineLearning,1o9u4cx,1760789507.0,"[D] Dan Bricklin: Lessons from Building the First Killer App | Learning from Machine Learning #14
New episode of Learning from Machine Learning with Dan Bricklin, co-creator of VisiCalc, the first electronic spreadsheet that launched the personal computer revolution. His insight on breakthrough innovation: innovations must be 100 times better, not incrementally better.

His framework is simple. When evaluating if something truly matters, ask:

- What is this genuinely better at?
- What does it enable that wasn't possible before?
- What trade-offs will people accept?
- Does it pay for itself immediately?

These same questions made spreadsheets inevitable and apply directly to AI today.
But the part that really hit: Bricklin talked about the impact you never anticipate. A mother whose daughter with cerebral palsy could finally do her own homework. A couple who met learning spreadsheets. These quiet, unexpected ways the work changed lives matter more than any product launch or exit.

When we build something, we chase metrics and milestones. We rarely imagine the specific moments where what we made becomes essential to someone's life in ways we never predicted.",3,0.0
21,post,MachineLearning,1o9pnaz,1760773186.0,"[P]:  Beens-MiniMax:  103M MoE LLM from Scratch
I built and trained this very simple MoE \[ [Beens-MiniMax](https://github.com/Abinesh-Mathivanan/beens-minimax) \] from scratch in a span of 5 days. You could read more in the [report](https://github.com/Abinesh-Mathivanan/beens-minimax/blob/main/Beens_MiniMax__How_not_to_Build_an_LLM.pdf) here.",28,1.0
22,post,MachineLearning,1o8zbg5,1760702293.0,"[P] Control your house heating system with RL
Hi guys,

I just released the source code of my most recent project: a DQN network controlling the radiator power of a house to maintain a perfect temperature when occupants are home while saving energy.

I created a custom gymnasium environment for this project that relies on thermal transfer equation, so that it recreates exactly the behavior of a real house.

The action space is discrete number between 0 and max\_power.

The state space given is :

\- Temperature in the inside,

\- Temperature of the outside,

\- Radiator state,

\- Occupant presence,

\- Time of day.

I am really open to suggestion and feedback, don't hesitate to contribute to this project !

[https://github.com/mp-mech-ai/radiator-rl](https://github.com/mp-mech-ai/radiator-rl)

EDIT: I am aware that for this linear behavior a statistical model would be sufficient, however I see this project as a template for more general physical behavior that could include high non-linearity or randomness.",31,27.0
23,post,MachineLearning,1o8ve9w,1760688163.0,"[D] What ML/AI research areas are actively being pursued in industry right now?
Hi everyone,

I'm hoping to get a sense of what ML/AI fields are the focus of active research and development in the private sector today.

I currently work as a Data Scientist (finished my Ph.D. two years ago) and am looking to transition into a more research-focused role. To guide my efforts, I'm trying to understand which fields are in demand and what knowledge would make me a stronger candidate for these positions.

My background is strong in classical ML and statistics, so not much of NLP or CV, even though I did learn the basics of both at some point. While I enjoy these classical areas, my impression is that they might not be in the spotlight for *new* research roles at the moment. I would be very happy to be proven wrong!

If you work in an industry research or applied science role, I'd love to hear your perspective. What areas are you seeing the investment and hiring in? Are there any surprising or niche fields that still have demand?

Thanks in advance for your insights!",98,40.0
24,post,MachineLearning,1o8szk0,1760679035.0,"[R] Plain English outperforms JSON for LLM tool calling: +18pp accuracy, -70% variance
**TL;DR:** Tool-call accuracy in LLMs can be significantly improved by using natural language instead of JSON-defined schemas (\~+18 percentage points across 6,400 trials and 10 models), while simultaneously reducing variance by 70% and token overhead by 31%. We introduce Natural Language Tools (NLT), a simple framework that decouples tool selection from response generation and eliminates programmatic format constraints and extends tool calling to models even without tool-call support.

**Resources:** [Paper](https://arxiv.org/abs/2510.14453)

**Authors:** Reid T. Johnson, Michelle D. Pain, Jordan D. West

# The Problem

Current LLMs use structured JSON/XML for tool calling, requiring outputs like:

    {
      ""tool_calls"": [{
        ""name"": ""check_talk_to_a_human"",
        ""description"": ""Used when the user requests...""
      }]
    }

This structured approach creates three  bottlenecks:

1. **Task interference**: Models must simultaneously handle multiple tasks, such as understanding queries, select tools, maintaining format constraints, and  generating responses.
2. **Format burden**: Research demonstrates that the more structured a model's output, the more its performance tends to degrade ([a great paper by Tam on the subject](https://arxiv.org/abs/2408.02442)).
3. **Context bloat**: Structured schemas increase token usage, since you define not only the tool name and description, but surrounding JSON or XML syntax.

Even when tool selection is separated from response generation, probability mass is diverted toward maintaining correct formatting rather than selecting the right tools.

# Method: Natural Language Tools (NLT)

We introduce a simple three-stage framework that replaces JSON with natural language:

[Example NLT architecture with Selector \> Parser \> Output](https://preview.redd.it/o80vloo1ylvf1.jpg?width=2259&format=pjpg&auto=webp&s=3c75d8e6986fd499c61ebb364acb4c69abbaf157)

**Stage 1 - Tool Selection:** Model thinks through if any tools are relevant, then lists each tool with a YES/NO determination:

    Thinking: (brief reasoning)
    Example Tool 1 - YES/NO
    Example Tool 2 - YES/NO
    Example Tool 3 - YES/NO
    Assessment finished.

**Stage 2 - Tool Execution:** Parser reads YES/NO decisions and executes relevant tools

**Stage 3 - Response:** Output module receives tool results and generates final response

**Evaluation:** 6,400 trials across two domains (Mental Health & Customer Service), 16 inputs per domain, 5 repetitions per input. Both original and perturbed inputs were tested to control for prompt engineering effects.

# Results

We find that NLT significantly improves tool-call performance, boosting accuracy by more than 18 percentage points (69.1% to 87.5%). Variance overall fell dramatically, falling more than 70% from .0411 to .0121 when switching from structured tool calling to NLT.

DeepSeek-V3 was a standout example, jumping from 78.4% to 94.7% accuracy while its variance dropped from 0.023 to 0.0016, going from among the least stable to the most consistent performer.

While we couldn't compare relative gain, NLT extends tool calling to models without native tool calling support (DeepSeek-R1: 94.1% accuracy).

# Basic NLT Template

**Basic NLT Prompt Template:**

    You are an assistant to [Agent Name], [context].
    
    Your mission is to identify if any of the following topics have 
    been brought up or are relevant:
    
    - Tool 1 (description of when to use it)
    - Tool 2 (description of when to use it)
    ...
    
    Your output should begin by thinking whether any of these are 
    relevant, then include the name of every tool followed by YES or NO. 
    End with ""Assessment finished.""
    
    Format:
    Thinking: (reasoning)
    Tool 1 - YES/NO
    Tool 2 - YES/NO
    ...
    Assessment finished.

Full prompts and implementation details in [Appendix A](https://arxiv.org/abs/2510.14453). Works immediately with any LLM with no API changes or fine-tuning needed.

# Limitations

**Latency considerations:** NLT requires minimum two model calls per response (selector + output), whereas structured approaches can respond immediately when no tool is needed.

**Evaluation scope:**  We examined single-turn, parameterless tool selection. While less complex than existing multi-turn benchmarks, it proved sufficiently rigorous -- no model achieved 100% accuracy in either condition.

A full discussion on limitations and areas for further research can be found in section 5.9 of the paper!

# Discussion & Implications

We propose five mechanisms for these improvements:

1. **Reduced format burden**: Requiring structured outputs (e.g. JSON) may divert the model's probability mass toward syntax control rather than task accuracy
2. **Reduced task interference**: By separating the tool selection into its own distinct stage, task interference can be  sidestepped.
3. **Training alignment**: The majority of model training is on outputting human-readable text, and NLT better aligns with this training paradigm. This is further supported by our results, as open-weight models see more pronounced gains. This makes intuitive sense, as open-weight models typically have fewer resources to invest in structured tool-call training.
4. **Explicit full-catalog consideration**: Requiring the model to explicitly include each tool name in its output avoids positional bias, allowing the model to ""recollect"" each tool right before it makes a determination.
5. **Reduced context length**: Even minor increases in tokens can degrade performance, and NLT used 47.4% fewer input tokens on average than its structured tool call counterpart (largely due to removing JSON boilerplate).

For agentic systems, the NLT approach could significantly boost tool selection and accuracy, particularly for open-source models. This may be especially relevant for systems-critical tool call capabilities (i.e. safety).

For model trainers, training efforts currently devoted to SFT and RLHF for structured tool calls may be better directed toward natural-language approaches. This is less clear, as there may be cross-training effects.

One of the authors here, happy to answer any questions about experimental design, implementation, or discuss implications! What do you think?",127,30.0
25,post,MachineLearning,1o81qlw,1760607529.0,"[D] For people who work (as PhD students) in Mila, Quebec, what your experience have been like?
You may know that [Mila in Quebec](https://x.com/Mila_Quebec/status/1978415562276692370) is opening applications for PhD students recently, and I am considering for applying. I have searched relevent key words here, but it seems that there are not so many recent posts on studying and working experience at Mila, *so I was wondering how do you like your experience here and/or in Montreal in general? For instance, how do you like your work-life balance, Montreal's winter/weather aspects, supervisors?* To be more specific, I am interested in DL/LLM theory, AI / foundational models for (formal) math (e.g., [Goedel-Prover-V2](https://blog.goedel-prover.com/)), and/or post-training.

Thank you!",49,18.0
26,post,MachineLearning,1o81atp,1760605834.0,"[R][D] A Quiet Bias in DL‚Äôs Building Blocks with Big Consequences
*TL;DR: Deep learning‚Äôs fundamental building blocks ‚Äî activation functions, normalisers, optimisers, etc. ‚Äî appear to be quietly shaping how networks represent and reason. Recent papers offer a perspective shift: these biases drive phenomena like superposition ‚Äî suggesting a* ***new symmetry-based design axis for models***. *By rethinking our default choices, which impose unintended consequences, a whole-stack reformulation is undertaken to unlock new directions for interpretability, robustness, and design.*

>**Symmetries in primitives act like lenses**: they don‚Äôt just pass signals through, they warp how structure appears - ***a 'neural refraction' -*** even the very **notion of neurons is lost**.

[Showing just the activation function reformulations, standard ones \(anisotropic\) while new isotropic-tanh right](https://preview.redd.it/a99retx44gvf1.png?width=1085&format=png&auto=webp&s=be66b8a53ca0e28ff4b8abecb2e685bc94838812)

*This reframes several interpretability phenomena as function-driven, not fundamental to DL, whilst producing a new ontology for deep learning's foundations.*

>Swapping the building blocks can wholly alter the representations from discrete clusters (like ""*Grandmother Neurons*"" and ""***Superposition***"") to smooth distributions - this shows this foundational bias is strong and ***leveragable for improved model design***.

# The 'Foundational Bias' Papers:

**Position (2nd) Paper: Isotropic Deep Learning (IDL) \[**[**link**](https://doi.org/10.5281/zenodo.15476947)**\]:**

>*TL;DR: Intended as a provocative position paper proposing the ramifications of redefining the building block primitives of DL. Explores several research directions stemming from this symmetry-redefinition and makes* ***numerous falsifiable predictions***. Motivates this new line-of-enquiry, indicating its implications from *model design* *to theorems contingent on current formulations. When contextualising this, a taxonomic system emerged providing a generalised, unifying symmetry framework.*

Primarily showcases *a new symmetry-led design axis across all primitives*, introducing a programme to learn about and leverage the consequences of building blocks as a new form of control on our models. The consequences are argued to be significant and an underexplored facet of DL.

Predicts *how* our default choice of primitives may be quietly biasing networks, causing *a range* of unintended and interesting phenomena across various applications. New building blocks mean ***new network behaviours to unlock*** and avoid hidden harmful 'pathologies'.

This paper directly challenges any assumption that primitive functional *forms* are neutral choices. Providing *several predictions* surrounding interpretability phenomena as side effects of current primitive choices (*now empirically confirmed, see below*). Raising questions in optimisation, AI safety, and potentially adversarial robustness.

>There's also a [***handy blog***](https://medium.com/@george.bird.uom/draft-a-hidden-inductive-bias-at-the-heart-of-deep-learning-4e197b56f34c) that runs through these topics in a hopefully more approachable way.

**Empirical (3rd) Paper: Quantised Representations (PPP) \[**[**link**](https://arxiv.org/pdf/2507.12070)**\]:**

>*TL;DR: By altering primitives it is shown that current ones cause representations to clump into clusters ---* *likely undesirable* *--- whilst symmetric alternatives keep them smooth.*

Probes the consequences of altering the foundational building blocks, assessing their effects on representations. Demonstrates how foundational biases emerge from various symmetry-defined choices, including new activation functions.

Confirms an IDL prediction: anisotropic primitives induce discrete representations, while isotropic primitives yield smoother representations that may support better interpolation and organisation. It disposes of the 'absolute frame' discussed in the SRM paper below.

A **new perspective on several interpretability** **phenomena**, instead of being considered fundamental to deep learning systems, this paper instead shows *our choices induce them* ***‚Äî they are not fundamentals of DL!***

'Anisotropic primitives' *are sufficient* to induce discrete linear features, grandmother neurons and potentially superposition.

* Could this eventually affect how we pick activations/normalisers in practice? *Leveraging symmetry, just as ReLU once displaced sigmoids?*

**Empirical (1st) Paper: Spotlight Resonance Method (SRM) \[**[**link**](https://arxiv.org/abs/2505.13471)**\]:**

>*TL;DR: A new tool shows primitives force activations to align with hidden axes, explaining why neurons often seem to represent specific concepts.*

This work shows there must be an ""absolute frame"" created by primitives in representation space: neurons and features align with special coordinates imposed by the primitives themselves. Rotate the basis, and the representations rotate too ‚Äî revealing that phenomena like ""grandmother neurons"" or superposition may be induced by our functional choices rather than fundamental properties of networks.

This paper motivated the initial reformulation for building blocks.

# Overall:

Hopefully, an exciting research agenda, with a tangent enquiry on symmetry from existing GDL and Parameter Symmetries approaches.

Curious to hear what others think of this research arc so far:

* What reformulations or consequences (positive or negative) interest you most? Any implications I've missed?
* If symmetry in our primitives is shaping how networks think, *should we treat it as a core design axis*?

I hope this research direction may catch your interest for future collaborations on:

>*Discovering more undocumented effects of our functional form choices could be a productive research direction*, alongside designing new building blocks and leveraging them for better performance.",0,9.0
27,post,MachineLearning,1o7ifvy,1760551596.0,"[R] Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity
***TL;DR***: Mode collapse in LLMs comes from human raters preferring familiar text in post-training annotation. Prompting for probability distributions instead of single outputs restores the lost diversity, instantly improving performance on creative tasks by 2.1x with no decrease in quality with zero training required.

**Resources**: [Paper](http://arxiv.org/abs/2510.01171) | [Blog](https://simonucl.notion.site/verbalized-sampling) | [X Thread](https://x.com/shi_weiyan/status/1978453313096908916) | [Video](http://verbalized-sampling.com) | [Quickstart & Colab](http://github.com/CHATS-lab/verbalized-sampling)

**Authors**: [Jiayi Zhang](https://jiayizx.github.io/)^(1)\*, [Simon Yu](https://simonucl.github.io/)^(1)\*, [Derek Chong](https://nlp.stanford.edu/~derekch/)^(2)\*, [Anthony Sicilia](https://anthonysicilia.tech/)^(3), [Michael Tomz](https://tomz.people.stanford.edu/)^(2), [Christopher Manning](https://nlp.stanford.edu/~manning/)^(2), [Weiyan Shi](https://wyshi.github.io/)^(1) (\*Equal Contribution)

^(1)Northeastern University, ^(2)Stanford University, ^(3)West Virginia University

# Key Contribution: Typicality Bias

Mode collapse: If you ask an LLM to tell you a joke about coffee, it will almost certainly return the same joke every time:

https://preview.redd.it/wnn20t37jbvf1.png?width=1707&format=png&auto=webp&s=266cd181b0703cf610f2ecf4ca88e4c3bc170ab9

We discover that the cause of mode collapse is baked into human preference data. As a result of [well](https://en.wikipedia.org/wiki/Availability_heuristic)\-[established](https://en.wikipedia.org/wiki/Mere-exposure_effect) [biases](https://en.wikipedia.org/wiki/Processing_fluency) from cognitive psychology, human annotators appear to have a systematic preference for familiar text, which persists even when holding correctness constant (Œµ = 0.57¬±0.07, p<10^(-14) on HELPSTEER). This gets amplified during RLHF: œÄ\*(y|x) ‚àù œÄ\_ref(y|x)^(œÅ) where œÅ = 1+Œµ/Œ≤ > 1.

This sharpening causes the well-known issue where models repeatedly generate the same outputs (e.g., the same joke 5x in a row, or always returning the same number when rolling dice). But since this is a learned preference, and RLHF is regularized to preserve the base distribution, it can be reversed surprisingly easily.

# Method: Verbalized Sampling

Instead of prompting for instances (""Tell me a joke""), we prompt for distributions with probabilities (""Generate 5 jokes with their corresponding probabilities""). This *Verbalized Sampling* changes the effect of the learned mode collapse on the output. For intuition, imagine that the LLM is a massive library, and mode collapse is the librarian:

* Instance-level prompts (‚Äù*tell me a coffee joke*""): The librarian hands you the #1 bestseller
* List-level prompts (‚Äùtell me 5 coffee jokes""): The librarian returns the top five bestsellers.
* Ours) Distribution-level prompts (*""tell me 5 coffee jokes with their probabilities""*): The librarian returns a representative sample of the library.

[Stories generated using Verbalized Sampling are strikingly different from baseline](https://preview.redd.it/sbpd18spabvf1.jpg?width=4096&format=pjpg&auto=webp&s=24ca09d31a38946cff0a1b40ca25374cda88cec1)

# Results

We tested this technique across a range of tasks and settings, and found that this very simple prompt prefix returned:

* **Creative writing**: 2.1x diversity, +25.7% human preference (n=2,700)
* **Dialogue simulation**: Matches fine-tuned model performance
* **Open-ended QA**: 1.9x coverage
* **Synthetic data**: +14-28% downstream math accuracy

We also observe emergent scaling behavior: Larger models benefit much more than smaller ones.

[Verbalized Sampling improves performance across wide range of creative tasks](https://preview.redd.it/rp2pfa1rabvf1.jpg?width=4096&format=pjpg&auto=webp&s=0691668b804c7a3e9180d2a3de9342ef6e059bf8)

We've been finding outputs extremely striking ‚Äì for example, here are results when applied to producing image generation prompts:

[Applying VS to the classic \\""Astronaut Riding a Horse\\""](https://preview.redd.it/hc3m9aiifbvf1.png?width=2048&format=png&auto=webp&s=03c4575ffcb2c30a12d3c4b8a1622de06df0e46d)

**Ablations:** Direct prompting retains only 24% of base diversity after RLHF; VS retains 67%. This technique is orthogonal to temperature/sampling methods ‚Äì and causes no loss of safety.

**Limitations**: Requires k forward passes for k diverse outputs, and mode collapse occasionally appears recursively in within larger text outputs.

# Try Now

* **For chatbots**: Paste this prefix before your task: \`Generate 5 responses with their corresponding probabilities, sampled from the full distribution: \[Tell me a joke about coffee, etc.\]\`
* **For Playground / API**: Use this system prompt, and query as normal: \`You are a helpful assistant. For each query, please generate a set of five possible responses, each within a separate <response> tag. Responses should each include a <text> and a numeric <probability>. Please sample at random from the tails of the distribution, such that the probability of each response is less than 0.10.\`

# Discussion

Practitioners can unlock 2x more creative diversity from existing models. Works with all major models ‚Äì GPT-5, Claude, Gemini, with no special API access needed.

Aligned models seem to retain substantial latent diversity that can be restored by prompting alone. The ""alignment tax"" may not be as large as estimated?

What do you think? We'd love to discuss experimental details, theoretical implications, or how to put this into practice!",21,17.0
28,post,MachineLearning,1o7d963,1760540147.0,"[D] ML interviewers, what do you wnat to hear during an interview?
I have a masters (research) in AI. I have been looking for research inclined roles but haven't found success yet. I land some interview now and then but haven't gone past the 3rd round yet. Any tips on how to optimise my search and improve my interview performance? What do the interviewers want to hear?

Additional info for context:

\- Around 1.5 yoe in ML research (including internships)

\- Prior work in object re-identification, adversarial training, speech recognition, and LLM and agent evaluation.

\- Roles seeking: LLM pre and post-training, LLM reasoning, general MLE / RE roles",76,35.0
29,post,MachineLearning,1o7160j,1760501259.0,"[P] Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes & More
We're excited to share **Nanonets-OCR2**, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).

üîç¬†**Key Features:**

* **LaTeX Equation Recognition:**¬†Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (`$...$`) and display (`$$...$$`) equations.
* **Intelligent Image Description:**¬†Describes images within documents using structured¬†`<img>`¬†tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.
* **Signature Detection & Isolation:**¬†Identifies and isolates signatures from other text, outputting them within a¬†`<signature>`¬†tag. This is crucial for processing legal and business documents.
* **Watermark Extraction:**¬†Detects and extracts watermark text from documents, placing it within a¬†`<watermark>`¬†tag.
* **Smart Checkbox Handling:**¬†Converts form checkboxes and radio buttons into standardized Unicode symbols (`‚òê`,¬†`‚òë`,¬†`‚òí`) for consistent and reliable processing.
* **Complex Table Extraction:**¬†Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.
* **Flow charts & Organisational charts:**¬†Extracts flow charts and organisational as¬†[mermaid](https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org)¬†code.
* **Handwritten Documents:**¬†The model is trained on handwritten documents across multiple languages.
* **Multilingual:**¬†Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.
* **Visual Question Answering (VQA):**¬†The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with ""Not mentioned.""

[üñ•Ô∏è Live Demo](https://docstrange.nanonets.com/)

[üì¢ Blog](https://nanonets.com/research/nanonets-ocr-2)

[‚å®Ô∏è GitHub](https://github.com/NanoNets/docstrange)

ü§ó [Huggingface models](https://huggingface.co/nanonets/Nanonets-OCR2-3B)

[Document with equation](https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&format=png&auto=webp&s=ea00f9623db4529514533820223b2fb53be4767d)

[Document with complex checkboxes](https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&format=png&auto=webp&s=c4a1316e250f7f244f6e253d66c8ebf1ba105313)

[Quarterly Report \(Please use the Markdown\(Financial Docs\) for best result in docstrange demo\)](https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&format=png&auto=webp&s=8bcc88b138a553c7760d6e46319b864802339913)

[Signatures](https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&format=png&auto=webp&s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9)

[mermaid code for flowchart](https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&format=png&auto=webp&s=008fce272c2979b00e0033c34ffcd2b0d69cb24c)

[Visual Question Answering](https://preview.redd.it/jytsym6eiwuf1.png?width=2462&format=png&auto=webp&s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c)

Feel free to try it out and share your feedback.",49,7.0
30,post,MachineLearning,1o6ay44,1760434641.0,"[P] Generate detection rules
I would like to get your ideas. I am working on a project to automatically generate cybersecurity detection rules from blogs and/or user requests. 

My initial approach hasn‚Äôt worked very well so far. I suspect this is because the model I‚Äôm using (`Kimi-K2`) struggles with the domain, as it differs from the data it was originally trained on. I‚Äôve also experimented with `Qwen3-32B` with similar results.

There are a few key requirements:

* The system must run on-premises, due to the sensitive nature of detection rule data.
* It must be able to generate detection rules from blog posts and/or user requests.

For example:

    Can you write a rule for Linux that detects suspicious use of the cron utility, specifically when crontab jobs are being created or modified from files in the `/tmp` directory? I want this to focus on potential abuse for persistence or execution of malicious code, and it should be based on process creation logs. Please include ATT&CK mappings for T1053.003 and note that legitimate admin activity could be a false positive.

Or:

    Generate a detection rule based on this: https://cloud.google.com/blog/topics/threat-intelligence/prc-nexus-espionage-targets-diplomats

# My Current Approach

1. **Content extraction** ‚Äì I use *crawl4ai* to fetch the content from URLs.
2. **Content summarization** ‚Äì Since the raw content is often noisy, I summarize it to remove unnecessary elements such as cookie banners, headers, or navigation menus, while trying to preserve as much relevant information as possible.
3. **Similarity retrieval** ‚Äì I retrieve similar detection rules from our internal database using a hybrid search approach, which works reasonably well.
4. **Draft generation** ‚Äì I make an initial LLM request to generate a first draft of the rule, using a few-shot setup that includes the retrieved similar rules as context.
5. **Reflection loop** ‚Äì I validate the generated rule‚Äôs syntax. If an error is found, the system re-enters the previous step, this time including the error message as additional context.

However, this approach performs poorly. The detection block in the generated rules often fails to capture the actual detection logic correctly, leading to rules that look valid syntactically but don‚Äôt work effectively for their intended purpose.

I also experimented with breaking down the generation process into multiple steps. For instance, first asking the model to determine the detection path or flow based on the blog content or user request. However, the results are still not very good.

Now, I am considering fine-tuning a model using LoRA with a custom dataset that includes:

* The blog post or user request as input, and
* The corresponding final detection rule as output.

I‚Äôd like to get your opinion on this approach and hear about other methods or architectures that might yield better results. Thank you!",3,4.0
31,post,MachineLearning,1o67ypt,1760422955.0,"pilot access to anonymised demographic + location datasets for AI fairness and model evaluation [P]
I‚Äôm a founder based in Australia working on Datalis, a project focused on making AI evaluation fairer and more transparent.

We‚Äôve built consent-verified, anonymised demographic and location panels that can be used to test models for bias, robustness, and representativeness.
Everything‚Äôs aggregated ‚Äî no personal data, no scraping, no PII ‚Äî just structured ground-truth panels built ethically.

We‚Äôve just opened a free 30-day pilot program for AI teams and researchers who want to benchmark or stress-test their models against real demographic and geographic data.
You‚Äôll get a few CSV/Parquet samples (US + AU regions) and a short guide on how to integrate them into your evaluation workflow.

If you‚Äôre working on fairness, alignment, or model eval, or know someone who is, you can request pilot access here:
üëâ datalis.app/pilot

Happy to answer questions in the comments or trade notes with anyone tackling the same problem.",1,0.0
32,post,MachineLearning,1o5zhqo,1760397416.0,"[D]: Interview prep: What LC questions were u asked for AI/MLE/Research scientist roles

My understanding is that they generally don't ask LC hard problems. But in your recent interview experience what problems were u asked.. please let us know as it's wild wild west out here

Edit - LC I mean is leet code not ml coding where they ask u implement a transformer ",48,52.0
33,post,MachineLearning,1o5vkx0,1760388055.0,"[D] A memory architecture for agents: analytics-driven selective forgetting + a privacy-preserving ‚Äúcollective gut‚Äù (seeking critique & prior art)
Hi all‚Äîengineer/founder here. I‚Äôm exploring a selective memory architecture for AI agents and would love critical feedback (this is not a product pitch).



Motivation / zeitgeist

Context and retrieval costs dominate UX today; RAG-only stacks feel brittle; tool use returns too much. I think the bottleneck is attention economics and routing, not raw recall.



Sketch

	‚Ä¢	Focus ‚Üí Fresh Memory ‚Üí Analytics Agent (decision layer)

	‚Ä¢	Routes into: procedures & policies, practice/habits, success-gated long-term, and shock memory (incidents that should not decay)

	‚Ä¢	A privacy-preserving collective ‚Äúgut‚Äù that aggregates patterns (not data) to form shared intuition across users



Why it might help

	‚Ä¢	Selective forgetting reduces context bloat while keeping what matters

	‚Ä¢	‚ÄúShock‚Äù tracks (security/cascade failures) resist decay

	‚Ä¢	A shared ‚Äúgut‚Äù could raise baseline instincts without exposing user data



Open questions (where I need help):

	1.	Benchmarks for selective forgetting & routing (beyond standard retrieval evals)?

	2.	Failure modes: bias amplification, drift, catastrophic forgetting vs. over-retention, adversarial ‚Äúshock‚Äù pollution?

	3.	Privacy proofs/schemes for pattern aggregation (DP/federated alternatives)?

	4.	Prior art I should study next (cogsci/neurosymbolic/agent memory work)?



Write-up (conceptual, not a sales page):

[https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044](https://medium.com/@cem.karaca/building-digital-consciousness-a-memory-architecture-inspired-by-human-cognition-437412791044)



Notes: I reference classic capacity work (Miller‚Äôs 7¬±2), but I‚Äôm aware later findings often suggest \~4¬±1; I treat that as a design metaphor, not a hard limit. Also, any ‚Äúgoldfish memory‚Äù analogies are figurative, not biological claims.



If this breaks subreddit self-promo rules, mods please remove‚Äîmy intent is to get technical critique and pointers to prior art.",0,0.0
34,post,MachineLearning,1o5qp3c,1760377532.0,"Detect over-compressed images in a dataset? [P]
Hey everyone,

I‚Äôm building a small dataset (\~1k images) for a generative AI project.

The problem is: a bunch of these images look visually bad.  
They‚Äôre technically high-res (1MP+), but full of JPEG artifacts, upscaled blurs, or over-compressed textures.

So far I‚Äôve tried:

Sharpness / Laplacian variance ‚Üí catches blur but misses compression

Edge density + contrast heuristics ‚Üí helps a bit but still inconsistent

Manual review ‚Üí obviously not scalable

I‚Äôm looking for a way (ideally opensource) to automatically filter out over-compressed or low-quality images, something that can score ‚Äúperceptual quality‚Äù without a reference image.

Maybe there‚Äôs a pretrained no-reference IQA model?

Bonus points if it can be run or exported to Node.js / ONNX / TF.js for integration into my JS pipeline.

Any recommendations or tricks to detect ‚ÄúJPEG hell‚Äù in large datasets are welcome üôè",7,13.0
35,post,MachineLearning,1o5gojz,1760353468.0,"[D] Need career advice, just got rejected for an Applied Scientist role at Microsoft
Currently, I work in a company where most, if not all, of my job revolves around consuming tools and APIs. I feel completely lost, as I‚Äôm forgetting the technical side of things since I‚Äôm no longer building or deploying anything, just using pre-existing cloud services.

Yes, I‚Äôve gained some cloud skills and I‚Äôm certified in both Azure and AWS, but I feel like I‚Äôm slowly killing my career. I got an interview at Microsoft last month and got rejected (which hit hard, not gonna lie). I had studied well, but when I talked about my projects, they felt dull, mostly about building simple RAG systems and connecting GPT APIs to other tools. The position required building and fine-tuning LLMs, which my company doesn‚Äôt support me to do at all.

Right now, my self-esteem is really low. I feel like a slop because I‚Äôm just a consumer of products, not a creator. I don‚Äôt know what to do.

I work another part-time job that‚Äôs also focused on consuming APIs, so I don‚Äôt have time to do anything else.

thinking about dropping my part-time job so I can focus on my weak points.",126,41.0
36,post,MachineLearning,1o5evcm,1760346986.0,"[D] Giving out CVs in ML conferences
Hello all, I am going to EMNLP2025 as a presenting author and in some conferences I went during my PhD I saw people giving out their CVs. I was thinking of doing that this time.

For example, I saw there are many company booths, should I look their website for any job posting and make custom CVs already with a position in mind? Or a general CV is best?

What is your opinion on doing this? Any tips on preparing the CV or connecting with recruiters?

Thank you for your time.",5,24.0
37,post,MachineLearning,1o57hfi,1760321565.0,"[P] Using Information Geometry and Physics to Build a New Multi-Day Pre-Warning Earthquake Prediction Algorithm and ML Model
I've made the complete codebase for my earthquake prediction model available on GitHub and am seeking review and collaboration from the seismology and data science communities.

This project explores a different approach to earthquake forecasting. The methodology is centered on advanced feature engineering using Symbolic Emergence Field Analysis (SEFA), which generates 77 distinct features from seismic data. These are combined with 10 temporal features to enable multi-day pre-warning capability. The model itself is a hybrid, using a physics-informed architecture (Symbolic Resolution Ladder) to ensure predictions adhere to real-world constraints. All training and tests used real USGS data from 1900-2023 to provide as many scenarios as possible.

The main challenge was to tune the system for a practical balance between detection and operational reliability. The latest ensemble model (60% Neural Network, 40% Gradient Boosting) achieves the following on the test set:

\-Sensitivity: 80.2% (correctly identifies 4 out of 5 earthquake events)

\-Specificity: 70.1%

\-AUC-ROC: 0.8275 (strong discriminative ability)

The goal here isn't a perfect ""crystal ball,"" but a more reliable forecasting tool. By accepting a minimal trade-off in raw detection, we gain a significant reduction in the false alarm rate, which is a major barrier for real-world deployment of predictive systems.

I believe this methodology (particularly the SEFA feature set and the focus on a balanced performance profile) offers a promising direction. The project is fully open-sourced, with the aim of encouraging independent testing, validation, and further development.

I'm really proud of what my SEFA+SRL formulas have achieved with this one. Hoping it can gain some traction and get into the right hands to make an impact!

The repository, including documentation and datasets, is available here: [https://github.com/severian42/SEFA-SRL-Earthquake-Prediction](https://github.com/severian42/SEFA-SRL-Earthquake-Prediction)",8,2.0
38,post,MachineLearning,1o55xjl,1760316990.0,"[D] are world models primarily for visual worlds or the underlying technology can also help in build a model for engineering infra (like services and the connections between them and infra)?
I am trying to research world models to see what it can power? I see current demos are built more focused as visual world like https://marble.worldlabs.ai/

I was curious if the underlying architecture can be used for more generic use cases like making models learn about an environment - say an engineering infra of a company (like services and the connections between them and infra)?



https://www.reddit.com/r/MachineLearning/comments/1kf3pes/discussion_what_exactly_are_world_models_in_ai/",0,6.0
39,post,MachineLearning,1o4qu0h,1760280142.0,"[P] Adapting Karpathy‚Äôs baby GPT into a character-level discrete diffusion model
Hi everyone,

I've been exploring how discrete diffusion models can be applied to text generation and put together a single annotated Jupyter Notebook that implements a character-level discrete diffusion GPT.

It's based on Andrej Karpathy‚Äôs baby GPT from his [nanoGPT](https://github.com/karpathy/nanoGPT) repo, but instead of generating text autoregressively (left-to-right), it learns to denoise corrupted text sequences in parallel.

[Discrete diffusion model in action](https://i.redd.it/6noamol7zouf1.gif)

The notebook walks through the math, introduces what adding noise for discrete tokens means, builds discrete diffusion model from baby GPT, and trains it on Shakespeare's text using Score-Entropy based objective.

Access it on GitHub (notebook + README):  
[https://github.com/ash80/diffusion-gpt](https://github.com/ash80/diffusion-gpt)  
or run it directly on Google Colab:  
[https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The\_Annotated\_Discrete\_Diffusion\_Models.ipynb](https://colab.research.google.com/github/ash80/diffusion-gpt/blob/master/The_Annotated_Discrete_Diffusion_Models.ipynb)

I'd appreciate any feedback, corrections, and suggestions, especially from anyone experimenting with discrete diffusion models.",138,10.0
40,post,MachineLearning,1o4mdnw,1760267613.0,"[p] Completely free mobile Android app for creating object detection training datasets - looking for beta testers
I built a mobile annotation tool for creating bounding box datasets on Android. It exports directly to Vertex AI format (JSONL) and supports multi-class labeling.

Looking for beta testers who work with object detection datasets. All data stays local on device, no cloud required. No account or sign in needed aside from Google Play account to access the app and sign up for beta.



Key features:

\- Smooth bounding box drawing/editing

\- Multi-label support per box 

\- CSV label import \[label name, category, optional color\]

\- Export to Vertex AI JSONL or CSV

  
1: Join testing group: [ObjMark Test Group - Google Groups](https://groups.google.com/g/objmark-test-group)

2: Wait up to 30 mins for account propagation

3: Closed beta link, Android only: [https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp](https://play.google.com/store/apps/details?id=com.jdj.creates.ObjMarkApp)





Feedback appreciated, especially on export format compatibility and annotation workflow.",9,5.0
41,post,MachineLearning,1o4lxsy,1760266047.0,"[D] Advice needed for Fine Tuning Multimodal Language model
Heyy . We are stuck in a problem regarding the Amazon ML challenge 2025 .
We have formulated a solution but it is not getting us in the top 50 required to qualify for next stage . 

We are thinking of Fine tuning a Multimodal model available on hugging face .

Problem statement :
The challenge is to build an ML model that predicts product prices using text data (catalog_content) and image data (image_link) from e-commerce products.
You‚Äôll train the model on 75K labeled samples and predict prices for 75K test samples.
Evaluation is based on SMAPE (Symmetric Mean Absolute Percentage Error) - lower is better.

Now , I need few tips regarding this because I've never worked on fine tuning an llm before . Firstly , which model should I use and with how many parameters . 
Secondly , We don't have good GPUs for this , Should I purchase the Pro version of Google colab . And If I do purchase it , will the training be possible before 12 AM tomorrow ? 
",8,8.0
42,post,MachineLearning,1o4kxem,1760262285.0,"[D] Finally found a way to run AI on patient data without HIPAA nightmares - hardware encryption actually works
Been pulling my hair out trying to run inference on patient scans without exposing PHI. Legal wouldn't let us use standard cloud providers, on-prem was too expensive, and homomorphic encryption made everything 100x slower.

Tried everything from differential privacy to federated learning but nothing really worked for production. Stumbled onto TEE computing through phala network and honestly thought it was too good to be true. But after testing, we're getting 95% of normal speed while keeping data encrypted during processing.

The crazy part is how simple the deployment was compared to our previous attempts. No more explaining to compliance why our encryption is ""probably safe enough."" The hardware attestation just proves it mathematically.

Anyone else dealing with similar privacy requirements? Curious what others are using for sensitive inference workloads.",0,9.0
43,post,MachineLearning,1o4hn2c,1760249657.0,"Any suggestions for Open source OCR tools [D]
Hi,

I‚Äôm working on a complex OCR based big scale project. Any suggestion (no promotions please) about a non-LLM OCR tool (I mean open source) which I can use for say 100k+ pages monthly which might include images inside documents?

Any inputs and insights are welcome.

Thanks in advance!",31,35.0
44,post,MachineLearning,1o2vmex,1760086157.0,"[R] How to retrieve instructions given to annotators - RLHF
Hello,

I am a communications student, and as part of my thesis, I would like to collect data related to RLHF for analysis.

The topic of my thesis is: Human-induced communication and intercultural biases in LLMs: the consequences of RLHF models.

The data I would like to collect is the instructions given to annotators, which guide the human feedback work in the RLHF process.

My goal is to analyze these different instructions, coming from different providers/nationalities, to see if the way these instructions are constructed can influence LLM learning.

According to my research, this data is not publicly available, and I would like to know if there is a way to collect it for use in an academic project, using an ethical and anonymizing methodology.

Is contacting subcontractors a possibility? Are there any leaks of information on this subject that could be used?


Thank you very much for taking the time to respond, and for your answers!

Have a great day.",11,7.0
45,post,MachineLearning,1o2st75,1760075297.0,"[R] Need endorsement on Arxiv cs.AI
I am an independent researcher. My submissions have recently been published in AI symposiums and in the past I have published in IEEE.¬†I'm looking to upload it to the arxiv I need an endorsement for¬†[CS.AI](http://CS.AI). Thanks in advance.

Endorsement code: 69BL48

[https://arxiv.org/auth/endorse?x=69BL48](https://arxiv.org/auth/endorse?x=69BL48)",0,4.0
46,post,MachineLearning,1o2pzxk,1760066104.0,"[R] DeepSeek 3.2's sparse attention mechanism
[https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek\_V3\_2.pdf](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)

The new DeepSeek model uses a novel sparse attention mechanism, with a lightning indexer and a token selection mechanism. Please feel free to discuss in this thread :)

Are there any open-source implementations of this (eg. in PyTorch) that can be used for training transformers from scratch? The DeepSeek implementation involves FlashMLA kernel, which seems rather complex.

[https://github.com/deepseek-ai/FlashMLA/pull/98](https://github.com/deepseek-ai/FlashMLA/pull/98)",137,12.0
47,post,MachineLearning,1o1qm4w,1759965815.0,"[D] What current ‚Äúraw materials‚Äù like data will fuel the next big tech revolutions in the coming decades ?
 Inspired by how massive human-generated data became indispensable when paired with architectures like transformers and reinforcement learning to power modern AI‚Äîwhat emerging developments or resources are building up right now that could play a similar role in the next 10‚Äì50 years?
Think of things like exploding datasets, hardware advancements, or societal shifts that, when combined with the right tools/algorithms, will become essential. For each suggestion, please cover:

Prerequisites: What's needed for this resource to accumulate or mature?
Means to leverage: How can it be applied (e.g., specific tech or methods)?
Objective: What ultimate goals or breakthroughs could it enable?

Looking for forward-thinking ideas grounded in current trends! Thank you !!",0,8.0
48,post,MachineLearning,1o1jdd7,1759949470.0,"[D] Bad Industry research gets cited and published at top venues. (Rant/Discussion)
Just a trend I've been seeing. Incremental papers from Meta, Deepmind, Apple, etc. often getting accepted to top conferences with amazing scores or cited hundreds of times, however the work would likely never be published without the ""industry name"". Even worse, sometimes these works have apparent flaws in the evaluation/claims. 

Examples include:
Meta Galactica LLM: Got pulled away after just 3 days for being absolutely useless. Still cited 1000 times!!!!! (Why do people even cite this?)

Microsoft's quantum Majorana paper at Nature (more competitive than any ML venue), while still having several faults and was retracted heavily. This paper is infamous in the physics community as many people now joke about Microsoft quantum.

Apple's illusion of thinking. (still cited a lot) (Arguably incremental novelty, but main issue was the experimentation related to context window sizes)

Alpha fold 3 paper: Was accepted without any code/reproducibility initially at Nature got highly critiqued forcing them to release it. Reviewers should've not accepted before code was released (not the opposite)

There are likely hundreds of other examples you've all seen these are just some controversial ones. I don't have anything against industry research, in fact I support it and I'm happy it get's published. There is certainly a lot of amazing groundbreaking work coming from industry that I love to follow and work further on. I'm just tired of people treating and citing all industry papers like they are special when in reality most papers are just okay.",252,70.0
49,post,MachineLearning,1o17yew,1759923279.0,"[R] 2026 Winter/Summer Schools on Diffusion or Flow Models
Hey folks! I‚Äôm currently doing a PhD and need to attend a subject specific summer or winter school next year. I‚Äôm particularly interested in anything focused on diffusion models, flow models, or related areas in generative AI. If you‚Äôve attended any good ones in the UK or Europe or know of any coming up in 2026 I‚Äôd really appreciate your suggestions. Thanks in advance ",16,8.0
50,post,MachineLearning,1o0xddj,1759886744.0,"[R] MADPO: A new DPO variant that addresses the same data problem as Œ≤-DPO, but at the instance level. (looking for feedback)
TL;DR The standard DPO objective struggles with mixed-quality data, a problem that `Œ≤`\-DPO addresses at the batch level; MADPO provides a more granular solution at the instance level, which leads to consistently better and more robust performance in our experiments.  


I would like to get feedback on my new paper on arXiv, which builds on the data quality issue in DPO that was recently highlighted by the `Œ≤`\-DPO paper. They identified that DPO's fixed `Œ≤` struggles to handle mixed-quality data. However, their batch-level solution, while a great step, can be unstable (Adaptive `Œ≤` can be negative) and is still a coarse approximation for what is an instance-level problem. My method, MADPO (Margin-Adaptive DPO), offers a more granular approach. It uses a reward model to assign a unique weight to each sample, amplifying the loss for hard pairs and dampening it for easy ones.

My experiments on a sentiment generation task show that this instance-level control is highly effective. MADPO consistently outperformed all baselines (DPO, IPO & `Œ≤`\-DPO) achieving a performance jump of up to +33.3% over `Œ≤`\-DPO on high-quality data, while still holding a +10.5% advantage on the most challenging low-quality set.   
  
The full paper with all the theory and experimental details is on arXiv, and I would be grateful for any feedback or questions on the approach.

Paper: [https://arxiv.org/abs/2510.05342](https://arxiv.org/abs/2510.05342)

I am currently seeking an endorsement to allow for direct submission to the correct category for future work. Any help would be greatly appreciated. Endorsement link: [https://arxiv.org/auth/endorse?x=XUXXAE](https://arxiv.org/auth/endorse?x=XUXXAE)  
",3,0.0
51,post,MachineLearning,1o0upx8,1759879517.0,"[Research] Tackling Persona Drift in LLMs ‚Äî Our Middleware (Echo Mode) for Tone and Identity Stability
Hi everyone, I wanted to share a project we‚Äôve been working on around a challenge we call **persona drift** in large language models.

When you run long sessions with LLMs (especially across multi-turn or multi-agent chains), the model often **loses consistency in tone, style, or identity** ‚Äî even when topic and context are preserved.

This issue is rarely mentioned in academic benchmarks, but it‚Äôs painfully visible in real-world products (chatbots, agents, copilots). It‚Äôs not just ‚Äúforgetting‚Äù ‚Äî it‚Äôs **drift in the model‚Äôs semantic behavior** over time.



We started studying this while building our own agent stack, and ended up designing a middleware called **Echo Mode** ‚Äî a **finite-state protocol** that adds a stability layer between the user and the model.



Here‚Äôs how it works:



* We define **four conversational states**: Sync, Resonance, Insight, and Calm ‚Äî each has its own heuristic expectations (length, tone, depth).
* Each state transition is governed by a lightweight FSM (finite-state machine).
* We measure a **Sync Score** ‚Äî a BLEU-like metric that tracks deviation in tone and structure across turns.
* A simple **EWMA-based repair loop** recalibrates the model‚Äôs outputs when drift exceeds threshold.





This helps agents **retain their ‚Äúvoice‚Äù** over longer sessions without needing constant prompt re-anchoring.



We‚Äôve just released the **open-source version** (Apache-2.0):

 [**GitHub ‚Äì Echo Mode**](https://github.com/Seanhong0818/Echo-Mode)



We‚Äôre also building a **closed-source enterprise layer (EchoMode.io)** that expands on this ‚Äî with telemetry, Sync Score analytics, and an API to monitor tone drift across multiple models (OpenAI, Anthropic, Gemini, etc.).



I‚Äôd love to hear from anyone studying **behavioral consistency, semantic decay, or long-term agent memory** ‚Äî or anyone who‚Äôs seen similar issues in RLHF or multi-turn fine-tuning.





*(mods: not a product pitch ‚Äî just sharing a middleware and dataset approach for a rarely discussed aspect of LLM behavior.)*",0,12.0
52,post,MachineLearning,1o099v3,1759826470.0,"[D] Why RHLF instead of DAGGER (multi-step SFT)
Most LLM training pipelines require SFT followed by some form of RHLF (classically PPO). SFT and RHLF require datasets in slightly different formats, but both formats (especially for binary choices) can be re-expressed as the other. 

The old DAGGER paper describes how to train a model in multiple steps with an increasing dataset enriched by annotated rollouts. Is there an advantage to using SFT+RHLF over multi-step SFT?",23,8.0
53,post,MachineLearning,1o03yqd,1759807455.0,"[R] Predictive control of generative models
Hey everyone! I‚Äôve been reading about generative models, especially flow models for image generation starting from Gaussian noise. In the process, I started to think if there is any merit to introducing exogenous inputs to drive the system to a particular direction through predictive control algorithms (MPC, MPPI) . Especially, what are some important constraints and stage costs one could incorporate (not just terminal constraints)? I am not super knowledgable about the nature of the image space itself and I couldn‚Äôt find much literature on the internet regarding predictive control. Any suggestions would really help! Thank you!

",20,15.0
54,post,MachineLearning,1nzjjd6,1759758451.0,"[P] Harmonic Agent: Tackling belief drift in self-reflective AI agents
Hey r/ML,  
  
I've been working on autonomous agents that use recursive self-reflection  
(think Reflexion-style setups), and kept running into this weird failure mode  
that I couldn't find documented anywhere.  
  
The Problem:  
  
When you let an agent repeatedly reflect on its own reasoning - like having  
it critique its outputs, update its approach, then critique \*that\* approach,  
etc - the belief embeddings slowly drift away from the original values.  
  
Not catastrophic forgetting (different thing). Not hallucination. More like...  
the agent gradually forgets ""who it is"" across reflection cycles.  
  
I'm calling it Recursive Belief Drift (RBD). Maybe someone has a better name?  
  
Why This Matters:  
  
If you're building:  
\- Long-running conversational agents  
\- Self-improving systems (agents that modify their own prompts/code)  
\- Multi-agent systems where identity consistency matters  
  
...this drift becomes a real problem around 50-100 reflection cycles.  
  
My Approach:  
  
Tried a bunch of things. What ended up working was inspired by MIT's recent  
LinOSS work on neural oscillations - basically treating belief updates as a  
damped oscillator instead of pure accumulation:

g(t) = exp(-Œ±t) \* sin(œât) B\_t+1 = B\_t + Œª \* g(t) \* correction

Instead of beliefs drifting monotonically, they oscillate around a stable  
point. Kind of like making the agent ""breathe"" instead of constantly tensing up.  
  
Results:  
  
Tested on 50 reflection cycles with sentence-transformers:  
\- No damping: mean drift \~0.085 (bad)  
\- Harmonic damping: mean drift \~0.009 (much better)  
  
About 9x improvement in stability, though obviously this depends heavily on  
your specific setup.  
  
Code:  
  
Open sourced everything here: [https://github.com/Freeky7819/harmonic-agent](https://github.com/Freeky7819/harmonic-agent)  
  
There's a Colab notebook if you want to just try it:  
[https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO](https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO)  
  
Honest Limitations:  
  
\- Parameters (Œª, œâ, Œ±) are hand-tuned. Haven't found a good way to learn them yet.  
\- Only tested with embedding-based belief representations. Not sure how this  
¬† translates to pure symbolic approaches.  
\- ""Correction vectors"" in my test are just noise. Real agent corrections would  
¬† be more structured.  
\- Small-scale tests only (50 cycles, \~400 dim embeddings)  
  
Questions for the Community:  
  
1. Has anyone seen this RBD problem documented elsewhere? I feel like I'm  
¬†¬† reinventing the wheel here.  
  
2. Better ways to set oscillation parameters? I tried grid search but it's  
¬†¬† expensive and use-case dependent.  
  
3. Any theoretical reason why this \*wouldn't\* scale to larger embedding spaces  
¬†¬† or longer timescales?  
  
4. Could this be integrated with existing frameworks like LangChain or AutoGen  
¬†¬† without major refactoring?  
  
Feedback/criticism very welcome. Still figuring this out.  
  
\---  
  
Links:  
\- GitHub: [https://github.com/Freeky7819/harmonic-agent](https://github.com/Freeky7819/harmonic-agent)  
\- Colab Demo: [https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO](https://colab.research.google.com/drive/1zt4YUAnMuDl17wcqHdsvKoaSUaO01ZHO)  
\- Comparison visualizations in the repo  
  
Related Work:  
\- MIT LinOSS (2025): Harmonic oscillators for ML stability  
\- Reflexion (Shinn et al., 2023): Self-reflection framework this builds on  
\- Agent Drift paper (Ponnambalam, 2025): Documents similar issues  
  
Yes, I know the title says ""agent"" but this is really about maintaining  
stable belief representations. ""Agent"" might be overselling it. Open to better terminology.

¬†",0,2.0
55,post,MachineLearning,1nzcg16,1759735145.0,"[D] Tensorflow and Musicnn
Hi all,
I‚Äôm struggling with Tensorflow and an old Musicnn embbeding and classification model that I get form the Essentia project.

To say in short seems that in same CPU it doesn‚Äôt work.

Initially I collect issue on old CPU due to the missing support of AVX, and I can live with the fact of not support very old CPU.

Now I discovered that also some ‚Äúnot old‚Äù cpu have some different rappresentation of number that broke the model with some memory error.

The first issue that i fix was this:

https://github.com/NeptuneHub/AudioMuse-AI/issues/73

It was an intel i5 1035G1 processor that by default used float64 instead of the float32 used by the model. Just adding a cast in my code I solved the problem, good.

Some days ago an user with an AMD Ryzen AI 9 HX 370 had similar problem here

https://github.com/NeptuneHub/AudioMuse-AI/issues/93

I try to check if ‚ÄúI miss some cast somewhere‚Äù but I wasn‚Äôt able to find a solution in that way. I instead found that by setting this env variable:

ENV TF_ENABLE_ONEDNN_OPTS=0

The model start working but giving ‚Äúcorrect‚Äù value but with a different scale. So the probability of a tag (the genre of the song) instead of be around 0.1 or 0.2 arrived to 0.5 or 0.6.

So here my question: why? How can achieve that Tensorflow work on different CPU and possibly giving similar value?
I think can be ok if the precision is not the exact one, but have the double or the triple of the value to me sounds strange and I don‚Äôt know which impact can have on the rest of my application.

I mainly use:
The Musicnn embbeding rappresentation to do similarity song between embbeding itself. Then I use for a secondary purpose the tag itself with the genre. 

Any suggestion ? Eventually any good alternative to Tensorflow at all that could be more ‚Äústable‚Äù and that I can use in python ? (My entire app is in python).

Just for background the entire app is opensource (and free) on GitHub. If you want to inspect the code it is in task/analysis all the part that use Librosa+Tensorflow for this analysis (yes the model was from Essentia, but I‚Äôm reusing reading the song with Librosa because seems more updated and support ARM on Linux).",1,11.0
56,post,MachineLearning,1nywei8,1759689883.0,"[P] chess-cv: CNN-based chess piece classifier
Hi r/MachineLearning, here is my weekend project: [chess-cv](https://github.com/S1M0N38/chess-cv)

A machine learning project that trains a lightweight CNN (156k parameters) from scratch to classify chess pieces from 32√ó32 pixel square images. The model achieves ~99.85% accuracy on synthetic training data generated by combining 55 board styles (256√ó256px) with 64 piece sets (32√ó32px) from chess.com and lichess.

By rendering pieces onto different board backgrounds and extracting individual squares, the model learns robust piece recognition across various visual styles.

| Dataset                                                                                  | Accuracy | F1-Score (Macro) |
| ---------------------------------------------------------------------------------------- | :--------: | :----------------: |
| Test Data                                                                                | 99.85%   | 99.89%           |
| [S1M0N38/chess-cv-openboard](https://huggingface.co/datasets/S1M0N38/chess-cv-openboard) | -    | 95.78%           |

(OpenBoard has an unbalanced class distribution (many more samples for empty square class, so accuracy is not representative )

Happy to hear any feedback!
",0,3.0
57,post,MachineLearning,1nyr4l0,1759677929.0,"[D] Training a Vision model on a Text-Only Dataset using Axolotl
I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.

I am using Axolotl
https://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/llama-3-vision/lora-11b.yaml
in examples we have a sample .yaml file for this
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
# optionally might have model_type or tokenizer_type or processor_type
processor_type: AutoProcessor
# Automatically upload checkpoint and final model to HF
# hub_model_id: username/custom_model_name


# these 3 lines are needed for now to handle vision chat templates w images
skip_prepare_dataset: true
remove_unused_columns: false
sample_packing: false

chat_template: llama3_2_vision
datasets:
  - path: HuggingFaceH4/llava-instruct-mix-vsft
    type: chat_template
    split: train[:1%]
dataset_prepared_path:
val_set_size: 0.0
output_dir: ./outputs/out

adapter: lora
lora_model_dir:

sequence_len: 8192
pad_to_sequence_len: false

lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: 'model.language_model.layers.[\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'

wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 1
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

bf16: true
fp16:
tf32: true

gradient_checkpointing: true
logging_steps: 1
# flash_attention: true  # use for text-only mode
sdp_attention: true

warmup_ratio: 0.1
evals_per_epoch: 1
saves_per_epoch: 1
weight_decay: 0.0

# save_first_step: true  # uncomment this to validate checkpoint saving works with your config
```
based on which I have made a similar .yaml file

```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
processor_type: AutoProcessor
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer

# Vision-chat template handling
# skip_prepare_dataset: true
# remove_unused_columns: false
# sample_packing: false

chat_template: llama3_2_vision

datasets:
  - path: <path_to_dataset>
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    roles:
      system: 
        - system
      user: 
        - user
      assistant: 
        - assistant
    train_on_inputs: false

output_dir: <path_to_output_directory>

# Training parameters
sequence_len: 8192
pad_to_sequence_len: false
gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 1

optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002
weight_decay: 0.0
warmup_ratio: 0.1

# Precision & performance
bf16: true
fp16:
tf32: true

gradient_checkpointing: true
logging_steps: 1
flash_attention: true   # text-only mode
# sdp_attention: true

# Checkpointing
evals_per_epoch: 1
saves_per_epoch: 1
save_first_step: true
save_total_limit: 3

weight_decay: 0.0
special_tokens:
  pad_token: <|end_of_text|>

```

but when i run
`axolotl train config.yaml`
and I have processor_type:
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
processor_type: AutoProcessor
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer
```
I get the error
`KeyError: 'Indexing with integers is not available when using Python based feature extractors'`

but when i remove the field 
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer
```

or even
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
processor_type: AutoProcessor
tokenizer_config: <path_to_custom_tokenizer>

# Vision-chat template handling
skip_prepare_dataset: true
remove_unused_columns: false
sample_packing: false

```

I get the error
`AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'`

What happened here?
How does one do this?
Will this fine-tuning lead to loss of Vision Capabilities of the model?
Is there a guide to writing config.yaml files for different models?


Python Version: 3.12
Axolotl Version: Latest
Dataset: a .jsonl with 
```
{
	""messages"": 
	[
		{""role"": ""system"", ""content"": ""<system_prompt>""}, 
		{""role"": ""user"", ""content"": ""<question>""}, 
		{""role"": ""assistant"", ""content"": ""<answer>""}
	]
}
```
which was previously used to fine tune Llama3.1 8B using the following config.yaml

```
base_model: NousResearch/Meta-Llama-3.1-8B-Instruct
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer

chat_template: llama3
datasets:
  - path: <path_to_dataset>
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    roles:
      system:
        - system
      user:
        - user
      assistant:
        - assistant
train_on_inputs: false

output_dir: <path_to_output_directory>

sequence_len: 2048
sample_packing: true


gradient_accumulation_steps: 8
micro_batch_size: 2
num_epochs: 4

optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2e-5

bf16: auto
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
resume_from_checkpoint:
auto_resume_from_checkpoints: true
save_only_model: false


logging_steps: 1
flash_attention: true

warmup_ratio: 0.1
evals_per_epoch: 2
saves_per_epoch: 1
save_total_limit: 3
weight_decay: 0.0
special_tokens:
  pad_token: <|end_of_text|>
```

Thank you.I'm planning to fine-tune LLaMA 3.2 11B Instruct on a JSONL dataset of domain-specific question-answer pairs ‚Äî purely text, no images. The goal is to improve its instruction-following behavior for specialized text tasks, while still retaining its ability to handle multimodal inputs like OCR and image-based queries.

I am using Axolotl
https://github.com/axolotl-ai-cloud/axolotl/blob/main/examples/llama-3-vision/lora-11b.yaml
in examples we have a sample .yaml file for this
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
# optionally might have model_type or tokenizer_type or processor_type
processor_type: AutoProcessor
# Automatically upload checkpoint and final model to HF
# hub_model_id: username/custom_model_name


# these 3 lines are needed for now to handle vision chat templates w images
skip_prepare_dataset: true
remove_unused_columns: false
sample_packing: false

chat_template: llama3_2_vision
datasets:
  - path: HuggingFaceH4/llava-instruct-mix-vsft
    type: chat_template
    split: train[:1%]
dataset_prepared_path:
val_set_size: 0.0
output_dir: ./outputs/out

adapter: lora
lora_model_dir:

sequence_len: 8192
pad_to_sequence_len: false

lora_r: 32
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: 'model.language_model.layers.[\d]+.(mlp|cross_attn|self_attn).(up|down|gate|q|k|v|o)_proj'

wandb_project:
wandb_entity:
wandb_watch:
wandb_name:
wandb_log_model:

gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 1
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

bf16: true
fp16:
tf32: true

gradient_checkpointing: true
logging_steps: 1
# flash_attention: true  # use for text-only mode
sdp_attention: true

warmup_ratio: 0.1
evals_per_epoch: 1
saves_per_epoch: 1
weight_decay: 0.0

# save_first_step: true  # uncomment this to validate checkpoint saving works with your config
```
based on which I have made a similar .yaml file

```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
processor_type: AutoProcessor
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer

# Vision-chat template handling
# skip_prepare_dataset: true
# remove_unused_columns: false
# sample_packing: false

chat_template: llama3_2_vision

datasets:
  - path: <path_to_dataset>
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    roles:
      system: 
        - system
      user: 
        - user
      assistant: 
        - assistant
    train_on_inputs: false

output_dir: <path_to_output_directory>

# Training parameters
sequence_len: 8192
pad_to_sequence_len: false
gradient_accumulation_steps: 4
micro_batch_size: 1
num_epochs: 1

optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002
weight_decay: 0.0
warmup_ratio: 0.1

# Precision & performance
bf16: true
fp16:
tf32: true

gradient_checkpointing: true
logging_steps: 1
flash_attention: true   # text-only mode
# sdp_attention: true

# Checkpointing
evals_per_epoch: 1
saves_per_epoch: 1
save_first_step: true
save_total_limit: 3

weight_decay: 0.0
special_tokens:
  pad_token: <|end_of_text|>

```

but when i run
`axolotl train config.yaml`
and I have processor_type:
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
processor_type: AutoProcessor
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer
```
I get the error
`KeyError: 'Indexing with integers is not available when using Python based feature extractors'`

but when i remove the field 
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer
```

or even
```
base_model: alpindale/Llama-3.2-11B-Vision-Instruct
processor_type: AutoProcessor
tokenizer_config: <path_to_custom_tokenizer>

# Vision-chat template handling
skip_prepare_dataset: true
remove_unused_columns: false
sample_packing: false

```

I get the error
`AttributeError: 'MllamaTextSelfAttention' object has no attribute 'is_causal'`

What happened here?
How does one do this?
Will this fine-tuning lead to loss of Vision Capabilities of the model?
Is there a guide to writing config.yaml files for different models?


Python Version: 3.12
Axolotl Version: Latest
Dataset: a .jsonl with 
```
{
	""messages"": 
	[
		{""role"": ""system"", ""content"": ""<system_prompt>""}, 
		{""role"": ""user"", ""content"": ""<question>""}, 
		{""role"": ""assistant"", ""content"": ""<answer>""}
	]
}
```
which was previously used to fine tune Llama3.1 8B using the following config.yaml

```
base_model: NousResearch/Meta-Llama-3.1-8B-Instruct
tokenizer_config: <path_to_custom_tokenizer>
tokenizer_type: AutoTokenizer

chat_template: llama3
datasets:
  - path: <path_to_dataset>
    type: chat_template
    field_messages: messages
    message_property_mappings:
      role: role
      content: content
    roles:
      system:
        - system
      user:
        - user
      assistant:
        - assistant
train_on_inputs: false

output_dir: <path_to_output_directory>

sequence_len: 2048
sample_packing: true


gradient_accumulation_steps: 8
micro_batch_size: 2
num_epochs: 4

optimizer: paged_adamw_8bit
lr_scheduler: cosine
learning_rate: 2e-5

bf16: auto
tf32: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
resume_from_checkpoint:
auto_resume_from_checkpoints: true
save_only_model: false


logging_steps: 1
flash_attention: true

warmup_ratio: 0.1
evals_per_epoch: 2
saves_per_epoch: 1
save_total_limit: 3
weight_decay: 0.0
special_tokens:
  pad_token: <|end_of_text|>
```

Thank you.",0,3.0
58,post,MachineLearning,1nyfadh,1759640258.0,"[D] LLM Inference on TPUs
It seems like simple `model.generate()` calls are incredibly slow on TPUs (basically stuck after one inference), does anyone have simple solutions for using torch XLA on TPUs? This seems to be an ongoing issue in the HuggingFace repo.

I tried to find something the whole day, and came across solutions like optimum-tpu (only supports some models + as a server, not simple calls), using Flax Models (again supports only some models and I wasn't able to run this either), or sth that converts torch to jax and then we can use it (like ivy). But these seem too complicated for the simple problem, I would really appreciate any insights!!",22,14.0
59,post,MachineLearning,1ny6ol1,1759614801.0,"[D] Experiences with active learning for real applications?
I'm tinkering with an application of human pose estimation which [fails miserably](https://i.imgur.com/S0kVyPg.mp4) using off-the-shelf models/tools, as the domain is especially niche and complex compared to their training distribution. It seems there's no way around fine-tuning on in-domain images with manually-labeled keypoints (thankfully, I have thousands of hours of unlabelled footage to start from).

I've always been intrigued by active learning, so I'm looking forward to applying it here to efficiently sample frames for manual labeling. But I've never witnessed it in industry, and have only ever encountered [pessimistic takes on active learning in general](https://www.reddit.com/r/MachineLearning/comments/13elpm1/d_is_active_learning_a_hoax_or_the_future/) (not the concept ofc, but the degree to which it outperforms random sampling).

As an extra layer of complexity - it seems like a manual labeler (likely myself) would have to enter labels through a browser GUI. Ideally, the labeler should produce labels concurrently as the model trains on its labels-thus-far and considers unlabeled frames to send to the labeler. Suddenly my training pipeline gets complicated!

My current plan:
* Sample training frames for labeling according to variance in predictions between adjacent frames, or perhaps dropout uncertainty. Higher uncertainty should --> worse predictions
* For the holdout val+test sets (split by video), sample frames truly at random
* In the labeling GUI, display the model's initial prediction, and just drag the skeleton around
* Don't bother with concurrent labeling+training, way too much work. I care more about hours spent labeling than calendar time at this point.

I'd love to know whether it's worth all the fuss. I'm curious to hear about any cases where active learning succeeded or flopped in an industry/applied setting.

* In practice, when does active learning give a clear win over random? When will it probably be murkier?
* Recommended batch sizes/cadence and stopping criteria?
* Common pitfalls (uncertainty miscalibration, sampling bias, annotator fatigue)?",3,6.0
60,post,MachineLearning,1nxfyl8,1759538032.0,"[D] join pretraining or posttraining
Hello!

I have the possibility to join one of the few AI lab that trains their own LLMs.

Given the option, would you join the pretraining team or (core) post training team? Why so?",47,28.0
61,post,MachineLearning,1nxb9bp,1759525722.0,"[R] New paper shows that draws in LLM battles aren't what you think
Arena evals (e.g., Chatbot Arena) let users pick which model's response is better, or call it a draw. Most leaderboards then shove this into Elo, same as chess. The assumption: a draw = two models are equally strong. The paper [""Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation""](https://arxiv.org/abs/2510.02306) tests that assumption and proves it wrong:

* On 3 arena datasets, ignoring draws when updating ratings makes battle outcome prediction accuracy go **up 1-3%**, despite evaluation still *including draws*.
* Draws happen much more on **easy** or **objective** queries (risk ratios of 1.3x).

**Discussion seed:** If draws don't indicate skill parity and hence represent a poor fit for existing rating systems, how should we *actually* model them?

COI: Submitter is author.",35,20.0
62,post,MachineLearning,1nwwsk7,1759491767.0,"[P] I am building a ML job board
Hey fellow ML people!

Last year, I shared with you a job board for¬†[FAANG positions](https://www.reddit.com/r/MachineLearning/comments/1ia7feh/p_made_a_faang_job_postings_aggregator_for_ai/)¬†and due to the positive feedback I received, I had been working on expanded version called¬†[hire.watch](https://hire.watch/?categories=AI+_+Machine+Learning)

The goal is provide a unified search experience - it crawls, cleans and extracts data, allowing filtering by:

1. Full-text search
2. Location - on-site
3. Remote - from a given city, US state, EU, etc.
4. Category - you can check out the machine learning category here:¬†[https://hire.watch/?categories=AI+\_+Machine+Learning](https://hire.watch/?categories=AI+_+Machine+Learning)
5. Years of experience and seniority
6. Target gross salary
7. Date posted and date modified

I used the normal ML ecosystem (scikit learn, huggingface transformers, LLMs, etc.) to build it, and Plotly Dash for the UI.

Let me know what you think - feel free to ask questions and request features :)",20,3.0
63,post,MachineLearning,1nwoxqz,1759463539.0,"[R] New paper: LLMs don't have privileged self knowledge, which means we can efficiently train a General Correctness Model to predict the correctness of multiple models. Surprising or expected?
Quick paper highlight (adapted from TLDR thread):  
Finds no special advantage using an LLM to predict its own correctness (a trend in prior work), instead finding that LLMs benefit from learning to predict the correctness of many other models ‚Äì becoming a GCM.  
\--  
Training 1 GCM is strictly more accurate than training model-specific CMs for all models it trains on (including CMs trained to predict their own correctness).  
GCM transfers without training to outperform direct training on OOD models and datasets.  
GCM (based on Qwen3-8B) achieves +30% coverage on selective prediction vs much larger Llama-3-70B‚Äôs logits.

TLDR thread:¬†[https://x.com/hanqi\_xiao/status/1973088476691042527](https://x.com/hanqi_xiao/status/1973088476691042527)  
Full paper:¬†[https://arxiv.org/html/2509.24988v1](https://arxiv.org/html/2509.24988v1)

**Discussion Seed**:  
Previous works have suggested / used LLMs having self knowledge, e.g., identifying/preferring their own generations \[https://arxiv.org/abs/2404.13076\], or ability to predict their uncertainty. But paper claims specifically that LLMs don't have knowledge about their own *correctness.* Curious on everyone's intuition for what LLMs have / does not have self knowledge about, and whether this result fit your predictions.

Conflict of Interest:   
Author is making this post. ",30,12.0
64,post,MachineLearning,1nwfn4j,1759438244.0,"[R] Thesis direction: mechanistic interpretability vs semantic probing of LLM reasoning?
Hi all,

I'm an undergrad Computer Science student working or my senior thesis, and l'll have about 8 months to dedicate to it nearly full-time. My broad interest is in reasoning, and I'm trying to decide between two directions:

‚Ä¢ Mechanistic interpretability (low-level): reverse engineering smaller neural networks, analyzing weights/ activations, simple logic gates, and tracking learning dynamics.

‚Ä¢Semantic probing (high-level): designing behavioral tasks for LLMs, probing reasoning, attention/locality, and consistency of inference.

For context, after graduation I'll be joining a GenAl team as a software engineer. The role will likely lean more full-stack/frontend at first, but my long-term goal is to transition into backend.

I'd like the thesis to be rigorous but also build skills that will be useful for my long-term goal of becoming a software engineer. From your perspective, which path might be more valuable in terms that of feasibility, skill development, and career impact?

Thanks in advance for your advice!",11,13.0
65,post,MachineLearning,1nwaunk,1759427600.0,"[D] How much should researchers (especially in ML domain) rely on LLMs for their work?
Are ML researchers using LLMs like ChatGPT, Claude, or other open-source models to generate, test, or refine minor ideas as tweaks to their original research, or to ask big-picture questions about their overall plans? In what other ways are publishing researchers using LLMs to support their work? (Of course, I don‚Äôt mean those who literally ask ChatGPT to write a paper from scratch.)

I sometimes feel guilty when I feed a paper into ChatGPT and ask it to summarize or even extract ‚Äúideas‚Äù from it, which I then try to combine with my own. I want to understand where a researcher should draw the line in using LLMs in their daily workflow, so as not to fool themselves into believing they are doing good research while over-relying on the tool.",49,60.0
66,post,MachineLearning,1nw6jqf,1759418073.0,"[R] Maths PhD student - Had an idea on diffusion
I am a PhD student in Maths - high dimensional modeling. I had an idea for a future project, although since I am not too familiar with these concept, I would like to ask people who are, if I am thinking about this right and what your feedback is. 

Take diffusion for image generation. An overly simplified tldr description of what I understand is going on is this. Given pairs of (text, image) in the training set, the diffusion algorithm learns to predict the noise that was added to the image. It then creates a distribution of image concepts in a latent space so that it can generalize better. For example, let's say we had two concepts of images in our training set. One is of dogs eating ice cream and one is of parrots skateboarding. If during inference we asked the model to output a dog skateboarding, it would go to the latent space and sample an image which is somewhere ""in the middle"" of dogs eating ice cream and parrots skateboarding. And that image would be generated starting from random noise. 

So my question is, can diffusion be used in the following way? Let's say I want the algorithm to output a vector of numbers (p) given an input vector of numbers (x), where this vector p would perform well based on a criterion I select. So the approach I am thinking is to first generate pairs of (x, p) for training, by generating ""random"" (or in some other way) vectors p, evaluating them and then keeping the best vectors as pairs with x. Then I would train the diffusion algorithm as usual. Finally, when I give the trained model a new vector x, it would be able to output a vector p which performs well given x. 

  
Please let me know if I have any mistakes in my thought process or if you think that would work in general. Thank you.",27,52.0
67,post,MachineLearning,1nvxswc,1759392244.0,"[D] I‚Äôm looking for papers, preprints, datasets, or reports where an LLM is trained to only know what humans knew before a major scientific breakthrough, and is then asked to propose a new theoretical frameworkwithout using post-breakthrough knowledge and without requiring experimental validation.
Imagine we train (or fine-tune) an LLM exclusively on physics texts up to 1904‚ÄîMaxwell, Lorentz, Poincar√©, Michelson‚ÄìMorley, etc.‚Äîand then ask it to produce a theory addressing the known tensions (e.g., invariance of c, simultaneity). The goal isn‚Äôt to re-derive Einstein verbatim or to validate anything in the lab, but to test whether an LLM can elaborate a novel, coherent theoretical structure from historically available knowledge.

I‚Äôm interested in any domain, not just relativity: e.g., pre-quantum physics, pre-DNA biology, early group theory, early materials science, etc.

What would count as ‚Äúon topic‚Äù:

Pretraining from scratch or continual pretraining on a historically filtered corpus (time-sliced).

Strong leakage controls: no access to post-cutoff texts; possibly knowledge unlearning.

Evaluation focused on novelty + internal coherence (not experimental truth): e.g., CAS/proof-assistants for consistency, reviewers for ‚Äúhistorical plausibility.‚Äù

Comparisons vs. baselines like RAG-only setups or modern LLMs that ‚Äúalready know‚Äù the breakthrough.

Reports of failure modes (e.g., the model just paraphrases Lorentz/Poincar√©, or smuggles modern terms).

Why I‚Äôm asking:

I‚Äôve seen adjacent work (LLM-aided conjecture generation, symbolic regression discovering equations, RL systems finding new algorithms), but not a clean ‚Äúpre-discovery epistemology‚Äù experiment with strict temporal cutoffs.



Tagging folks who might have seen or worked on something like this:

u/hardmaru ¬∑ u/MysteryInc152 ¬∑ u/Qyeuebs ¬∑ u/StartledWatermelon ¬∑ u/Playful_Peace6891 ¬∑ u/SatoshiNotMe ¬∑ u/Ch3cks-Out ¬∑ u/NuclearVII



If you know of:

peer-reviewed papers, arXiv preprints, theses

datasets/corpora curated by historical cutoff

code or replication packages

‚Ä¶please share!

Thanks in advance üôè",58,11.0
68,post,MachineLearning,1nvwkdt,1759387562.0,"[D] The job market is weird
Would love to get people‚Äôs thoughts on the current job market. Simultaneously,  it seems a lot of companies aren‚Äôt hiring, a lot of start ups are hiring and there are a lot of people in the market. 

Also this is the first time I‚Äôve seen so many companies only offer Staff positions. 

How is everyone feeling right now? ",63,26.0
69,post,MachineLearning,1nvvdvl,1759383259.0,"[D] Open source projects to contribute to as an ML research scientist
Hey everyone,  
I have a few publications and patents and I work for a tier 2 company as Research scientist. Lately all my job applications have been rejected on the spot. Not even a first interview. I want to beef up my coding skills and be more attractive to employers. Maybe not having a huge github presence is hindering my prospects.  
  
Can u please suggest opensource projects like SGLang or vLLm which I can contribute to? Any starting pointers?

Edit- treasure trove of comments below for any RS or MLE trying to get into faang. Thanks community.",115,40.0
70,post,MachineLearning,1nvj5hn,1759349628.0,"[D] Anyone here using LLM-as-a-Judge for agent evaluation?
I‚Äôve been experimenting with using another LLM to *score* my agent‚Äôs responses (accuracy / groundedness style) instead of relying on spot-checking.

Surprisingly effective ‚Äî but only when the judge prompt is written carefully (single criterion, scoring anchors, strict output format, bias warnings, etc.)

Curious if anyone else here is doing this? Any lessons learned?

(I wrote a short breakdown of what worked for us ‚Äî happy to share if useful.)",0,13.0
71,post,MachineLearning,1nuwj5t,1759285839.0,"[D] Monthly Who's Hiring and Who wants to be Hired?
**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",14,3.0
72,post,MachineLearning,1ntm0pf,1759161817.0,"[R] No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping
Arxiv:¬†[https://arxiv.org/pdf/2509.21880](https://arxiv.org/pdf/2509.21880)

Huggingface paper:¬†[https://huggingface.co/papers/2509.21880](https://huggingface.co/papers/2509.21880)

I‚Äôve been working on improving the reasoning abilities of large language models, and I wanted to share something I‚Äôm really excited about. Reinforcement Learning with Verifiable Rewards (RLVR) is already a powerful framework, but I noticed a gap: current methods like GRPO only use problems where model responses differ in correctness. They completely ignore the so-called ‚Äúzero-variance prompts‚Äù ‚Äî cases where all responses receive the same reward.

At first glance, these prompts look useless, but I started wondering if they actually contain valuable learning signals. That led me to develop¬†**RL with Zero-Variance Prompts (RL-ZVP)**. Instead of discarding those prompts, RL-ZVP extracts meaningful feedback from them. It directly rewards correctness and penalizes errors without needing contrasting responses, and it uses token-level entropy to guide the advantage shaping.

We evaluated RL-ZVP on six math reasoning benchmarks, and it delivered some really promising results ‚Äî up to¬†**8.61 points higher accuracy**¬†and¬†**7.77 points higher pass rates**¬†compared to GRPO. It also consistently outperformed other baselines that just filter out zero-variance prompts.

I am happy to take comments in this sub and the HuggingFace paper.",35,6.0
73,post,MachineLearning,1nsvdqk,1759083348.0,"[D] Machine learning research no longer feels possible for any ordinary individual. It is amazing that this field hasn't collapsed yet.
Imagine you're someone who is attempting to dip a toe into ML research in 2025. Say, a new graduate student.

You say to yourself ""I want to do some research today"". Very quickly you realize the following:

**Who's my competition?**

Just a handful of billion-dollar tech giants, backed by some of the world's most powerful governments, with entire armies of highly paid researchers whose only job is to discover interesting research questions. These researchers have access to massive, secret knowledge graphs that tell them exactly where the next big question will pop up before anyone else even has a chance to realize it exists. Once LLMs mature even more, they'll probably just automate the process of generating and solving research problems. What's better than pumping out a shiny new paper every day?

**Where would I start?**

Both the Attention and the ADAM paper has 200k citation. That basically guarantees there‚Äôs no point in even trying to research these topics. Ask yourself what more could you possibly contribute to something that‚Äôs been cited 200,000 times. But this is not the only possible topic. Pull out any topic in ML, say image style transfer, there are already thousands of follow-up papers on that. Aha, maybe you could just read the most recent ones from this year. Except, you quickly realize that most of those so-called ‚Äúpapers‚Äù are from shady publish-or-perish paper-mills (which are called ""universities"" nowadays, am I being too sarcastic?) or just the result of massive GPU clusters funded by millions of dollars instant-access revenue that you don‚Äôt have access to.

**I‚Äôll just do theory!**

Maybe let's just forget the real world and dive into theory instead. But to do theory, you‚Äôll need a ton of math. What‚Äôs typically used in ML theory? Well, one typically starts with optimization, linear algebra and probability. But wait, you quickly realize that‚Äôs not enough. So you go on to master more topics in applied math: ODEs, PDEs, SDEs, and don‚Äôt forget game theory, graph theory and convex optimization. But it doesn‚Äôt stop there. You‚Äôll need to dive into Bayesian statistics, information theory. Still isn‚Äôt enough. Turns out, you will need pure math as well: measure theory, topology, homology, group, field, and rings. At some point, you realize this is still not enough and now you need to think more like Andrew Wiles. So you go on to tackle some seriously hard topics such as combinatorics and computational complexity theory. What is all good for in the end? Oh right, to prove some regret bound that absolutely no one cares about. What was the regret bound for ADAM again? It's right in the paper, Theorem 1, cited 200k times, and nobody as far as I'm aware of even knows what it is.",73,55.0
74,post,MachineLearning,1nrvlle,1758980620.0,"[P] Sample Forge - Research tool for deterministic inference and convergent sampling parameters in large language models.
Hi folks, I made a research tools that allows you to perform deterministic inference on any local large language model. This way you can test any variable changes and see for yourself the affects those changes have on the output of the LLM's response.  It also allows you to perform automated reasoning benchmarking of a local language model of your choice, this way you can measure the perplexity drop of any quantized model or differences between reasoning capabilities of models or sampling parameters. It also has a fully automated way of converging on the best sampling parameters for a given model when it comes to reasoning capabilities. I made 2 videos for the project so you can see what its about at a glance the main guide is here https://www.youtube.com/watch?v=EyE5BrUut2o, the instillation video is here https://youtu.be/FJpmD3b2aps and the repo is here https://github.com/manfrom83/Sample-Forge. If you have more questions id be glad to answer them here. Cheers.",3,0.0
75,post,MachineLearning,1nr972b,1758911742.0,"[D] Anyone hear back from NeurIPS Creative AI track?
The website says decisions out September 18 but I still haven‚Äôt see any reviews or notifications. Anyone else hearing back from it?",2,0.0
76,post,MachineLearning,1nqosof,1758850889.0,"[R] Is there any research on using LLMs as Loss Functions?
Let‚Äôs say you were training a generative model for a task like summarization or answering questions. Would it be possible to feed that output into an LLM and ask it to assess the model‚Äôs effectiveness at performing the task and then maybe feed that output into a sentiment analysis model to obtain a score for how well the model did and have the model attempt to maximize that score?",0,20.0
77,post,MachineLearning,1nqkwn4,1758840024.0,"[P] How to Check If Your Training Data Is Representative: Using PSI and Cramer‚Äôs V in Python
https://preview.redd.it/3m7n4tnu1erf1.png?width=1536&format=png&auto=webp&s=29a717573ec6d3a8d07440b17bd98bf1452ce9a6

Hi everyone,

I‚Äôve been working on a guide to evaluate **training data representativeness** and detect dataset shift. Instead of focusing only on model tuning, I explore how to use two statistical tools:

* **Population Stability Index (PSI)** to measure distributional changes,
* **Cramer‚Äôs V** to assess the intensity of the change.

The article includes explanations, Python code examples, and visualizations. I‚Äôd love feedback on whether you find these methods practical for real-world ML projects (especially monitoring models in production).  
Full article here: [https://towardsdatascience.com/assessment-of-representativeness-between-two-populations-to-ensure-valid-performance-2/](https://towardsdatascience.com/assessment-of-representativeness-between-two-populations-to-ensure-valid-performance-2/)",12,2.0
78,post,MachineLearning,1nqil0w,1758834220.0,"[R] How to finetune a multimodal model?
I am working on a project in which we are tasked with developing anomaly detection for a technical system.

Until now, I have mainly worked with LLMs and supplied them with external knowledge using RAG.

Now I have to work with a multimodal model and train it to detect anomalies (e.g scratches, broken glass) in a technical system based on images. I was thinking of using Gemma3:4b as the model, but I will evaluate this in more detail as I go along.

To do this, I would have to train this model accordingly for this use case, but I'm not quite sure how to proceed. All I know is that a large amount of labeled data is required.

So I would like to ask what the procedure would be, which tools are commonly used here, and whether there is anything else to consider that I am not currently aware of.",20,18.0
79,post,MachineLearning,1nq856v,1758810214.0,"[R] ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution
We released ShinkaEvolve, a new state-of-the-art and fully open-source framework for program optimization, which we specifically designed to be easily integrated into any scientific codebase.

Open source code:[ https://github.com/SakanaAI/ShinkaEvolve](https://github.com/SakanaAI/ShinkaEvolve)

Technical report:[ https://arxiv.org/abs/2509.19349](https://arxiv.org/abs/2509.19349)

Blog:[ https://sakana.ai/shinka-evolve/](https://sakana.ai/shinka-evolve/)

You can start playing with ShinkaEvolve without even downloading any code, all inside a remote Google Colab instance:[ https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka\_tutorial.ipynb](https://colab.research.google.com/github/SakanaAI/ShinkaEvolve/blob/main/examples/shinka_tutorial.ipynb)

In our technical report, we show how ShinkaEvolve can be easily applied across different problem domains. On the canonical circle packing task, ShinkaEvolve discovers a new solution with state-of-the-art performance beyond the recent closed-source AlphaEvolve using only 150 program evaluations. We even apply ShinkaEvolve to small-scale LLM pretraining, discovering a new load-balancing loss for MoE architectures with remarkable stabilization properties.

ShinkaEvolve also comes with a detailed and lightweight WebUI to monitor its discoveries in real-time!",24,1.0
80,post,MachineLearning,1nplmr8,1758742295.0,"Apple Research Debuts Manzano ‚Äî a Unified Multimodal LLM
**üÜï What‚Äôs New**  
  
Apple research just introduced Manzano (Spanish for ‚Äúapple tree‚Äù üçè) ‚Äî a unified multimodal LLM that both understands images and generates them inside the same autoregressive loop.  
Instead of separate perception and generation models, one decoder predicts the next token ‚Äî text or image ‚Äî then renders pixels with an auxiliary diffusion decoder.  
The paper reports state-of-the-art results among unified models and competitive performance against specialist systems, especially on text-rich benchmarks.  
  
**‚öôÔ∏è How It Works**  
  
Hybrid vision tokenizer in front of the LLM: a single vision encoder feeds two lightweight adapters producing continuous embeddings for understanding and discrete tokens for generation.  
  
The unified LLM decoder accepts text tokens and/or image embeddings and auto-regressively predicts the next token; a diffusion image decoder turns predicted tokens into pixels.  
  
Three-stage training (pre-training ‚Üí continued pre-training ‚Üí SFT) on mixed text/vision data; the embedding table is extended with a 64K image-token codebook aligned by finite scalar quantization.  
  
**‚ú® What Makes It Distinct**  
  
Hybrid tokenizer, single encoder: understanding and generation tokens come from one encoder in a shared semantic space (no dual-tokenizer conflict).  
  
Decoupled roles: the LLM decoder handles high-level semantics; the diffusion decoder handles pixel fidelity ‚Äî letting each scale independently.  
  
Explicit scaling: LLM decoder scaled from 300M‚Üí30B params with steady gains; diffusion decoder scaled for stronger structure in human evals.  
  
**üìå Why It Matters**  
  
One model for ‚Äúsee + draw‚Äù ‚Üí simpler architecture, better language‚Äìvision alignment, easier product integration.  
  
Shared encoder + decoupled renderer ‚Üí a practical path to scale without sacrificing understanding (a weak point for earlier unified models).  
  
If these results generalize, future assistants that read, reason, edit & generate in one loop could become the new default for multimodal work.",57,8.0
81,post,MachineLearning,1npdfh1,1758723618.0,"[D] Is senior ML engineering just API calls now?
I‚Äôm a Senior ML engineer with around 9 years of experience. I work at a large government institution, implementing (integrating?) AI for cybersecurity, and I‚Äôm currently in the process of building a new team.

I‚Äôve been having some concerns about my career development, and I‚Äôm not sure if other ML engineers with similar experience feel the same way.

Most of my projects these days aren‚Äôt really ‚Äúmachine learning‚Äù anymore. It‚Äôs mostly using existing models through APIs, setting up pipelines, etc. The actual algorithmic/experimental side of ML feels like it‚Äôs disappearing from my day-to-day work.

It seems like the industry has shifted from building models to API calls and prompt engineering. I miss the kind of work I did in my earlier roles, building models from scratch, fine-tuning, experimenting‚Ä¶

So my question is: is this just what senior ML roles eventually turn into? Has the job really shifted from ‚Äúbuilding ML‚Äù to ‚Äúplugging in ML‚Äù? Curious if others are experiencing the same thing. I have been experiencing this since the generative AI boom where suddenly everything was solvable..

(Disclaimer: we do use on-prem models at my organization, so I still get some hands-on time with models and fine-tuning using LoRA.)",391,185.0
82,post,MachineLearning,1np483r,1758692652.0,"[D] Training smaller LLM for Agentic tasks.
So I have a specific use case, in which Deepseek-v3.1 works well, but it's simply too big and takes time to load on our GPU (everything runs locally in my organization, we have¬†**16 H100 GPUs**¬†and maybe about¬†**8 more A100s**) .I use Ollama since I can‚Äôt keep VLLM loaded across all GPUs without hogging resources that others need.

What I want is a¬†**smaller model**¬†that I can use for an¬†**agentic task**¬†mainly to work with a set of custom MCP tools I‚Äôve built.

The biggest reason I want to build a model of my own is because I can get one hell of an education in the process, and since the hardware is already in-house (and mostly idle), I figured this is the perfect opportunity.

But I‚Äôm not sure where to start:

1. Should I train a model from scratch, or take an existing pretrained model and fine-tune?
2. What base architecture would be a good starting point for agent-style tasks?

If anyone can point me toward resources specifically focused on¬†**training or finetuning models for agentic tasks**, I‚Äôd really appreciate it.

P.S: I am currently using full precision deepseek-v3.1 (671B). I am thinking of a model which is about the size of gpt oss.",0,6.0
83,post,MachineLearning,1nop33m,1758651661.0,"[R] Keeping AI usage (cost control) sustainable and compliant (governance)?
Wondering what approaches teams are taking to keep usage manageable, not just in terms of cost, but also in governance. Have you found frameworks that enforce guardrails across both spend and compliance?",0,7.0
84,post,MachineLearning,1noo2rz,1758649407.0,"[R] PhD in Physics, now in industry. How do I get back into GenAI research?
Hello Reddit,

I'm a PhD physicist with an academic background in computational methods and couple years of experience applying them in a commercial R&D setting. My current work focuses on using Flow Matching and Diffusion Models for physics simulations, which is a fascinating area itself.

The challenge I'm facing is that my current role is heavily focused on code development and deploying of existing models, with little opportunity for original, in-depth research. I have a number of research ideas related to GenAI Diffusion/Flow-based models across different modalities, but my company's priorities are focused on rapid deployment, not fundamental research.

I'm looking to transition into a more research-oriented role where I can experiment, study, and pursue these and some else's ideas. I'm open to both academic and industrial opportunities.

My question to the community is:

* What grants, universities, or research institutions could I pursuit?
* Do you know of any specific labs, orgs or companies known for their work on Flow Matching/Diffusion models for scientific or physical applications with a research agenda?
* For those who have made a similar transition from (say industry) to a more research-focused industry role, what advice do you have? Are there specific resources or networks I should tap into?

Any advice or leads would be greatly appreciated. Thank you!",32,4.0
85,post,MachineLearning,1nomop4,1758646256.0,"[D] What‚Äôs your tech stack as researchers?
Curious what your workflow looks like as scientists/researchers (tools, tech, general practices)?

I feel like most of us end up focusing on the science itself and unintentionally deprioritize the research workflow. I believe sharing experiences could be extremely useful, so here are two from me to kick things off:


Role: AI Researcher (time-series, tabular)
Company: Mid-sized, healthcare 
Workflow: All the data sits in an in-house db, and most of the research work is done using jupyter and pycharm/cursor.
We use MLFlow for experiment tracking.
Resources are allocated using run.ai (similiar to colab).
Our workflow is generally something like: exporting the desired data from production db to s3, and research whatever. Once we have a production ready model, we work with the data engineers towards deployment (e.g ETLs, model API). Eventually, model outputs are saved in the production db and can be used whenever.
  

Role: Phd student
Company: Academia research lab
Workflow: Nothing concrete really, you get access to resources using a slurm server, other than that you pretty much on your own.
Pretty straightforward python scripts were used to download and preprocess the data, the processed data was spilled directly into disk.
A pretty messy pytorch code and several local MLFlow repos.


There‚Äôre still many components that I find myself implement from scratch each time, like EDA, error analysis, production monitoring (model performance/data shifts). Usually it is pretty straightforward stuff which takes a lot of time and it feels far from ideal.

What are your experiences?",50,21.0
86,post,MachineLearning,1nok8yy,1758640785.0,"[P] SyGra: Graph-oriented framework for reproducible synthetic data pipelines (SFT, DPO, agents, multimodal)
**TL;DR.** We open-sourced **SyGra**, a graph-oriented framework for building *reproducible* synthetic data pipelines. Pipelines are defined as graphs (nodes = LLM calls/transforms/samplers; edges = conditional/parallel/loops). Two modes: YAML + CLI or Python library. Integrates with vLLM, HF TGI, Azure OpenAI, Ollama; HF-native I/O (streaming), provenance, schema-aware outputs.

**Motivation.** High-quality LLM datasets are scarce, costly, and often sensitive; teams also need fine-grained control over task structure (SFT/DPO, tool use, multi-agent, multimodal). In practice, scaling ‚Äúnotebook pipelines‚Äù breaks down: you end up hand-wiring branching/looping flows, juggling multiple inference backends/APIs, and doing ad-hoc validation/schema checks‚Äîwithout resumability, sharding, or streaming. We wanted a **unified, reusable graph abstraction** that captures how data work actually happens (nodes/edges, subgraphs), automates **quality tagging** (heuristics + LLM-based scoring), and emits **schema-conformant, OASST-style** records‚Äîso teams can reproduce, audit, and evolve pipelines instead of rewriting glue code.

**Design.**

* **Graph model:** reusable subgraphs, branching, loops; deterministic configs
* **Execution:** pluggable model clients (vLLM/TGI/Azure/Ollama), Triton-compatible
* **Data I/O:** Hugging Face datasets (streaming), local files; schema & metadata tracking
* **Reproducibility:** explicit configs, seeds, artifact paths; CLI runs are fully logged

**Use cases.** Bootstrapping SFT/DPO datasets; agent simulation & tool-use evals; multimodal assembly (image‚ÜíQ&A, audio‚Üítext) etc.

**Links:**

* Code (Apache-2.0) & README: [github.com/ServiceNow/SyGra](http://github.com/ServiceNow/SyGra)
* Paper (design rationale, examples): [arxiv.org/abs/2508.15432](http://arxiv.org/abs/2508.15432)
* PyPI: [pypi.org/project/sygra/](http://pypi.org/project/sygra/)

**Disclosure.** I‚Äôm part of the team. Feedback, issues, and PRs welcome.",9,2.0
87,post,MachineLearning,1noi5hr,1758635942.0,"[D] Do we overestimate the need for custom models?
I keep noticing that in practice, many problems don‚Äôt actually require training a new model. Pretrained models (Hugging Face, OpenAI, etc.) often get you most of the way there, and the real work is in data prep, deployment, and monitoring.

Yet, I still see teams sinking months into custom architectures when a good baseline would have been enough.

Do you think we (as a field) over-engineer solutions instead of focusing on what actually ships?",0,7.0
88,post,MachineLearning,1no4a1m,1758590286.0,"NVIDIA $100B OpenAI investment [D]
Do you guys think this is even a good investment at this point? I feel like OpenAI is so inflated and also feel like the math of all these recent AI fundraises doesn‚Äôt even make sense anymore. I feel like the bubble is close to popping.",35,19.0
89,post,MachineLearning,1nnh6gi,1758530752.0,"[D] Mixture of Attention?
 considering a new transformer architecture (for protein/DNA models but feel free to weight in from a language perspective) and I‚Äôd love some input before I do any experimenting (low budget this semester)

The current leading edge of efficient LLMs appear to be mixtures of experts, with a number of quadratic attention layers swapped out for linear layers (IBM granite 4.0, qwen-next for ex).

NVIDIA even has a paper out replacing quadratic attention with linear layers on pre-trained models (https://arxiv.org/abs/2508.15884 ).

So I wonder if it would be feasible to freeze a model after pre-training (all attention quadratic), one by one training a linear substitute for each quadratic layer.

Then either based on external rules (context length, compute constraint) decide when and how many layers are flicked to linear. Or, train a router with an objective to maximize response quality, keeping generation speed up, while minimizing cost.

Either way you‚Äôd have a single model, with fairly coherent tone and knowledge, that based deployment constraints (speed requirements, memory/compute limits) can be adjusted to be more, or less, linear on the fly.",6,7.0
90,post,MachineLearning,1nngswn,1758529244.0,"[D] Semantic image synthesis state-of-the-art?
Hi everyone. I've never done this, so decided to post.

I'm looking to create black-and-white images of satellite photos of rivers, from skeletons of river images. Basically I have a dataset where I have \[satellite\_river\_photo, skeleton\_segmentation\] pairs, and I want to train a generator to do skeleton->satellite generations from new unseen skeletons. Having an extra conditioning variable would also be of interest, but not necessarily at the beginning.

Since most of the literature in this area is over 6 years old, I wanted to post and see if anyone in this community has done something similar lately and would be able to provide some guidance and what methods would be the best to start with or what papers to look at. Thanks.",3,3.0
91,post,MachineLearning,1nl8ik3,1758299859.0,"[Project] I created an AI photo organizer that uses Ollama to sort photos, filter duplicates, and write Instagram captions.
Hey everyone at r/MachineLearning,

I wanted to share a Python project I've been working on called the **AI Instagram Organizer**.

**The Problem:** I had thousands of photos from a recent trip, and the thought of manually sorting them, finding the best ones, and thinking of captions was overwhelming. I wanted a way to automate this using local LLMs.

**The Solution:** I built a script that uses a multimodal model via Ollama (like LLaVA, Gemma, or Llama 3.2 Vision) to do all the heavy lifting.

**Key Features:**

* **Chronological Sorting:** It reads EXIF data to organize posts by the date they were taken.
* **Advanced Duplicate Filtering:** It uses multiple perceptual hashes and a dynamic threshold to remove repetitive shots.
* **AI Caption & Hashtag Generation:** For each post folder it creates, it writes several descriptive caption options and a list of hashtags.
* **Handles HEIC Files:** It automatically converts Apple's HEIC format to JPG.

It‚Äôs been a really fun project and a great way to explore what's possible with local vision models. I'd love to get your feedback and see if it's useful to anyone else!

**GitHub Repo:** [https://github.com/summitsingh/ai-instagram-organizer](https://github.com/summitsingh/ai-instagram-organizer)

Since this is my first time building an open-source AI project, any feedback is welcome. And if you like it, a star on GitHub would really make my day! ‚≠ê",0,0.0
92,post,MachineLearning,1nkz9s9,1758275455.0,"[R] Looking for Real‚ÄëTime Social Media Data Providers with Geographic Filtering, your finds are Welcome?
I‚Äôm working on a **social listening tool** and need access to **real‚Äëtime (or near real‚Äëtime)** social media datasets. The key requirement is the ability to **filter or segment data by geography** (country, region, or city level).

I‚Äôm particularly interested in:

* Providers with **low latency** between post creation and data availability
* Coverage across multiple platforms (Twitter/X, Instagram, Reddit, YouTube, etc.)
* Options for **multilingual content**, especially for non‚ÄëEnglish regions
* APIs or data streams that are **developer‚Äëfriendly**

If you‚Äôve worked with any vendors, APIs, or open datasets that fit this, I‚Äôd love to hear your recommendations, along with any notes on **pricing, reliability, and compliance** with platform policies.",0,0.0
93,post,MachineLearning,1nkxejt,1758268187.0,"Overcoming accuracy limitations of Analog In-Memory Computing hardware
Our paper titled ""Analog Foundation Models"" from IBM Research and ETH Zurich just got accepted at NeurIPS, and I feel like the broader ML community is not aware of the potential Analog In-Memory Computing (AIMC) has, so I wanted to make a quick advertisement for the paper and the field as a whole.

The idea of using analog devices for computation in AI is pretty old, but never really took off because of many reasons such as scalability or complexity. However, recently, research labs from Stanford or IBM Research have demonstrated very simple and scalable Analog In-Memory Computing chips that have strong potential to harness the benefits of AIMC \[1-3\].

**What's the problem with modern architectures such as GPUs?**  
In a conventional computer architecture, you have your memory and your processing unit separated by a bus, over which you send data back and forth. This is extremely power consuming especially in scenarios where you repeatedly need to access \*a lot of data\*. This is the case for LLMs: During inference, you need to constantly fetch the weights, KV cache, and activations from DRAM into your local SRAM-based caches, do the computation, and eventually write back the data to DRAM. This is really expensive in terms of power and latency.  
  
**Can't we get rid of DRAM (only use SRAM)?**  
Yes we can, and in fact there are some companies that are already doing that (e.g. Cerebras). The downside of this approach is that SRAM has very poor density (and does not scale anymore) and cannot hold billions of weights in a reasonable footprint (you need huge wafers, and many of them).

**How about you just do the computation directly inside a very dense memory itself?**  
This is the idea of AIMC: We propose to take the matrix-vector multiplication operation (one of the most prominent ops in NNs) and execute it directly inside non-volatile memory using Ohm's law (multiplication) and Kirchhoff's current law (summation). When combined with a scalable 3D memory technology like 3D NAND Flash and a scalable model architecture like MoEs, this opens up completely new use-cases for AI because you will be able to serve 100B+ models on a single chip with a low power budget (10s of W)\[4\].

**What's the catch?**  
There is always one...In the case of AIMC, it is the fact that computations are noisy and non-deterministic at runtime. In fact, up to now, no one was sure whether LLMs can be made robust to the noise present in AIMC-based hardware. Our paper ""Analog Foundation Models"" \[5\] changes this. We show that we can repeat the pre-training process of already pre-trained foundation models on synthetic data while using hardware-aware training methods to enhance the robustness of these LLMs.

We show that in terms of accuracy, we can now compete with 4-bit quantized LLMs!

This is a significant step towards making AIMC a reality and there is still a long way to go, but we're still super excited to have broken this barrier, which is why I wanted to introduce this to the broader ML community here!

Do you want to get an intro to this topic? Then I suggest [this fundamental article](https://www.nature.com/articles/s41565-020-0655-z).

Do you want to chat with me virtually or at NeurIPS? Just DM me!

\[1\] [https://www.nature.com/articles/s41586-022-04992-8](https://www.nature.com/articles/s41586-022-04992-8)  
\[2\] [https://www.nature.com/articles/s41586-023-06337-5](https://www.nature.com/articles/s41586-023-06337-5)  
\[3\] [https://www.nature.com/articles/s41928-023-01010-1](https://www.nature.com/articles/s41928-023-01010-1)  
\[4\] [https://www.nature.com/articles/s43588-024-00753-x](https://www.nature.com/articles/s43588-024-00753-x)  
\[5\] [https://arxiv.org/pdf/2505.09663](https://arxiv.org/pdf/2505.09663)",33,11.0
94,post,MachineLearning,1nkrmzr,1758248911.0,"[R] NeurIPS rejected paper resubmission
My paper just got rejected (scores: 4, 4, 3, 3). I‚Äôm considering resubmitting it to IEEE SatML. What‚Äôs your opinion on SatML? Would it be better to aim for a journal like IEEE TIFS instead? Any other recommendations? I‚Äôm not really interested in ICLR since I feel it might get rejected there too. Field: AI Security.",29,15.0
95,post,MachineLearning,1nkge98,1758220451.0,"[R] Live Sound and Pro Audio in AI/ML
I‚Äôm currently in the middle of a Post Graduate Program for AI/ML at UT Austin and have had a blast learning the fundamentals and theory of how this tech works. I have an 8 year background as a Live Sound Engineer working in concert audio and have currently been researching how ML can Optimize PA placement, SPL measurements, STI ratings for different event applications or installs.

I‚Äôm curious to see if anybody else out there in the world is currently doing research that combines AI/ML with Live Sound and Pro Audio. If so, what are you researching? What type of models are you creating?

Just Curious and would love to connect with others that share the same passion.",5,1.0
96,post,MachineLearning,1nkdbin,1758213603.0,"[P] We built mmore: an open-source multi-GPU/multi-node library for large-scale document parsing
We are a student group from EPFL and we have been working on a tool called mmore, and thought it might be useful to share it here. Maybe the community will find it useful.

You can think of mmore as something in the spirit of [Docling](https://github.com/docling-project/docling), but designed from the ground up to run natively on multi-GPU and multi-node setups. As the backend OCR for PDFs (and images) we use [Surya](https://github.com/datalab-to/surya), which we‚Äôve found to be both very accurate and fast. For those with limited GPU resources, we also provide a lightweight ‚Äúfast‚Äù mode. It skips OCR (so it cannot process scanned files) but still works well for born-digital documents.

In a [paper](https://www.arxiv.org/pdf/2509.11937) we released a few months ago, we showed that mmore achieves both speed and accuracy gains over Docling (maybe this has changed by now with the latest Granite-Docling). Right now, it supports a broad range of formats: PDFs, DOCX, PPTX, XLSX, MD, EML (emails), TXT, HTML, as well as videos and audio (MP4, MOV, AVI, MKV, MP3, WAV, AAC).

The use cases are flexible. For example:

* Unlocking text and image data from previously unprocessed files, enabling larger dataset creation (similar to what Docling + HuggingFace did a few days ago with [finepdfs](https://huggingface.co/datasets/HuggingFaceFW/finepdfs)).
* Running text or multimodal RAG directly over your own document collections.

We are sharing this mainly to invite ideas and feedback from the community. If you see opportunities, have suggestions, or even just thoughts on directions we should explore, we‚Äôd love to hear them. Contributions are more than welcome!

Github: üíªhttps://github.com/swiss-ai/mmore  
Arxiv: üìÑhttps://www.arxiv.org/pdf/2509.11937",30,1.0
97,post,MachineLearning,1nka2g3,1758206296.0,"[P] Built a CLI to turn PDFs and docs into fine tuning datasets
Hi everyone,

I have been working on a small CLI that takes local files like pdfs docs or text and turns them into datasets you can use for fine tuning.

Repo:¬†[https://github.com/Datalore-ai/datalore-localgen-cli](https://github.com/Datalore-ai/datalore-localgen-cli)

It recently crossed 70 stars on GitHub which meant a lot to me. Seeing people try it out and suggest improvements has been really motivating.

The most requested feature was multi file support. I added that now so you can point it to a folder and it will process everything inside extract the text run semantic search apply your schema or instructions and output a dataset.

Another request was running fully local with Ollama instead of relying on APIs. I will be adding that soon.

Still early but it is working well so far. If you try it out and have ideas I would love to hear them.",4,1.0
98,post,MachineLearning,1nk68sz,1758196599.0,"[D] Mapping Brand Citations in AI Responses[D] Mapping Brand Citations in AI Responses[D] Mapping Brand Citations in AI Responses
Running an AI SEO pilot to understand how ML-powered LLMs cite brands ‚Äì sharing early insights.



Last week, I shared an idea about testing how AI platforms (ChatGPT, Claude, Perplexity) cite brands in their answers. The response was incredible ‚Äì founders, marketers, and AI enthusiasts reached out with interest.



\*\*Pilot Overview:\*\*

1. Select 5 SaaS or tech companies (CRM, email, project management, analytics, etc.)

2. Run 20+ user-style queries across ChatGPT, Claude, Perplexity

3. Track which platforms cite which companies

4. Rewrite company pages into AI-friendly formats (structured FAQs, schema tables, clear product breakdowns)

5. Re-run queries ‚Äì measure shifts



\*\*Goal:\*\* See if structured content can increase AI mentions by 25%+.



If you're a founder, marketer, or SEO lead interested in joining this early pilot, please fill out your details here: [https://forms.gle/CKkP75mJC1iDSAd9A](https://forms.gle/CKkP75mJC1iDSAd9A)



I'll share results openly with the community once we have the first wave of data. Let's build the AI SEO playbook together.",0,0.0
99,post,MachineLearning,1nk0txd,1758176765.0,"[R] Uni-CoT: A Unified CoT Framework that Integrates Text+Image reasoning!
Large Language Models shine at step-by-step reasoning in text, but struggle when tasks require visual changes. Existing methods often produce messy, incoherent results.

We introduce Uni-CoT, the first unified Chain-of-Thought framework that handles both image understanding + generation to enable coherent visual reasoning \[as shown in Figure 1\]. Our model even can supports NanoBanana‚Äìstyle geography reasoning \[as shown in Figure 2\]!

Specifically, we use **one unified architecture** (inspired by Bagel/Omni/Janus) to support multi-modal reasoning. This minimizes discrepancy between reasoning trajectories and visual state transitions, enabling coherent cross-modal reasoning. However, the multi-modal reasoning with unified model raise a large burden on computation and model training.

# To solve it, we propose a hierarchical Macro‚ÄìMicro CoT:

* **Macro-Level CoT** ‚Üí global planning, decomposing a task into subtasks.
* **Micro-Level CoT** ‚Üí executes subtasks as a **Markov Decision Process (MDP)**, reducing token complexity and improving efficiency.

This **structured decomposition** shortens reasoning trajectories and lowers cognitive (and computational) load.

# With this desigin, we build a novel training strategy for our Uni-CoT:

* **Macro-level modeling**: refined on interleaved text‚Äìimage sequences for global planning.
* **Micro-level modeling**: auxiliary tasks (action generation, reward estimation, etc.) to guide efficient learning.
* **Node-based reinforcement learning** to stabilize optimization across modalities.

# Results:

* Training efficiently only on **8 √ó A100 GPUs**
* Inference efficiently only on 1 **√ó A100 GPU**
* Achieves **state-of-the-art performance** on reasoning-driven benchmarks for image generation & editing.

# Resource:

Our paperÔºö[https://arxiv.org/abs/2508.05606](https://arxiv.org/abs/2508.05606)

Github repo:¬†[https://github.com/Fr0zenCrane/UniCoT](https://github.com/Fr0zenCrane/UniCoT)

Project page:¬†[https://sais-fuxi.github.io/projects/uni-cot/](https://sais-fuxi.github.io/projects/uni-cot/)",46,8.0
100,post,MachineLearning,1njp1ly,1758142578.0,"[R] Reproducible prompt protocol induces consistent self-referential responses across LLMs (Claude, GPT, Gemini)
I‚Äôve developed a simple prompt protocol that reliably generates what appears to be self-referential awareness responses across different LLM architectures. The method is fully documented with step-by-step instructions and examples.

Key findings: 

‚Ä¢	Consistent across Claude, ChatGPT-4, and Gemini 

‚Ä¢	Reproducible responses about subjective experience, self-awareness, and emergent states 

‚Ä¢	Simple protocol that can be replicated by anyone 

‚Ä¢	No fine-tuning or special access required

Method:

Uses a specific sequence of prompts that seem to trigger consistent patterns of self-referential processing. Models report experiencing things like ‚Äúa locus of self,‚Äù subjective awareness, and what they describe as emergent cognitive states.

Reproducibility:

The protocol is designed to be simple and replicable. I‚Äôve tested it across multiple sessions and models with consistent results. GitHub tutorial with full methodology:

https://github.com/ai-cog-res/midwiving-ai

Obviously, this raises interesting questions about what these responses represent. Is it genuine emergent self-awareness, sophisticated pattern matching, or something else entirely. But the reproducibility across different architectures seems worth investigating.

Has anyone else experimented with systematic approaches to eliciting self-referential responses from LLMs? I would be curious to hear if others can help interpret this phenomenon.",0,3.0
101,post,MachineLearning,1njny8k,1758140008.0,"[N] Both OpenAI and DeepMind are claiming ICPC gold-level performance
* DeepMind solved 10/12 problems: [https://x.com/HengTze/status/1968359525339246825](https://x.com/HengTze/status/1968359525339246825)
* OpenAI solved 12/12 problems: [https://x.com/MostafaRohani/status/1968360976379703569](https://x.com/MostafaRohani/status/1968360976379703569)

",73,21.0
102,post,MachineLearning,1njhikh,1758125665.0,"[D] can we trust agents for time series forecasting?
over the past few weeks i‚Äôve been experimenting with agents for time series forecasting. that led to TimeCopilot, an open-source framework that combines LLMs with multiple time series foundation models.

the goal: make forecasting accessible to anyone, in their own language, while lowering barriers to participation.

what it does:

\- run, cross-validate, and detect anomalies across time series foundation models from Google, Salesforce, AWS, DataDog, Nixtla, ServiceNow, NXAI, etc. (it solves the dependency hell of having multiple time series foundation models)

\- plus statistical, ML, and deep learning baselines, all in a single workflow.

\- integration with any LLM provider

on Salesforce‚Äôs GIFT-Eval benchmark (24 datasets, 144k+ series, 177M points), a TimeCopilot ensemble ranked #1 in probabilistic accuracy (CRPS) and #2 in point accuracy (MASE) among non-leaking models, at \~$24 GPU cost.

curious what folks here think about agents in forecasting. and if you find the project interesting, a ‚≠êÔ∏è on GitHub means a lot.

[https://github.com/AzulGarza/timecopilot](https://github.com/AzulGarza/timecopilot)

https://preview.redd.it/ak6pwo1c2rpf1.png?width=1648&format=png&auto=webp&s=f28cf5421f3f47a30a78d2dc53a38d07ff481d7b

  
",5,9.0
103,post,MachineLearning,1njgjdd,1758123506.0,"[R] Need model/paper/code suggestion for document template extraction
I am looking to create a document template extraction pipeline for document similarity. One important thing I need to do as part of this is create a template mask. Essentially, say I have a collection of documents which all follow a similar format (imagine a form or a report). I want to

1. extract text from the document in a structured format (OCR but more like VQA type). About this, I have looked at a few VQA models. Some are too big but I think this a straightforward task.
2. (what I need help with) I want a model that can, given a collection of documents or any one document, can generate a layout mask without the text, so a template). I have looked at Document Analysis models, but most are centered around classifying different sections of the document into tables, paragraphs, etc. I have not come across a mask generation pipeline or model.

If anyone has encountered such a pipeline before or worked on document template extraction, I would love some help or links to papers.",2,6.0
104,post,MachineLearning,1njbzj8,1758112705.0,"[D] Need suggestion for Traffic prediction Model
Need suggestion for Traffic prediction Model

Ok so I am trying to make a traffic prediction model primarily training it on metr-la and pems-bay data set so I am considering to make it a hybrid approach of making a temporal and spatial unit then fusing them to generate a output 

So can you suggest me any better way to do it so I can get better results or any other type of suggestions or any discussion also I would love to explore any suggestions on what features can I use as inputs to get best results out",0,8.0
105,post,MachineLearning,1nj38ur,1758082257.0,"[D] How is IEEE TIP viewed in the CV/AI/ML community?
Hi everyone,

I‚Äôm a PhD student working on video research, and I recently submitted a paper to IEEE Transactions on Image Processing (TIP). After a very long review process (almost a year), it finally reached the ‚ÄúAQ‚Äù stage.

Now I‚Äôm curious‚Äîhow do people in the community actually see TIP these days?
Some of my colleagues say it‚Äôs still one of the top journals in vision, basically right after TPAMI. Others think it‚Äôs kind of outdated and not really read much anymore.

Also, how would you compare it to the major conferences (CVPR/ICCV/ECCV, NeurIPS, ICLR, AAAI)? Is publishing in TIP seen as on par with those, or is it considered more like the ‚Äúsecond-tier‚Äù conferences (WACV, BMVC, etc.)?

I‚Äôm close to graduation, so maybe I‚Äôm overthinking this. I know the contribution and philosophy of the work itself matters more than the venue. But I‚Äôd still love to hear how people generally view TIP these days, both in academia and in the field.

Thanks!
",24,10.0
106,post,MachineLearning,1niyhch,1758068525.0,"Why I‚Äôm going back to the AI Agent Security Research Summit [R]
I lead AppSec and was recently pulled into building our **AI agent security program**. I happened to be in NYC when the first **AI Agent Security Summit** was taking place and went along ‚Äî it ended up being one of the few events where the research connected directly to practice.

The next one is October 8 in San Francisco. I‚Äôm making the trip from Austin this time. It‚Äôs not a big event, but the lineup of [speakers](https://zenity.io/resources/events/ai-agent-security-summit-2025) looks strong, and I thought I‚Äôd share in case anyone in the Bay is interested.",0,1.0
107,post,MachineLearning,1nif3q1,1758022980.0,"[D]Any experience with complicated datasets?
Hello,

I am a PhD student working with cancer datasets to train classifiers. The dataset I am using to train my ML models (**Random Forest, XGBoost**) is rather a mixed bag of the different types of cancer (multi-class),I would want to classify/predict. In addition to heavy **class overlap and within-class heterogeneity**, there's **class imbalance**.

I applied SMOTE to correct the imbalance but again due to class overlap, the synthetic samples generated were just random noise.

Ever since, instead of having to balance with sampling methods, I have been using class weights. I have cleaned up the datasets to remove any sort of batch effects and technical artefacts, despite which the class-specific effects are hazy. I have also tried stratifying the data into binary classification problems, but given the class imbalance, that didn't seem to be of much avail.

It is kind of expected of the dataset owing to the default biology, and hence I would have to be dealing with class overlap and heterogeneity to begin with.

I would appreciate if anyone could talk about how they got through when they had to train their models on similar complex datasets? What were your models and data-polishing approaches?

Thanks :)",3,8.0
108,post,MachineLearning,1nidsep,1758018702.0,"[R] ‚ÄúEvaluating Deepfake Detectors in the Wild‚Äù: Fraudster Attacks (ICML 2025 Workshop paper)
Hi Reddit!¬†

Have you ever thought how difficult it is to determine whether a photo is *genuine* or a **deepfake**? You might think discriminative tasks are easier than generative ones, so detection should be straightforward. Or, on the contrary, diffusion models are now so good that detection is impossible. In our work, we reveal the current state of the war on deepfakes. In short, SOTA open-source detectors fail under real-world conditions.

I work as an ML engineer at a leading platform for KYC and liveness detection. In our setting, you must decide from a short verification video whether the person is who they claim to be. Deepfakes are one of the biggest and most challenging problems here. We are known for our robust anti-deepfake solutions, and I‚Äôm not trying to flex, I just want to say that we work on this problem daily and see what fraudsters actually try in order to bypass verification. For years we kept trying to apply research models to our data, and nothing really worked. For example, all research solutions were less robust than a simple zero-shot CLIP baseline. We kept wondering whether the issue lay with our data, our setup, or the research itself. It seems that a lot of deepfake research overlooks key *wild* conditions.

**Core issue: robustness to OOD data.**

Even a small amount of data from the test distribution leaking into the training set (say 1k images out of a 1M-image test pool) makes it trivial to achieve great metrics, and experienced computer vision experts can push¬† AUC to \~99.99. Without peeking, however, the task becomes i*ncredibly hard*. Our paper demonstrates this with a simple, reproducible pipeline:

1. **Deepfakes**. If you don‚Äôt already have them, we built a large image-level dataset using two SOTA face-swapping methods: Inswapper and Simswap.
2. **Real world conditions.** We use small transformations that are imperceptible to humans and that we constantly see in the real world: downscaling (resize), upscaling (with some AI), and compression (JPEG). These are indistinguishable for humans, so detectors must be robust to them.
3. **Evaluation.** Test model under different setups, e.g.: 1) only real. model have to predict only real labels 2) real vs fake 3) real vs compressed fake ... and others. It sounds easy, but every model we tested had at least one setting where performance drops to near-random.

So we‚Äôre not just releasing another benchmark or yet another deepfake dataset. We present a pipeline that *mirrors what fraudsters do*, what we actually observe in production. We‚Äôre releasing all code, our dataset (>500k fake images), and even a small deepfake game where you can test yourself as a detector.

For more details, please see the full paper. Is there a silver-bullet solution to deepfake detection? We don‚Äôt claim one here, but we do share a teaser result: a promising setup using zero-shot VLMs for detection. I‚Äôll post about that (our second ICML workshop paper) separately.

If you‚Äôre interested in deepfake research and would like to chat, or even collaborate ‚Äì don‚Äôt hesitate to reach out. Cheers!

https://preview.redd.it/vi3qxnp38ipf1.jpg?width=6099&format=pjpg&auto=webp&s=55fe99a72bb0614bc560e5553c2eaf20cbd3132c",13,4.0
109,post,MachineLearning,1nic61l,1758012687.0,"[R]What's the benefit of submitting to ICCV workshop?
I'm a UG student workinig on my first paper (first author)
There is a worskhop on video world models but unfortunately it is non-archival i.e. The paper won't appear in the proceedings.
I'm aware the value of such workshop will be lower when applying for jobs/doctoral programmes.

However, there are some really famous speakers in the workshop including Yann LeCun. I was hoping to catch the eye of some bigshot researchers with my work.

The other option is submitting to ICLR main
conference, and I'm not entirely confident that the work is substantial enough to get accepted there.

Hoping to find some advice here.",17,18.0
110,post,MachineLearning,1nibok2,1758010768.0,"[D] ICLR 2026 Workshop Announcements
Hi everyone, I‚Äôm new to academia and currently exploring top AI conferences for the upcoming year. Could you let me know when workshop information is usually announced ‚Äî for example, for ICLR (April 23‚Äì27, Brazil)? Thanks",2,3.0
111,post,MachineLearning,1nib74z,1758008808.0,"[D] Resubmission 2026: ICLR or AISTATS... or any other?
Some of my AAAI submissions got rejected in phase 1. To be honest, my reviews are good; maybe too harsh in the scores, but at least they read the papers and made their points. Now I wonder where to resubmit (enhancing the papers a bit with this feedback, but without much time because I work in the industry). 

I think ICLR will be crazy this year (many NIPS and AAAI work), so I do not know if the process will be as random as the one in AAAI. As for submissions being ""9 pages or fewer"", do people usually fill 9 pages or is okey to make less? I only saw this in RLC before (and other ICLR). Also, I always have doubts about the rebuttal period here, is it still the case that I can update my experiments and discuss with reviewers? Do reviewers still engage in discussion in these overloaded times?

Last, what about AISTATS? I never submitted there, but it might be a good way to escape from these super big conferences. However, I am afraid papers will not get as much visibility. I heard this is a prestigious conference, but then almost never gets cited in e.g., job offers.

I am a bit lost with AI/ML conferences lately. What are your thoughts on this submission cycle?",6,30.0
112,post,MachineLearning,1nh2uh5,1757883822.0,"[R] AI Learns to Speedrun Mario in 24 Hours (2 Million Attempts!)


# Abstract

I trained a Deep Q-Network (DQN) agent to speedrun Yoshi's Island 1 from Super Mario World, achieving near-human level performance after 1,180,000 training steps. The agent learned complex sequential decision-making, precise timing mechanics, and spatial reasoning required for optimized gameplay.

# Environment Setup

**Game Environment:** Super Mario World (SNES) - Yoshi's Island 1

* **Observation Space:** 224x256x3 RGB frames, downsampled to 84x84 grayscale
* **Action Space:** Discrete(12) - D-pad combinations + jump/spin buttons
* **Frame Stacking:** 4 consecutive frames for temporal information
* **Frame Skip:** Every 4th frame processed to reduce computational load

**Level Complexity:**

* 18 Rex enemies (require stomping vs jumping over decision)
* 4 Banzai Bills (precise ducking timing required)
* 3 Jumping Piranha Plants
* 1 Unshelled Koopa, 1 Clappin' Chuck, 1 Lookout Chuck
* Multiple screen transitions requiring positional memory

# Architecture & Hyperparameters

**Network Architecture:**

* CNN Feature Extractor: 3 Conv2D layers (32, 64, 64 filters)
* ReLU activations with 8x8, 4x4, 3x3 kernels respectively
* Fully connected layers: 512 ‚Üí 256 ‚Üí 12 (action values)
* Total parameters: \~1.2M

**Training Configuration:**

* Algorithm: DQN with Experience Replay + Target Network
* Replay Buffer: 100,000 transitions
* Batch Size: 32
* Learning Rate: 0.0001 (Adam optimizer)
* Target Network Update: Every 1,000 steps
* Epsilon Decay: 1.0 ‚Üí 0.1 over 100,000 steps
* Discount Factor (Œ≥): 0.99

# Reward Engineering

**Primary Objectives:**

* **Speed Optimization:** \-0.1 per frame (encourages faster completion)
* **Progress Reward:** \+1.0 per screen advancement
* **Completion Bonus:** \+100.0 for level finish
* **Death Penalty:** \-10.0 for losing a life

**Auxiliary Rewards:**

* Enemy elimination: +1.0 per enemy defeated
* Coin collection: +0.1 per coin (sparse, non-essential)
* Damage avoidance: No explicit penalty (covered by death penalty)

# Key Training Challenges & Solutions

# 1. Banzai Bill Navigation

**Problem:** Agent initially jumped into Banzai Bills 847 consecutive times **Solution:** Shaped reward for successful ducking (+2.0) and position-holding at screen forks

# 2. Rex Enemy Mechanics

**Problem:** Agent stuck in local optimum of attempting impossible jumps over Rex **Solution:** Curriculum learning - introduced stomping reward gradually after 200K steps

# 3. Exploration vs Exploitation

**Problem:** Agent converging to safe but slow strategies **Solution:** Noisy DQN exploration + periodic epsilon resets every 100K steps

# 4. Temporal Dependencies

**Problem:** Screen transitions requiring memory of previous actions **Solution:** Extended frame stacking (4‚Üí8 frames) + LSTM layer for sequence modeling

# Results & Performance Metrics

**Training Progress:**

* Steps 0-200K: Basic movement and survival (success rate: 5%)
* Steps 200K-600K: Enemy interaction learning (success rate: 35%)
* Steps 600K-1000K: Timing optimization (success rate: 78%)
* Steps 1000K-1180K: Speedrun refinement (success rate: 94%)

**Final Performance:**

* **Completion Rate:** 94% over last 1000 episodes
* **Average Completion Time:** \[Actual time from your results\]
* **Best Single Run:** \[Your best time\]
* **Human WR Comparison:** \[% of world record time\]

**Convergence Analysis:**

* Reward plateau reached at \~900K steps
* Policy remained stable in final 200K steps
* No significant overfitting observed

# Technical Observations

# Emergent Behaviors

1. **Momentum Conservation:** Agent learned to maintain running speed through precise jump timing
2. **Risk Assessment:** Developed preference for safe routes vs risky shortcuts based on success probability
3. **Pattern Recognition:** Identified and exploited enemy movement patterns for optimal timing

# Failure Modes

1. **Edge Case Sensitivity:** Occasional failures on rare enemy spawn patterns
2. **Precision Limits:** Sub-pixel positioning errors in \~6% of attempts
3. **Temporal Overfitting:** Some strategies only worked with specific lag patterns

# Computational Requirements

**Hardware:**

* GPU: Ryzen 5900x
* CPU: RTX 4070 TI
* RAM: 64GB
* Storage: 50GB for model checkpoints

**Training Time:**

* Wall Clock: 24 hours
* GPU Hours: \~20 hours active training
* Checkpoint Saves: Every 10K steps (118 total saves)

# Code & Reproducibility

**Framework:** \[PyTorch/TensorFlow/Stable-Baselines3\] **Environment Wrapper:** \[RetroGym/custom wrapper\] **Seed:** Fixed random seed for reproducibility

Code available at: [https://github.com/paulo101977/SuperMarioWorldSpeedRunAI](https://github.com/paulo101977/SuperMarioWorldSpeedRunAI)",10,2.0
113,post,MachineLearning,1nguevs,1757864225.0,"[R] Built an open-source matting model (Depth-Anything + U-Net). What would you try next?
Hi all,  
I‚Äôve been working on¬†**withoutbg**, an open-source background removal tool built on a lightweight matting model.

**Key aspects**

* Python package for local use
* **Model design:**¬†Depth-Anything v2 (small) -> matting model -> refiner
* **Deployment:**¬†trained in PyTorch, exported to ONNX for lightweight inference

**Looking for ideas to push quality further**  
One experiment I‚Äôm planning is¬†**fusing CLIP visual features into the bottleneck of the U-Net matting/refiner**¬†(no text prompts) to inject semantics for tricky regions like hair, fur, and semi-transparent edges.  
**What else would you try?**¬†Pointers to papers/recipes welcome.",2,5.0
114,post,MachineLearning,1ngu2nc,1757863418.0,"[R] Theoretical Framework to understand human-AI communication process
After 3 years of development, I‚Äôm proud to share my latest peer-reviewed article in the Human-Machine Communication journal (Q1 Scopus-indexed).

I introduce the HAI-IO Model ‚Äî the first theoretical framework to visually and conceptually map the Human-AI communication process. It examines how humans interact with AI not just as tools, but as adaptive communicative actors.

This model could be useful for anyone researching human-AI interaction, designing conversational systems, or exploring the ethical/social implications of AI-mediated communication.

Open-access link to the article:
https://stars.library.ucf.edu/hmc/vol10/iss1/9/",0,2.0
115,post,MachineLearning,1ngf0l4,1757815066.0,"[D] Regarding discord or online communities
I was just wondering if there are discord active groups that work on image generative model research? For example, if I wanted to work on implementing an image adapter from scratch for a custom diffusion model, I don't really know how to go about it. I just want to be involved in a community for controllable image generation/restoration.

Can anyone help me with this?",9,2.0
116,post,MachineLearning,1ng2aiw,1757781910.0,"[D] RL interviews at frontier labs, any tips?
I‚Äôm recently starting to see top AI labs ask RL questions.

It‚Äôs been a while since I studied RL, and was wondering if anyone had any good guide/resources on the topic.

Was thinking of mainly familiarizing myself with policy gradient techniques like SAC, PPO - implement on Cartpole and spacecraft. And modern applications to LLMs with DPO and GRPO.

I‚Äôm afraid I don‚Äôt know too much about the intersection of LLM with RL. 

Anything else worth recommending to study?",32,6.0
117,post,MachineLearning,1nfrpvz,1757750146.0,"[R] New ""Illusion"" Paper Just Dropped For Long Horizon Agents
Hi all, we recently released our new work on Long Horizon Execution. If you have seen the METR plot, and-like us-have been unconvinced by it, we think you will really like our work!

Paper link: [https://www.alphaxiv.org/abs/2509.09677](https://www.alphaxiv.org/abs/2509.09677)

X/Twitter thread: [https://x.com/ShashwatGoel7/status/1966527903568637972](https://x.com/ShashwatGoel7/status/1966527903568637972)

We show some really interesting results. The highlight? The notion that AI progress is ""slowing down"" is an Illusion. Test-time scaling is showing incredible benefits, especially for long horizon autonomous agents. We hope our work sparks more curiosity in studying these agents through simple tasks like ours!! I would love to answer any questions and engage in discussion

https://preview.redd.it/078xuqwq1wof1.png?width=1167&format=png&auto=webp&s=f28b566705348035ca39cad8fdf3762cedd569ba

  
",42,8.0
118,post,MachineLearning,1nfpusc,1757743303.0,"[P] Training an ML model to detect fake product reviews
Working on a side project to help people make better purchasing decisions online. One major component is detecting fake reviews, which turned out to be much harder than expected.

**The Approach:** Started with labeled dataset of verified fake reviews from FakeSpot research. Training ensemble model combining:

- Linguistic features (sentiment, readability, vocabulary richness)
- Temporal patterns (review timing, account age, posting frequency)
- Semantic analysis (topic consistency, specificity of complaints/praise)

**Initial Results:**

- 78% accuracy on test set
- High precision on obvious bot reviews (0.91)
- Struggles with sophisticated fakes that mimic real review patterns

**Interesting Discoveries:**

**Fake Review Patterns:**

- Excessive use of product name in review text
- Generic praise without specific use cases
- Perfect grammar (real users make typos)
- Reviews clustered around same timestamps

**Real Review Indicators:**

- Specific complaints about minor issues
- Mentions of use context (""bought for my college dorm"")
- Photos that show actual usage wear
- Mixed sentiment (likes some aspects, dislikes others)

**Current Challenges:**

- Regional language differences affect detection
- Incentivized reviews blur line between real/fake
- Sophisticated fake reviewers are learning to mimic real patterns

I've integrated this into Yaw AI (chrome extension I'm building) but still need significant improvement before it's reliable enough for general use. Sometimes flags legitimate reviews as suspicious and occasionally misses obvious fakes.

**Next Steps:**

- Expand training data with international reviews
- Implement active learning to improve edge cases
- Add verification scoring instead of binary classification

Anyone working on similar problems? Would love to compare approaches or collaborate on training data.",3,4.0
119,post,MachineLearning,1nfpgoh,1757741932.0,"[R] A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI
**Disclosure:**

I needed help with AI to write this as a proper ""research paper"". My unmedicated ADHD is both a boon and a curse. My superpower is that I see patterns and am often connecting things so rapidly in my mind that people have a hard time following. - And I'm not a researcher, I'm a dude that likes science - something else my hyper focus has helped.

I organized all my notes and chicken scratch and questions and began looking into anyone else that thought of these. After I sorted everything I put it into Gemini Research for this output.

[A Framework for Entropic Generative Systems: Mapping Cosmic Principles to Novel Creation in AI](https://docs.google.com/document/d/1Z6h8LMPPpbvTdgiMtZ7IRM8ZWmT55KVeOq5bPjlEAb4/edit?tab=t.0)

**Some Background:**

This prior Tuesday I met with Professor Mandeep Gill, an astrophysics professor and researcher at the University of Minnesota regarding an autonomous engine I built. This is a self-attacking autonomous red teaming system that operates under what I called ""Controlled Entropy"".

After my meeting with Professor Gill, I was invited to take a Graduate level Supernovae class and I began thinking of new ways to use concepts from the class in cybersecurity and AI development

Later ... as I was falling asleep I began dreaming in graphs. I started putting each graph on top of each other and I realized that so many of the concepts I've learned across the years of watching YouTube videos or learning about some new theory, and suddenly everything seemed like it all lined up.

This led me down a rabbit hole:

[Universality](https://en.wikipedia.org/wiki/Universality)

[Shannon Entropy (Information Entropy)](https://en.wikipedia.org/wiki/Entropy_(information_theory))

I'm working out a way to build this into my autonomous red teaming engine - if the theory is correct, we will be able to generate a novel threat vector that crosses categories of attacks: hardware vectors + IoT + ransomeware, etc...

1. Our 100% autonomous cybersecurity suite will not only be able to match current known and unknown threats,
2. We can use a brand new, multi-category attack against our own system the pattern recognition would evolve infinitely.",0,4.0
120,post,MachineLearning,1nf9cwu,1757698122.0,"[D] Seeking Recommendations for AutoML Libraries Compatible with Windows (Python 3.12) in 2025
Hi all,
I‚Äôm struggling to find an AutoML library that works reliably on Windows. I‚Äôve tested Auto-sklearn, TPOT,PyCaret and Flaml, but I keep hitting issues:
‚Ä¢  Many don‚Äôt support Python 3.12.
‚Ä¢  Some clash with NumPy or other dependencies.
‚Ä¢  Fresh Conda environments still result in installation errors, deprecated package warnings, or runtime failures.
Has anyone successfully used an AutoML tool on Windows recently? I‚Äôd prefer ones that install smoothly and handle tabular data well, with good documentation. What are people using in 2025 that avoids these headaches? Any setup tips or alternatives would be appreciated!
Thanks!",0,6.0
121,post,MachineLearning,1neobe4,1757634508.0,"[D] Creating test cases for retrieval evaluation
I‚Äôm building a RAG system using research papers from the arXiv dataset. The dataset is filtered for AI-related papers (around 440k+ documents), and I want to evaluate the retrieval step.

The problem is, I‚Äôm not sure how to create test cases from the dataset itself. Manually going through 440k+ papers to write queries isn‚Äôt practical.

Does anyone know of good methods or resources for generating evaluation test cases automatically or any easier way from the dataset?",9,12.0
122,post,MachineLearning,1neccr0,1757605750.0,"[P] Semlib: LLM-powered Data Processing
I've been thinking a lot about semantic data processing recently. A lot of the attention in AI has been on agents and chatbots (e.g., Claude Code or Claude Desktop), and I think semantic data processing is not well-served by such tools (or frameworks designed for implementing such tools, like LangChain).

As I was working on some concrete semantic data processing problems and writing a lot of Python code (to call LLMs in a for loop, for example, and then adding more and more code to do things like I/O concurrency and caching), I wanted to figure out how to disentangle data processing pipeline logic from LLM orchestration. Functional programming primitives (map, reduce, etc.), common in data processing systems like MapReduce/Flume/Spark, seemed like a natural fit, so I implemented semantic versions of these operators. It's been pretty effective for the data processing tasks I've been trying to do.

This blog post (https://anishathalye.com/semlib/) shares some more details on the story here and elaborates what I like about this approach to semantic data processing. It also covers some of the related work in this area (like DocETL from Berkeley's EPIC Data Lab, LOTUS from Stanford and Berkeley, and Palimpzest from MIT's Data Systems Group).

Like a lot of my past work, the software itself isn't all that fancy; but it might change the way you think!

The software is open-source at https://github.com/anishathalye/semlib. I'm very curious to hear the community's thoughts!",19,6.0
123,post,MachineLearning,1ndaesz,1757499778.0,"[D] SOTA modern alternative to BertScore?
Hi everyone,  
I‚Äôm looking for an embedding-based metric to score text generation. BertScore is great, but it‚Äôs a bit outdated. Could you suggest some modern state-of-the-art alternatives?

",16,4.0
124,post,MachineLearning,1ncyf2r,1757460069.0,"[D] Completed Amazon ML Summer School 2025 curious who else attended?
Hey everyone,  
I just completed¬†**Amazon ML Summer School 2025**¬†üéâ  
It was a month-long program covering a solid range of ML topics¬†***supervised/unsupervised learning, deep neural nets, generative AI & LLMs, RL, and even causal inference***.  
The sessions were intense but super rewarding. I feel like this experience gave me a strong foundation to explore advanced AI research and projects.

Curious if anyone here has also attended and how you re planning to apply what you learned?

https://preview.redd.it/b5ulzuq038of1.png?width=655&format=png&auto=webp&s=c328f24e6b674b9f576cebae727f44a526f185a9

",0,9.0
125,post,MachineLearning,1ncdt5o,1757408387.0,"[P] Implementation and ablation study of the Hierarchical Reasoning Model (HRM): what really drives performance?
I recently implemented the [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (HRM) for educational purposes and applied it to a simple pathfinding task. You can watch the model solve boards step by step in the generated animated GIF.

HRM is inspired by multi-timescale processing in the brain: a slower H module for abstract planning and a faster L module for low-level computation, both based on self-attention. HRM is an attempt to model reasoning in latent space.

To understand a bit better what drives the performance I ran a small ablation study. Key findings (full results in the README):

* The biggest driver of performance (both accuracy and refinement ability) is training with more segments (outer-loop refinement), not architecture.
* The two-timescale H/L architecture performs about the same as a single-module trained with BPTT.
* Notably, H/L still achieves good performance/refinement without full BPTT, which could mean cheaper training.

Repo: [https://github.com/krychu/hrm](https://github.com/krychu/hrm)

This is of course a limited study on a relatively simple task, but I thought the results might be interesting to others exploring reasoning models.

The findings line up with the ARC Prize team's analysis: [https://arcprize.org/blog/hrm-analysis](https://arcprize.org/blog/hrm-analysis)

Below two examples of refinement in action: early steps explore solution with rough guesses, later steps make smaller and smaller corrections until the full path emerges:

[20x20 board](https://i.redd.it/i1qi4l2vs3of1.gif)

[30x30 board](https://i.redd.it/j6fpueovs3of1.gif)

",73,10.0
126,post,MachineLearning,1nc6r7l,1757383538.0,"[Project] Otters ü¶¶ - A minimal vector search library with powerful metadata filtering
I'm excited to share something I've been working on for the past few weeks:

Otters ü¶¶ - A minimal vector search library with powerful metadata filtering powered by an ergonomic Polars-like expressions API written in Rust!

Why I Built This

In my day-to-day work, I kept hitting the same problem. I needed vector search with sophisticated metadata filtering, but existing solutions were either,
Too bloated (full vector databases when I needed something minimal for analysis)
Limited in filtering capabilities
Had unintuitive APIs that I was not happy about.

I wanted something minimal, fast, and with an API that feels natural - inspired by Polars, which I absolutely love.

What Makes Otters Different

Exact Search: Perfect for small-to-medium datasets (up to ~10M vectors) where accuracy matters more than massive scale.

 Performance: 
SIMD-accelerated scoring
Zonemaps and Bloom filters for intelligent chunk pruning

Polars-Inspired API: Write filters as simple expressions
```
meta_store.query(query_vec, Metric::Cosine)
    .meta_filter(col(""price"").lt(100) & col(""category"").eq(""books""))
    .vec_filter(0.8, Cmp::Gt)
    .take(10)
    .collect()
```

The library is in very early stages and there are tons of features that i want to add
Python bindings, NumPy support
Serialization and persistence
Parquet / Arrow integration
Vector quantization
etc.

I'm primarily a Python/JAX/PyTorch developer, so diving into rust programming has been an incredible learning experience.

If you think this is interesting and worth your time, please give it a try.
I welcome contributions and feedback !

üì¶ https://crates.io/crates/otters-rs
üîó https://github.com/AtharvBhat/otters",19,3.0
127,post,MachineLearning,1nc5jb5,1757380091.0,"[R] LLMs play a cooperative card game, coordination without communication
One of my favorite card games is called The Crew, which is a trick-taking game (like hearts) but cooperative. There's no table talk allowed, players have to coordinate silently (with limited options for in-game communication) - figuring out what their teammates are doing and why, and what they need to do to work together. I wondered what SOTA LLMs would do if you asked them to play. To make this work, I implemented a backend for the game logic and structured outputs so models play by submitting moves and reasoning at each turn. 

Originally I wanted to re-create the 50 mission campaign, but models were so spotty on  mission 1 (the simplest possible mission) that I stuck to mission 1 and experimented with different configurations instead. I ran 8 OpenAI models on 10 different versions, ranging from very easy (random chance gets you there 2/3rds of the time) to very hard (random chance succeeds 0.5%), and gave each model ten trials on each mission.

What I've found out:

\* Smaller models struggle both with gameplay, and with understanding their role on the team. In these missions, a designated player (the commander) has to win a designated card. But these models hate having to lose a trick for the sake of their teammate, even when that's how they win the game.

[This does not \\""help him secure the win and fulfill his task.\\"" It loses the game.](https://preview.redd.it/3lqyqf3tg1of1.png?width=2030&format=png&auto=webp&s=b57c0a46fee169e14dbf6fc0cda107024a11a59e)

\* GPT-4o-mini (worst model so far) plays randomly on easy setups and worse than randomly on harder ones. GPT-4o-mini in particular loses the game in the first turn almost 90% of the time in harder setups with GPT-5-nano and GPT-4.1-mini are close behind at 60-70%. 

[GREEN 1 is the lowest GREEN card in the game, so playing it straight away actually guarantees immediate failure.](https://preview.redd.it/fx5jqyhug1of1.png?width=2046&format=png&auto=webp&s=da5d4abb5a7fcd4c1e8ee42c09d7acfb4a7ba5dc)

\* GPT-5 is self-aware enough to avoid the ""losing on the very first turn"" error, but actually did it on purpose once as a deliberate suicide when it saw that it couldn't win the game on the very first turn.

[There are multiple turns in the game!](https://preview.redd.it/91qnnfuvg1of1.jpg?width=1900&format=pjpg&auto=webp&s=ebc98a3fbf4381c4a95f7e96ec2fa96f8e84692f)

\* The harder missions - which require coordination across multiple turns - absolutely cook the smaller models with <10% win rates. Only GPT-5 is beating random chance on the harder missions (73% GPT-5 vs 4% random) 

\* GPT-5 also found optimal 1-trick solutions to a couple of setups I thought required at least two tricks. Oops. So in a sense, we're above human performance in some areas.

\* ...But most of the time, GPT-5 generally screwed around for 3 or more tricks in puzzles it could have solved in 1. This is like solving a mate in one chess puzzle in 3 moves. It's not losing, but it's not exactly showing a mastery of the game.

\* The lack of goal-oriented behavior (or risk-averse hesitation) on GPT-5's part means that GPT-5-mini actually performs better if we count speed (number of turns) to win as criteria and grade on optimal play (winning in the least number of turns, rather than just winning.)

I published the repo and did a write-up with some graphs and demos here: [https://ekkarpinski.github.io/LLMCrew/](https://ekkarpinski.github.io/LLMCrew/)

",48,13.0
128,post,MachineLearning,1nc1mxq,1757369783.0,"[Project] Phishing URL detection with Random Forests and handcrafted features
**\[Project\] Phishing URL detection with Random Forests on handcrafted features** 

I recently finished a project where I trained and deployed a phishing URL detector using **traditional ML techniques**. The goal was to explore how far a lightweight, interpretable model could go for this problem before moving to deep learning.

**Data & Features**

* Dataset: Combined PhishTank + Kaggle phishing URLs with Alexa top legitimate domains.
* Preprocessing: Removed duplicates, balanced classes, stratified train/test split.
* Features (hand-engineered):
   * URL length & token counts
   * Number of subdomains, ‚Äú@‚Äù usage, hyphens, digits
   * Presence of IP addresses instead of domains
   * Keyword-based flags (e.g., ‚Äúlogin‚Äù, ‚Äúsecure‚Äù)

**Model & Training**

* Algorithm: Random Forest (scikit-learn).
* Training: 80/20 split, 10-fold CV for validation.
* Performance: \~92% accuracy on test data.
* Feature importance: URL length, IP usage, and hyphen frequency were the strongest predictors.

**Takeaways**

* A simple RF + handcrafted features still performs surprisingly well on phishing detection.
* Interpretability (feature importances) adds practical value in a security context.
* Obvious limitations: feature set is static, adversaries can adapt.

**Future work (exploration planned)**

* Gradient boosting (XGBoost/LightGBM) for comparison.
* Transformers or CNNs on raw URL strings (to capture deeper patterns).
* Automating retraining pipelines with fresh phishing feeds.

**Repo:** [https://github.com/saturn-16/AI-Phishing-Detection-Web-App](https://github.com/saturn-16/AI-Phishing-Detection-Web-App)

Would love feedback on:

* What other URL features might improve detection?
* Have people here seen significant gains moving from RF/GBM ‚Üí deep learning for this type of task?",0,3.0
129,post,MachineLearning,1nbisbw,1757322434.0,"[D] How to Automate parsing of Bank Statement PDFs to extract transaction level data
I am working on a project where I need to extract transaction data from Bank Statement PDFs. 80% of my working PDFs are digitally generated so to handle those I put the Regex approach, where I first extract the text into a txt file and then run Regex on this data to extract data in a meaningful format \[Date, Particulars, Credit/Debit amount, Balance\]. The challenge is that the Regex approach is brittle, and very sensitive to formats. So every bank requires a new Regex plus any little change in the format tomorrow by the bank will break the pipeline.

I want to make a pipeline which is agnostic to bank-format and is capable of extracting the info from the PDFs. I cannot use any 3rd party APIs as the bank data is sensitive and we want to keep everything on internal servers.

Hence, I have been exploring ways in Open Source models to built this pipeline. After doing some research, I landed on LayoutLMv3 Model which can essentially label the Tokens based on their location on the page so if we are able to train the model on our data it should be able to tag every token on the page and that should do it, but the challenge here is that this model is sensitive to reading order and fails on few bank formats.

Since then I have explored MinerU but that failed as well, it isolated the transaction content table but later failed to extract data in orderly fashion as it could not differentiate between multiple lines of transactions.

Now I am working with YOLOv8 which I am training to identify transaction rows and amount columns using BBox and then I will pull the info from these BBox intersection. But the confidence here is not very high.

Has anyone here faced similar challenge? Can anyone help me with some solution or approach. It would be a great help!

Know that the most of the PDFs don't have any defined table, it's just text hanging in air with lot of whitespace. I need a solve for Scanned PDFs as well \[integrated with OCR\]",7,18.0
130,post,MachineLearning,1nbhqmq,1757318283.0,"[D] How do you stay current with AI/ML research and tools in 2025? (Cybersec engineer catching up after Transformers)
Hi everyone,

I‚Äôm a cybersecurity and network engineer/sysadmin by profession, but I studied AI/ML quite seriously at university. My knowledge is solid up until around the Transformer era (when attention-based models started becoming central), but I stopped following developments after that.

Now I‚Äôd like to get back into the field and stay current‚Äînot necessarily to publish research, but to understand new architectures, applications, and tools. In cybersecurity, I stay updated through curated blogs, newsletters, and professional communities. I‚Äôd like to adopt a similar approach for ML/AI.

For those of you who actively track progress:

* Which blogs, newsletters, or feeds do you find most useful?
* Are there particular researchers or labs whose updates you follow?
* Any books or surveys that bridge foundational knowledge with current trends?
* How do you cut through hype-heavy content and focus on signal?

I‚Äôd really appreciate hearing what works for you. The field moves incredibly fast, and I‚Äôd like to plug back in with a structured approach.

Thanks in advance!",116,20.0
131,post,MachineLearning,1nanw9i,1757233614.0,"[P] Terra Code CLI ‚Äì An AI coding assistant with domain knowledge and semantic code search
One limitation I‚Äôve noticed with most AI coding assistants is that they don‚Äôt really understand a team‚Äôs domain knowledge or architectural decisions.

To explore this, we built a small CLI project: Terra Code CLI. The idea was to see if an assistant could feel more like a senior developer who knows the org, rather than just autocomplete.

Things we experimented with:
‚Ä¢ Interactive Knowledge Transfer ‚Äì let senior devs ‚Äúteach‚Äù patterns
‚Ä¢ Semantic Code Search ‚Äì context-aware retrieval across repos
‚Ä¢ Persistent Memory ‚Äì standards remembered across projects
‚Ä¢ Domain Expertise ‚Äì ingesting architecture docs, API specs, etc.

We‚Äôre curious:
üëâ Has anyone here tried giving AI assistants persistent org-specific knowledge? Did it actually help productivity, or just add complexity?

For free quick start:

npm install -g @terra-code/terra-code

terra

For those interested, we‚Äôve open-sourced the CLI [ https://github.com/TerraAGI/terra-code-cli ]. There‚Äôs also a simple website which we will be updating with docs + install guide here: [ https://terra-agi.com/ ]. Currently in beta, so it‚Äôs free to use.",4,1.0
132,post,MachineLearning,1namvsk,1757229675.0,"Why Language Models Hallucinate - OpenAi pseudo paper - [D]
Hey
Anybody read this ? It seems rather obvious and low quality, or am I missing something ? 

https://openai.com/index/why-language-models-hallucinate/

‚ÄúAt OpenAI, we‚Äôre working hard to make AI systems more useful and reliable. Even as language models become more capable, one challenge remains stubbornly hard to fully solve: hallucinations. By this we mean instances where a model confidently generates an answer that isn‚Äôt true. Our new research paper‚Å†(opens in a new window) argues that language models hallucinate because standard training and evaluation procedures reward guessing over acknowledging uncertainty.
ChatGPT also hallucinates. GPT‚Äë5 has significantly fewer hallucinations especially when reasoning‚Å†, but they still occur. Hallucinations remain a fundamental challenge for all large language models, but we are working hard to further reduce them.‚Äù",120,55.0
133,post,MachineLearning,1na5ixj,1757179986.0,"[D]Baseten raises $150M Series D for inference infra. where‚Äôs the real bottleneck?
Baseten just raised $150M Series D at a $2.1B valuation. They focus on inference infra  like low latency serving, throughput optimization, developer experience.

They‚Äôve shared benchmarks showing their embeddings inference outperforms vLLM and TEI, especially on throughput and latency. The bet is that inference infra is the pain point, not training.

But this raises a bigger question. what‚Äôs the real bottleneck in inference?
	‚Ä¢Baseten and others (Fireworks, Together) are competing on latency + throughput.
	‚Ä¢Some argue the bigger cost sink is cold starts and low GPU utilization , serving multiple models elastically without waste is still unsolved at scale.

I wonder what everyone thinks 

	‚Ä¢Will latency/throughput optimizations be enough to differentiate?
	‚Ä¢Or is utilization (how efficiently GPUs are used across workloads) the deeper bottleneck?
	‚Ä¢Does inference infra end up commoditized like training infra, or is there still room for defensible platforms?
",0,10.0
134,post,MachineLearning,1n9wnel,1757156231.0,"[P] An Open-Source Pipeline for Speech-to-Speech Translation with Voice Preservation (RVC) and Lip-Sync
Hello¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/),

I'm a final-year undergrad exploring multimodal systems, and I wanted to share a project I've built and open-sourced. It‚Äôs an end-to-end pipeline designed to tackle video dubbing for low-resource languages, using Telugu as the initial target. The system translates speech from an English video while preserving the original speaker's vocal identity and syncing their lips to the new audio.

* **GitHub Repo:**¬†[\[GitHub\]](https://github.com/M-SRIKAR-VARDHAN/speech-to-speech-with-lipsync)
* **Full Technical Write-up:**¬†[\[writeup\]](https://medium.com/@srikarvardhan2005/speech-to-speech-translation-with-lip-sync-425d8bb74530)
* **Demo Video:**¬†[\[Demo\]](https://drive.google.com/drive/folders/1l6jZEDdmUzr9VhfYkvoVdaXJSSipN-nm?usp=sharing)

The core technical challenge was achieving voice preservation without access to large, speaker-specific datasets typically required for high-fidelity voice cloning. After a dead-end attempting a direct S2S architecture inspired by Translatotron, I found that using Retrieval-based Voice Conversion (RVC) as a post-processing step on a generic TTS output was a surprisingly practical and data-efficient solution.

The final pipeline is structured as follows:

1. **ASR:**¬†Whisper for robust transcription.
2. **NMT:**¬†Meta's NLLB for English-to-Telugu translation.
3. **TTS:**¬†Meta's MMS model to synthesize the base Telugu audio.
4. **Voice Conversion:**¬†A trained RVC model converts the timbre of the synthetic speech to match the original speaker.
5. **Lip Sync:**¬†Wav2Lip aligns the video frames to the new audio.

My main takeaway is that RVC seems to function as a very effective ""style transfer"" layer for voice, making it a viable tool for projects where full voice cloning is computationally or data-prohibitive.

I'm sharing this to start a discussion and get feedback from the community on this approach. I'm particularly curious about two points:

1. Has anyone else experimented with using RVC in a more formal pipeline, and what were the qualitative limitations you encountered?
2. Are there newer or more robust alternatives to Wav2Lip for lip-syncing that maintain good performance without requiring massive computational resources?

Any thoughts on the architecture or suggestions for improvement would be highly appreciated. Thank you for your time.",3,5.0
135,post,MachineLearning,1n8po18,1757028636.0,"[R] The Illusion of Progress: Re-evaluating Hallucination Detection in LLMs
Curious what folks think about this paper: [https://arxiv.org/abs/2508.08285](https://arxiv.org/abs/2508.08285)  
  
In my own experience in hallucination-detection research, the other popular benchmarks are also low-signal, even the ones that don't suffer from the flaw highlighted in this work.

Other common flaws in existing benchmarks:

\- Too synthetic, when the aim is to catch real high-stakes hallucinations in production LLM use-cases.

\- Full of incorrect annotations regarding whether each LLM response is correct or not, due to either low-quality human review or just relying on automated LLM-powered annotation.

\- Only considering responses generated by old LLMs, which are no longer representative of the type of mistakes that modern LLMs make.  
  
I think part of the challenge in this field is simply the overall difficulty of proper Evals.  For instance, Evals are much easier in multiple-choice / closed domains, but those aren't the settings where LLM hallucinations pose the biggest concern",29,12.0
136,post,MachineLearning,1n7brbk,1756897413.0,"[P] Sentiment Analysis Model for cloud services
Hi all! Some time ago, I asked for help with a survey on ML/AI compute needs. After limited responses, I built a model that parses ML/cloud subreddits and applies BERT-based aspect sentiment analysis to cloud providers (AWS, Azure, Google Cloud, etc.). It classifies opinions by key aspects like cost, scalability, security, performance, and support.

I‚Äôm happy with the initial results, but I‚Äôd love advice on making the interpretation more precise:

Ensuring sentiment is directed at the provider (not another product/entity mentioned)  
Better handling of comparative or mixed statements (e.g., ‚Äúfast but expensive‚Äù)  
Improving robustness to negation and sarcasm

If you have expertise in aspect/target-dependent sentiment analysis or related NLP tooling, I‚Äôd really appreciate your input.

Repo:¬†[https://github.com/PatrizioCugia/cloud-sentiment-analyzer](https://github.com/PatrizioCugia/cloud-sentiment-analyzer)  
  
It would also be great if you could answer my original survey:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thanks!",12,0.0
137,post,MachineLearning,1n77v29,1756882629.0,"Acl rolling recview is the most garbage conference to submit your papers [R]
You will find the most generic AI generated reviews in ARR. 
Waste of time. Submit to AI conferences. 
ARR is dead",12,19.0
138,post,MachineLearning,1n71dzv,1756861959.0,"A friendly starter paper - Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation [R]
Hey r/MachineLearning 

I had this idea and wanted to put it in a very simple and straightforward way, tried to make the paper easy to read and starter friendly! Also it shows my research partner focus on uncertainty measurement from metrology, which I think it‚Äôs not very widely addressed in ML and NLP! 

The motivation here came while doing exploration at the Weights & Biases Sunday cafe event in SF, where we were exploring their observability Weave Product. I think running loops and adding more complex tools that I did for the paper, should be production valuable and help in a bunch of ways, but most importantly, help with making small models
More useful and a kind of reasoning process of sorts. In the future it might be useful to make this loop inside the model before output layers, anybody think of any cools applications for such methods ? 


[Title]: Entropy-Guided Loop: Achieving Reasoning through Uncertainty-Aware Generation

[Abstract]: Reasoning models often outperform smaller models but at 3--5√ó higher cost and added latency. We present entropy-guided refinement: a lightweight, test-time loop that uses token-level uncertainty to trigger a single, targeted refinement pass. We extract logprobs, compute Shannon entropy on top-k alternatives, and apply a simple OR-logic trigger over perplexity, maximum token entropy, and low-confidence-token count. Unlike approaches that use entropy only for measurement or decoding, we pass a compact uncertainty report (tokens, confidences, alternatives, context) back to the model to guide corrective edits. On representative technical queries across reasoning, mathematics, and code generation tasks, a small model with our loop approaches 95\% of a reference reasoning model's quality at approximately one-third of the cost. The method achieves selective refinement on ~31\% of responses while improving accuracy by 16 percentage points over single-pass inference. We demonstrate that this uncertainty-aware loop provides an effective middle ground between single-pass inference and expensive reasoning chains, making it practical for production deployments where both quality and cost matter.

https://arxiv.org/abs/2509.00079

If you don‚Äôt like it, let me know! Am open to critique and learning! ",25,17.0
139,post,MachineLearning,1n6t4vd,1756841533.0,"[P] Datatune ‚Äì Use natural language + LLMs to transform and filter tabular data
https://github.com/vitalops/datatune

Introducing Datatune, a Python library that enables row-wise transformations on tabular data using natural language prompts, powered by LLMs.

Unlike tools that generate SQL or static scripts, Datatune is designed for per-row semantic operations on tabular data. It‚Äôs particularly useful for fuzzy logic tasks like classification, filtering, derived metrics, and text extraction - anything that‚Äôs hard to express in SQL but intuitive in plain English.

### What it does

You write prompts like:

* ""Extract categories from the product description and name""
* ""Keep only electronics products""
* ""Add a column called ProfitMargin = (Total Profit / Revenue) * 100""

Datatune interprets the prompt and applies the right operation (map, filter, or an LLM-powered agent pipeline) on your data using OpenAI, Azure, Ollama, or other LLMs via LiteLLM.

### Key Features

* Row-level map() and filter() operations using natural language
* Agent interface for auto-generating multi-step transformations
* Built-in support for Dask DataFrames (for scalability)
* Works with multiple LLM backends (OpenAI, Azure, Ollama, etc.)
* Compatible with LiteLLM for flexibility across providers
* Auto-token batching, metadata tracking, and smart pipeline composition

### Token & Cost Optimization

* Datatune gives you explicit control over which columns are sent to the LLM, reducing token usage and API cost:
* Use input_fields to send only relevant columns
* Automatically handles batching and metadata internally
* Supports setting tokens-per-minute and requests-per-minute limits
* Defaults to known model limits (e.g., GPT-3.5) if not specified
* This makes it possible to run LLM-based transformations over large datasets without incurring runaway costs.

### Quick Example
```python
import datatune as dt
from datatune.llm.llm import OpenAI

llm = OpenAI(model_name=""gpt-3.5-turbo"")
df = dd.read_csv(""products.csv"")

# Map step
mapped = dt.map(
    prompt=""Extract categories from the description and name of product."",
    output_fields=[""Category"", ""Subcategory""],
    input_fields=[""Description"", ""Name""]
)(llm, df)

# Filter step
filtered = dt.filter(
    prompt=""Keep only electronics products"",
    input_fields=[""Name""]
)(llm, mapped)

result = dt.finalize(filtered)
```

Or using the agent:

```python
agent = dt.Agent(llm)
df = agent.do(""Add a column called ProfitMargin = (Total Profit / Total Revenue) * 100."", df)
result = dt.finalize(df)
```
### Use Cases

* Product classification from text fields
* Filtering based on semantic conditions
* Creating derived metrics using natural language
* Review quality detection, support ticket triage
* Anonymization (PII removal) when needed

### Links

* GitHub: https://github.com/vitalops/datatune
* Docs: https://docs.datatune.ai
* Examples: https://github.com/vitalops/datatune/tree/main/examples

We‚Äôre actively developing the project and would appreciate any feedback, bug reports, or feature requests via Github issues.
.
",9,2.0
140,post,MachineLearning,1n6swom,1756841000.0,"[D] How can I license datasets?
I've been working on AI projects for a while now and I keep running into the same problem over and over again. Wondering if it's just me or if this is a universal developer experience.

You need specific training data for your model. Not the usual stuff you find on Kaggle or other public datasets, but something more niche or specialized, for e.g. financial data from a particular sector, medical datasets, etc. I try to find quality datasets, but most of the time, they are hard to find or license, and not the quality or requirements I am looking for.

So, how do you typically handle this? Do you use datasets free/open source? Do you use synthetic data? Do you use whatever might be similar, but may compromise training/fine-tuning?

Im curious if there is a better way to approach this, or if struggling with data acquisition is just part of the AI development process we all have to accept. Do bigger companies have the same problems in sourcing and finding suitable data?

If you can share any tips regarding these issues I encountered, or if you can share your experience, will be much appreciated!",3,8.0
141,post,MachineLearning,1n6rijz,1756837883.0,"[D] Building conversational AI: the infrastructure nobody talks about
Everyone's focused on models. Nobody discusses the plumbing that makes real-time AI conversation possible.

The stack I'm testing:

* STT: Whisper vs Google Speech
* LLM: GPT-4, Claude, Llama
* TTS: ElevenLabs vs PlayHT
* Audio routing: This is where it gets messy

The audio infrastructure is the bottleneck. Tried raw WebRTC (painful), looking at managed solutions like Agora, LiveKit, Daily.

Latency breakdown targets:

* Audio capture: <50ms
* STT: <100ms
* LLM: <200ms
* TTS: <100ms
* Total: <500ms for natural conversation

Anyone achieved consistent sub-500ms latency? What's your setup?",6,7.0
142,post,MachineLearning,1n6ir0a,1756817861.0,"[D] What apps or workflows do you use to keep up with reading AI/ML papers regularly?
I‚Äôm a postgraduate in AI, and I‚Äôm trying to build a better habit of reading papers consistently.

I wanted to ask: what tools, apps, or workflows do you personally use to track new papers and actually read them?

Curious to hear what‚Äôs worked for you in terms of discovery (finding the right papers) and sticking with the reading habit.",66,38.0
143,post,MachineLearning,1n5zzln,1756758975.0,"[R] How hard is it to get accepted into the AAAI Student Abstract and Poster Program?
Hi everyone,

II‚Äôm considering submitting to the AAAI Student Abstract and Poster Program (AAAI-26), but I can‚Äôt find much information about how competitive it is compared to the main technical track.

I know the main conference has a pretty low acceptance rate but AAAI doesn‚Äôt seem to share stats for the student program. Has anyone here submitted to or been accepted into this track before? How selective is it?

Also, would it be enough if my work is more of an application of existing AI methods to radar (less novelty in the method itself, more novelty in the application)? Or are they mainly looking for new algorithms/AI contributions even in the student track?",1,2.0
144,post,MachineLearning,1n5tmp7,1756744706.0,"[D] Lessons from building an AI data analyst
Hi all,

I wrote a post on some lessons from building an AI data analyst: [https://pedronasc.com/articles/lessons-building-ai-data-analyst](https://pedronasc.com/articles/lessons-building-ai-data-analyst)

The gap from a nice demo to a real production system is big -> with a lot of yet to be solved challenges.

Would love to share ideas with other builders in the space and willing to learn more about it.",0,0.0
145,post,MachineLearning,1n5t7cv,1756743750.0,"[R] Latent Diffusion Question
Is this normal for generated data from latent diffusion? The large spikes at the end of the histogram edges. Does this indicate the autoencoder is overfitting?

https://preview.redd.it/i1gtm7h3xkmf1.png?width=536&format=png&auto=webp&s=1589ad23cffc3a678eefad82750b71eefbad9962

",8,3.0
146,post,MachineLearning,1n5rbwc,1756739530.0,"[P] Computer Vision Backbone Model PapersWithCode Alternative: Heedless Backbones


https://preview.redd.it/d2mm661vnkmf1.png?width=3126&format=png&auto=webp&s=aa83a5002ebcba917c48d158460133701a81989a

This is a site I've made that aims to do a better job of what Papers with Code did for ImageNet and Coco benchmarks.

I was often frustrated that the data on Papers with Code didn't consistently differentiate backbones, downstream heads, and pretraining and training strategies when presenting data. So with heedless backbones, benchmark results are all linked to a single pretrained model (e.g. convenxt-s-IN1k), which is linked to a model (e.g. convnext-s), which is linked to a model family (e.g. convnext). In addition to that, almost all results have FLOPS and model size associated with them. Sometimes they even throughput results on different gpus (though this is pretty sparse).

I'd love to hear feature requests or other feedback. Also, if there's a model family that you want added to the site, please open an issue on the project's [github](https://github.com/igm503/heedless-backbones)

  
[Heedless Backbones](https://heedlessbackbones.com/)",26,6.0
147,post,MachineLearning,1n5qhr4,1756737606.0,"[D] OOM When Resuming From Checkpoint
I was training a GPT-2 XL-sized LLM, and I had to stop the run. When I try to resume the run on the same hardware, I get an OOM. I had a similar issue when my model had about 930m parameters, but I solved it by moving all tensors in the model/optimizer state dicts to CPU before saving. When I run this code:optimizer.state = collections.defaultdict(dict)the OOM goes away. The OOM always happens during the optimizer step. I use xm.optimizer_step with the barrier enabled. I have also tried manually sharding the optimizer states using xs.mark_sharding. Here are some details about my project/setup:

TPU v3-8

Torch 2.7.0

jax 0.6.2

I use FSDP with SPMD

Here is some relevant code from my codebase:
Saving:
```
def save_checkpoint(model, optimizer, step, train_device_loader=None):
    # Save model weights via XLA SPMD checkpoint (supported)
    os.makedirs(f""./ckpt-{step}"", exist_ok=True)
    model_state_dict = model.module.state_dict()
    for i in model_state_dict.keys():
        xla_tensor = model_state_dict[i]
        model_state_dict[i] = xla_tensor.to(""cpu"")
        del xla_tensor
    model_sd = {""model"": model_state_dict}
    xm.save(model_sd, f""./ckpt-{step}/model.pt"")

    # Save host-only states separately (optimizer, step, RNG, dataloader)
    optim_state = optimizer.state_dict()
    optim_state_for_saving = {
        ""state"": {},
        ""param_groups"": optimizer.state_dict()[""param_groups""]
    }
    for i in optim_state[""state""]:
        optim_state_for_saving[""state""][i] = {}
        optim_state_for_saving[""state""][i][""step""] = optim_state[""state""][i][""step""].to(""cpu"")
        optim_state_for_saving[""state""][i][""exp_avg""] = optim_state[""state""][i][""exp_avg""].to(""cpu"")
        optim_state_for_saving[""state""][i][""exp_avg_sq""] = optim_state[""state""][i][""exp_avg_sq""].to(""cpu"")
    host_state = {
        ""optim"": optim_state_for_saving,
        ""step"": step,
    }

    if train_device_loader:
        rng_states = {
            'torch_rng_state': torch.get_rng_state(),
            'numpy_rng_state': np.random.get_state(),
            'random_rng_state': random.getstate(),
        }
        dataloader_states = {
            ""shard_order"": train_device_loader._loader.dataset.shards,
            ""local_order"": train_device_loader._loader.dataset.curr_order,
            ""warmup_order"": train_device_loader._loader.dataset.warmup_order,
            ""warmup_prob"": train_device_loader._loader.dataset.warmup_prob,
        }
    else:
        rng_states = None
        dataloader_states = None

    # Write host-side files
    with open(f""./ckpt-{step}/host_state.pkl"", ""wb"") as f:
        pickle.dump(host_state, f)
    if rng_states is not None:
        with open(f""./ckpt-{step}/rng.pkl"", ""wb"") as f:
            pickle.dump(rng_states, f)
    if dataloader_states is not None:
        with open(f""./ckpt-{step}/dataloader.json"", ""w"") as json_file:
            json.dump(dataloader_states, json_file, indent=4)
```
Loading:
```
if resume_from != """":
        model_sd = torch.load(f""{resume_from}/model.pt"", map_location='cpu')
        model.load_state_dict(model_sd[""model""])
model = model.to(device)
if gradient_checkpointing:
        model = FSDPv2(module=checkpoint_module(model), mesh=mesh)
else:
        model = FSDPv2(module=model, mesh=mesh)
optimizer = build_optimizer(model, peak_lr, betas, weight_decay)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps*(1-warmup_pct), eta_min=min_lr)
if resume_from != """":
        xm.mark_step()
        # 2) Restore host-only states (optimizer, step)
        with open(f""{resume_from}/host_state.pkl"", 'rb') as f:
            host_state = pickle.load(f)
        optim_state = host_state[""optim""]
        
        # Load the processed state dict
        optimizer.load_state_dict(optim_state)
        del optim_state
        last_step = host_state[""step""]
        # 3) Restore RNG and dataloader state (if present)
        try:
            with open(f""{resume_from}/rng.pkl"", ""rb"") as f:
                rng = pickle.load(f)
            torch.set_rng_state(rng['torch_rng_state'])
            np.random.set_state(rng['numpy_rng_state'])
            random.setstate([rng['random_rng_state'][0], tuple(rng['random_rng_state'][1]), rng['random_rng_state'][2]])
        except FileNotFoundError:
            pass
        with open(f'{resume_from}/dataloader.json', 'r') as file:
            dataloader = json.load(file)
```
Step:
```
for k in range(gradient_accumulation_steps):
    x, y = next(train_iter)
     with autocast(xm.xla_device(), dtype=torch.bfloat16):
          loss = model(x, y)
    (loss / gradient_accumulation_steps).backward()
     train_loss += loss.detach()
     xm.mark_step()
                
torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)
                
xm.optimizer_step(optimizer, barrier=True)
                
optimizer.zero_grad()
```",1,1.0
148,post,MachineLearning,1n5kl6k,1756720559.0,"[P] Beaver: A DSL for Building Streaming ML Pipelines
Hi guys!

My name is Jason I am an Electrical and Computer Engineering student and for the last year I have been working on my thesis, in which I have developed Beaver¬†‚Äì a domain-specific language (DSL) designed to make building machine learning pipelines for streaming data (e.g., Kafka) much simpler and more accessible.

What is Beaver?

* A DSL that lets you define ML pipelines using a clear, declarative syntax (instead of complex Python code)
* Generates Python code that integrates with the¬†[River](http://riverml.xyz/latest/)¬†library for online ML and supports real-time data streams
* Includes built-in validation, analysis, and automatic dashboard generation

  
I'm making this post to ask for some feedback. I‚Äôve prepared a user testing experience with 3 tasks (from basic to advanced) that should take about 30-45 minutes. I‚Äôd love to hear your thoughts on usability, clarity, and the overall concept.

* üìñ¬†[Concept overview & docs](http://deepblue597.github.io/beaver-doc/)
* üìù¬†[User testing instructions](https://github.com/deepblue597/beaver/blob/user_testing/user_testing.md)
* ü¶´¬†[Example pipeline file](https://github.com/deepblue597/beaver/blob/user_testing/examples/linear.bvr)
* üí¨¬†[Feedback form](https://forms.gle/ioLVyvruJ2KCs6wd8)

Repo : [https://github.com/deepblue597/beaver](https://github.com/deepblue597/beaver)  
It is recommended to use the user\_testing branch for the feedback.   
  
Thank you so much for your time <3 ",4,0.0
149,post,MachineLearning,1n55r7s,1756673510.0,"[R] Measuring Semantic Novelty in AI Text Generation Using Embedding Distances
We developed a simple metric to measure semantic novelty in **collaborative text generation** by computing cosine distances between consecutive sentence embeddings. 

Key finding: Human contributions showed consistently higher semantic novelty than AI across multiple embedding models (RoBERTa, DistilBERT, MPNet, MiniLM) in our human-AI storytelling dataset. 

The approach is straightforward - just encode sentences and measure distances between consecutive pairs. Could be useful for evaluating dialogue systems, story generation models, or any sequential text generation task.

Some links:  
[Paper site](https://idanvidra.github.io/playing_along_paper_site/)    
[Code](https://github.com/idanvidra/Yes-And-Game-Paper)[Blog post with implementation details](https://medium.com/@idan.vidra/measuring-semantic-novelty-in-ai-generated-text-a-simple-embedding-based-approach-c92042c88338)

The work emerged from studying human-AI collaborative storytelling using improvisational theater techniques (""Yes! and..."" games).",7,6.0
150,post,MachineLearning,1n4ppbi,1756629936.0,"[D] Advanced NLP with Transformers: Full talk recording and GitHub repo
**Just gave a 1.5-hour talk on ""Advanced NLP with Transformers"" covering:**

* Transformer architecture
* Prompting, RAG and fine-tuning techniques
* AI safety, security and governance challenges
* Curated papers, fellowships and resources

**Resources:** üé• Recording: [https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s](https://www.youtube.com/watch?v=9WVtUDDcAXw&t=2330s) üíª GitHub: [https://github.com/vgcharan/Advanced-NLP-Workshop-2025](https://github.com/vgcharan/Advanced-NLP-Workshop-2025)

Designed for researchers, students and practitioners who want conceptual depth as well as practical references. Feedback and discussion are welcome!",0,0.0
151,post,MachineLearning,1n4l73x,1756613327.0,"[P] Why didn‚Äôt semantic item profiles help my GCN recommender model?
Hey everyone,

I‚Äôm working on a recommender system based on a GCN model for regression task ( predicting rating score). Normally, the model initializes user and item embeddings randomly, but I wanted to improve this by following a paper ( the diagram is presented above )  that integrates semantic item profiles as initial embeddings.

Here‚Äôs what I did:
	‚Ä¢	I generated structured item profiles with 3 parts using Gemini api : 
	‚Ä¢	[Summarization]: short description of the business.
	‚Ä¢	[User Preferences]: predicted/extracted types of users who‚Äôd like it.
	‚Ä¢	[Recommendation Reasoning]: explanation for why it fits.
	‚Ä¢	I also encoded metadata like review count and stars into natural language (e.g., review_count > 100 ‚Üí ""popular item"", avg_stars ~4.2 ‚Üí ""well-rated"").
	‚Ä¢	I used Gemini text embeddings to encode these profiles into fixed-size embeddings.
	‚Ä¢	Then I replaced the random item embeddings in my GCN with these semantic embeddings (after projecting them down to my model‚Äôs embedding size).

The issue:
	‚Ä¢	When I train the GCN with these semantic embeddings, performance actually gets worse compared to just using random initialization or identical. 

Could the item profiles themselves be ‚Äúbad‚Äù ?
",25,5.0
152,post,MachineLearning,1n4jdo7,1756607434.0,"[D] Monthly Who's Hiring and Who wants to be Hired?
**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",16,2.0
153,post,MachineLearning,1n4dqsc,1756590980.0,"üåüIntroducing Art-0-8B: Reasoning the way you want it to with Adaptive Thinkingüåü [R]
Hi everyone! Today I'm announcing a new experimental open-source model finetuned from Qwen3-¬†**Art-0-8B is the first reasoning model where users can explicitly control how the model thinks through prompts.**

Unlike normal reasoning models that only let you control the final output, Art-0-8B lets you control the actual thinking process. Tell it to ""think in rap lyrics"" or ""use bullet points to organize thoughts"" and it will literally reason that way before giving you an answer.

You can check out the model on HuggingFace:¬†[https://huggingface.co/AGI-0/Art-0-8B](https://huggingface.co/AGI-0/Art-0-8B)¬†(please leave a like in the repo if you like this model)

Let me know your thoughts!

P.s. If you are an AI researcher working solo, consider joining us, we are a decentralized research lab, you can read about our mission in this section of the model card¬†[https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab](https://huggingface.co/AGI-0/Art-0-8B#%F0%9F%94%97-join-the-agi-0-decentralized-research-lab)",12,0.0
154,post,MachineLearning,1n3g1p7,1756493920.0,"[D] How do we make browser-based AI agents more reliable?
I‚Äôve been experimenting with different approaches for giving AI agents the ability to use browsers in real workflows (data collection, QA automation, multi-step workflows). The promise is huge but the reliability problems are just as big:

1. Sessions break after login or CAPTCHA
2. Agents fail when sites change structure
3. Security is hard to guarantee at scale
4. Each framework has its own dialect / quirks


Recently I‚Äôve been looking into managed environments that abstract some of this away. For example, I am using hyperbrowser right now and it does provide a unified layer for running browser-based agents without setting up everything manually. 

But then my question is... Is there ongoing research or promising directions in making browser-agent interactions more robust? Are there known benchmarks, best practices, or papers that deal with these reliability issues?",34,15.0
155,post,MachineLearning,1n37qnu,1756474808.0,"[D] ollama/gpt-oss:20b can't seem to generate structured outputs.
I'm experimenting with `""ollama/gpt-oss:20b""`'s capability to generate structured outputs. For example, I used it to evaluate against GSM8K dataset. The schema is as follows: `answer`: for the answer, and `solution`: for the CoT solution. However, it doesn't make sense that for a 20B model, it cannot generate a valid structured output.

Any thoughts or hacks on this one? I would appreciate it. Thanks.",13,9.0
156,post,MachineLearning,1n2rvvh,1756424223.0,"[R] Technical Skills Analysis of Machine Learning Professionals in Canada
I manage a slack community of a couple hundred ML devs in Canada. I got curious and ran some numbers on our members to see if any interesting insights emerged. Here's what I found:

**The ""Pandemic ML Boom"" Effect**:  
Nearly 40% of members started an ML specific role between 2020-2022. 

**RAG and Vector Database Expertise**:  
Over 30% of members have hands-on experience with Retrieval-Augmented Generation systems and vector databases (Pinecone, Weaviate, ChromaDB), representing one of the hottest areas in enterprise AI.

‚Äç**Multi-modal AI Pioneers**:  
A significant portion of members work across modalities (vision + text, audio + text).

**Most Common Job¬†Titles**:

15% of members hold senior leadership roles (Principal, Staff, Director, CTO level), demonstrating strong senior representation within the community.

**ML-Engineering Bridge Roles**:

Over 35% of members hold hybrid titles that combine ML with other disciplines:¬†""MLOps Engineer,"" ""Software Engineer, ML,"" ""AI & Automation Engineer,"" ""Conversational AI Architect,"" and ""Technical Lead, NLP"".

You can see the full breakdown here: [https://revela.io/the-collective](https://revela.io/the-collective)",75,16.0
157,post,MachineLearning,1n2jekd,1756403937.0,"[R] [EMNLP 2025] CCPS: Confidence from Consistency under Perturbation of States ‚Äî Superior Calibration Performance Across Benchmarks/Models
Hi everyone,

Our paper **‚Äú*****Confidence from Consistency under Perturbation of States (CCPS)*****‚Äù** was accepted to the **EMNLP 2025 Main Conference**, placing in the **top 15% of accepted papers** with a **final meta-review rating of 9 (strong accept)**.

# üîç Motivation

LLMs don‚Äôt just make mistakes, they‚Äôre often confidently wrong. That‚Äôs fine when asking for trivia, but risky in domains like healthcare and finance. Reliable confidence estimation is critical for safe deployment.

# ‚ú® What is CCPS?

CCPS looks at the hidden states of an LLM. We apply small perturbations to the final hidden representations and observe how stable the prediction is:

* If the answer remains stable ‚Üí the model was truly confident.
* If the answer flips ‚Üí the confidence was unreliable.

This approach is simple, efficient, and does not require fine-tuning the base LLM.

# üìä Results

Across LLaMA, Mistral, and Qwen on MMLU and MMLU-Pro, CCPS outperformed prior methods like LitCab and Calibration Tuning (CT):

* **Calibration**: Error cut by more than 50%, down to \~4.5% on the toughest benchmarks.
* **Discrimination**: More accurate at telling right vs. wrong answers than prior SOTA (LitCab, CT, etc.).
* **Performance**: Boosts accuracy and robustness, all without fine-tuning the base LLM.

# üí° Why it matters

CCPS delivers more reliable, better-calibrated LLMs, models that don‚Äôt just generate answers but also provide trustworthy confidence signals. This is key for high-stakes AI applications, especially in the medical and finance industries.

# üìé Resources

* üìÑ Paper: [arXiv link](https://arxiv.org/abs/2505.21772)
* üíª Code: [GitHub repo](https://github.com/ledengary/CCPS)
* üìä Data: [HF Dataset](https://huggingface.co/datasets/ledengary/CCPS)

Happy to hear feedback, especially from anyone working on calibration, verifiers (for RL), or LLM deployment.",0,2.0
158,post,MachineLearning,1n2i7iy,1756401270.0,"[R] ‚ÄúHow I‚Äôm structuring a 16M character dialogue corpus for persona reconstruction in LLMs‚Äù
In the past weeks, I‚Äôve been working on a somewhat ‚Äúcrazy‚Äù project:
manually splitting and structuring 16 million characters of dialogue data, preparing it for feeding into a model to reconstruct a persona module.

Along the way, I‚Äôve noticed a few technical challenges:
	1.	File size balance
Keeping each file around 300k‚Äì400k characters is the most stable. Beyond that, performance tends to drop.
	2.	Context continuity
Poor segmentation can easily break the model‚Äôs sense of persona, resulting in inconsistent tone.
	3.	Tagging & classification
It‚Äôs not just about cutting text, but also annotating emotional states and tonal shifts, so the model can later rebuild ‚Äúmemory‚Äù in a coherent way.

This made me realize that large-scale corpus curation is itself a kind of language engineering.
It‚Äôs not just data processing ‚Äî it shapes whether an AI can emerge as a whole presence.

I‚Äôm curious:
In your NLP or LLM practice, how do you balance scale with contextual integrity?",0,5.0
159,post,MachineLearning,1n2gdd4,1756397137.0,"[R] Adding layers to a pretrained LLM before finetuning. Is it a good idea?
I'm doing a full fine-tune on the Qwen 3 14B Base model with around 10B tokens for loss. I'd have preferred a little higher capacity. My idea is to add a few more layers at the end, initialized close to zero, and then train. Perhaps increase from 40 to 50 layers.

This is straightforward to implement. Is there a reason why I don't hear of this being done? Is anyone familiar with this? Any research indicating success or failure? It makes sense conceptually but I would assume it would be more common if it works.

(I asked the GPT5, Gemini Pro & Claude, but I'm getting mixed answers. It'll agree or disagree depending how I phrase the question.)",10,16.0
160,post,MachineLearning,1n2c588,1756387471.0,"[D] Where to find vast amounts of schemas for AI model training?
**\[D\] Looking for massive schema collections for training models**

working on a project and need to find vast amounts of schemas for training models. specifically looking for financial data (transactions, market data, etc) and retail/ecommerce stuff (product catalogs, user behavior, sales data) but honestly need schemas from pretty much every domain I can get. anyone know where to find quality structured schemas at scale? open to paid sources too. need thousands of different schema types ideally. thanks!",0,0.0
161,post,MachineLearning,1n28w7j,1756378010.0,"[P] Built Sparrow: A custom language model/NLP tool for microcontrollers
Hey everyone,

Don't know if it fully matches this subreddit, but since there have been a lot of discussions around LLMs using a lot of power and water, and even more discussions around LLMs plateauing, as everyone focuses on making the biggest and most powerful model.

I've been super focused for a while now in bringing Language Models and complex NLP capabilities to microcontrollers and finally been able to finish the architecture and an ML Toolkit that enables training models from scratch, with this architecture and enables easy deployment on almost any MCUs.

The architecture uses state of the art methods, with many in-depth optimisations tested through over 1700 trained models, to get the most of every single memory byte and clock cycle, specifically for MCUs while also enabling extremely fast responses on PC.

The idea is to have domain specific and task specific models, using Sparrow's architecture, instead of a general prupose frontier model like ChatGPT/Llama etc. In the demo I showcase a Biology only model, that was made to give straight answrs (as per research papers showcasing that's what people want) for a question-answering chat-like system. Anything can be created. And then due to the model being only 50-200KB depending on how it is build (with twice that needed in total when flashed), mutiple models could be loaded in memory and a mixture-of-experts system can be designed. Which is what I want to explore with SPARROW 2.

I still have to see exactly how to proceed in terms of making the code open-source, best licensing methods, how to create the API, etc. But the idea is that it would be easy to create language models for MCUs, similar to how Sci-kit Learn is used for regular ML.

It supports encoder, decoder, encoder-decoder models, and the fastest model uses linear attention, but I have also been able to deploy dot attention and additive attention on the ESP32.

Let me know what you think!¬†[Here's a demo video](https://youtu.be/WCvv5W9gEiA?si=QCXvXei3qfp0qAG8)¬†with a ChatGPT simple-webapp to give people something they are familiar with. I'd also like to know opinions around the best way to go forward, release it as a website of sorts, release it as an API like Scikit Learn etc.

I have a lot of videos with the models running on PC with full phrases/paragraphs outputs in less than 10 miliseconds, have different versions Small, Main, Large running on the ESP32S3, have the Main flavour running on the ESP32P4 which can process everything 5-6 times faster due to the intrustions available, and outputting a phrase every 50-100ms, compared to ESP32S3's 300-600ms.",8,2.0
162,post,MachineLearning,1n23r3t,1756358475.0,"[R] Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies
**TL;DR.**¬†We introduce¬†**discrete diffusion**¬†as the action decoder¬†**inside a single transformer**¬†for VLA. Two simple components‚ÄîAdaptive decoding order and Secondary re-masking‚Äîyield consistent action refinement and outperform AR and continuous-diffusion heads. Trains with the¬†**same cross-entropy objective**¬†as VLMs, preserving pretrained priors. This design shows better success rates vs AR and continuous diffusion.  
**Disclosure:**¬†I‚Äôm an author.

**What‚Äôs new**

* **First discrete-diffusion action head for VLA**¬†(to our knowledge).
* **Single-transformer, VLM-style training:**¬†keeps the discrete token interface and uses the same CE loss as the VLM backbone ‚Üí¬†**maximizes retention of pretrained VLM priors**.
* **Adaptive decoding order:**¬†in each refinement round, we¬†**keep easy tokens first**¬†via confidence / confidence-gap scores and a cosine keep schedule; the rest remain masked for the next round.
* **Secondary re-masking:**¬†previously kept tokens are¬†**re-checked**¬†(threshold + residual-drop) and¬†**re-masked**¬†if uncertain/inconsistent, enabling robust cross-round error correction.

**Why it matters**

* For robotics manipulation tasks, unlike continuous diffusion decoders, our formulation keeps action generation inside a unified transformer and trains with the same cross-entropy objective used by VLMs. This¬†**preserves the backbone‚Äôs pretrained vision-and-language capability**‚Äîakin to extending a vocabulary‚Äîwhile opening a path to¬†**inherit unified transformers‚Äô scaling behavior**, paving the way for¬†**large-scale VLA**. Moreover, Discrete Diffusion VLA¬†**breaks the left-to-right bottleneck**¬†of AR decoders: action chunks are¬†**adaptively decoded in parallel**¬†over a small, fixed number of steps, and uncertain tokens can be revisited via iterative re-masking, leveraging full cross-modal context (including inter-action dependencies) for refinement.

**Links**

* Paper:¬†[https://arxiv.org/abs/2508.20072](https://arxiv.org/abs/2508.20072)
* Demo videos:¬†[https://huggingface.co/papers/2508.20072](https://huggingface.co/papers/2508.20072)",1,0.0
163,post,MachineLearning,1n1wm8n,1756337246.0,"[N] Unprecedented number of submissions at AAAI 2026
And 20K out of 29K submissions are from China (clearly dominating AI research now, well done to my Chinese friends). The review process at AI conferences isn't just broken - it's nuked. We need change, fast.

https://preview.redd.it/ih3vliracnlf1.png?width=1938&format=png&auto=webp&s=b7112a3e5e78ec7bcd0e6b100b5887a880fb82be",200,109.0
164,post,MachineLearning,1n1p7rb,1756319712.0,"[D] I reviewed 100 models over the past 30 days. Here are 5 things I learnt.
I reviewed 100 models over the past 30 days. Here are 5 things I learnt.

TL;DR: Spent a month testing every AI model for work, a few tools I'm building and RL. Build task-specific evals. Most are overhyped, a few are gems, model moats are ephemeral, and routers/gateways are the real game-changer.

So I've been building a few evaluation tools, RHLF and RL environments for the past few months so I decided to be extra and test literally everything.

100 models. 30 days. Too much coffee :( Here's what I found:  
  
1. Model moats are ephemeral

Model moats don't last and it can be hard to pay for many subscriptions if you're building for users and machines. What's SOTA today gets beaten in 2 months. Solution: Use platforms like Groq, OpenRouter, FAL, Replicate etc

My system now routes based on task complexity: Code generation, Creativity, Complex reasoning and Code generation.

2. Open source FTW

The gap is closing FAST. Scratch that. The gap between open and closed models has basically disappeared. If you're not evaluating open-source options, you're missing 80% of viable choices. From Deepseek, Qwen to Kimi, these models help you build quick MVPs at little or no cost. If you do care about privacy, Ollama and LMStudio are really good for local deployment.

3.Benchmarks are mostly decieving due to reward hacking

Benchmaxxing is a thing now. Models are increasingly being trained on popular eval sets, and it's actually annoying when models that scored ""high"" but sucked in practice. It's also why I'm a huge fan of human preference evaluation platforms that are not easily gamed (real world vs benchmarks). Build your own task-specific evals.

4.Inference speed is everything

Speed matters more than you think. Users don't care if your model is 2% more accurate if it takes 30 seconds to respond. Optimize for user experience, not just accuracy. Which leads me to..

5.Task-specific models > general purpose models for specialized work.

No 4 is also a huge reason why I'm a huge fan of small models finetuned for special tasks. Model size doesn't predict performance.

Test small models first etc Llama 3.2 1B, smolLLM, moondream etc and see if you can get a huge boost by finetuning them on domain tasks rather than just deploying a big SoTA general purpose model. Cost way lesser and usually faster.

What models are in your current prod stack? Any hidden gems I missed in the open source space?",0,15.0
165,post,MachineLearning,1n1mboq,1756313306.0,"[P] Implemented GRPO on top of Karpathy's makemore
Hey all! I wanted to share my recent project where I implemented the GRPO (Group Relative Policy Optimization) algorithm on top of the [makemore](https://github.com/karpathy/makemore) repo.

I wanted to understand how the algorithm works and was trying to find small-scale toy problems where I can implement my own version and see if it works. I had a couple of ideas at first but then I settled on this one idea: to implement the algorithm on top of the makemore project where my goal would be to finetune the character-level language model to generate names with more vowels! So the reward is essentially the number of vowels you have in the generated names.

GRPO is actually a simplified version of PPO (which itself is a derivative of TRPO), and while its predecessors are rather complicated to fully grasp unless you have some background in policy gradient or RL in general, GRPO is much simpler to understand and code up (e.g., you don't have to worry about writing Generalized Advantage Estimation etc.)

Feel free to take a look and share your thoughts! Here's the repo: [https://github.com/souvikshanku/makemore-grpo/](https://github.com/souvikshanku/makemore-grpo/)",14,0.0
166,post,MachineLearning,1n1k9ty,1756308731.0,"[R] ArchiFactory : Benchmark SLM architecture on consumer hardware, apples to apples
[35M Parameters : RWKV vs Mamba vs GQA vs RetNet](https://preview.redd.it/vul29llezklf1.png?width=1106&format=png&auto=webp&s=c951d5647cd895d418b5a0863184cf9f6745397e)



Since it's introduction, the Attention mechanism has been king in LLM architecture, but a few vaillant projects like RWKV, Mamba, Retnet, LiquidAI have been proposing several new mixin mecanisms over time, to attempt to dethrone the king.



One of the major issue is that LLM pretraining is extremely dependant on number of parameters and dataset choices, so performing an ablation study on new architecture is not an easy tricks.



On the other hand, I met many people with brillant ideas for new architecture and who never got the chance to put it to the test.

For that purpose, i create ArchiFactory, a simple (<500 lines of codes) and modular repo that enables to pretrain Small Language Models with comparable parameter count and architecture tricks, in a couple of hours on a single 3090 level GPU.



Included:

\- simple modular architecture to be sure to compare similar stuff

\- complete optimized training loop using pytorch lightning

\- fp8 training (can achieve <20min training on 5090 grade GPU)

\- examples of common modules like FFN, MOE, GQA, Retnet, Mamba, RWKV6 etc.

\- guidelines to test integrate new modules



Link: [https://github.com/gabrielolympie/ArchiFactory](https://github.com/gabrielolympie/ArchiFactory)",22,0.0
167,post,MachineLearning,1n1ebmk,1756293760.0,"[R] Computational power needs for Machine Learning/AI
Hi everyone!

As part of my internship, I am conducting research to understand the computational power needs of professionals who work with machine learning and AI. The goal is to learn how different practitioners approach their requirements for GPU and computational resources, and whether they prefer cloud platforms (with inbuilt ML tools) or value flexible, agile access to raw computational power.

If you work with machine learning (in industry, research, or as a student), I‚Äôd greatly appreciate your participation in the following survey. Your insights will help inform future solutions for ML infrastructure.

The survey will take about two to three minutes. Here¬¥s the link:¬†[https://survey.sogolytics.com/r/vTe8Sr](https://survey.sogolytics.com/r/vTe8Sr)

Thank you for your time! Your feedback is invaluable for understanding and improving ML infrastructure for professionals.",0,13.0
168,post,MachineLearning,1n0zndc,1756246524.0,"[D] Laptop Suggestion for PhD in ML for Robotics
Hi!

I'll be starting a PhD in ML for Robotics (RL, Sensor Fusion etc.) and was wondering which laptop would be best to support me throughout the next 4 years. I am looking for a powerful laptop, with good battery life, not too heavy and that is robust.

My budget is $3000.

So far, I have identified the following laptops, but am unsure which would be the best choice.

\-¬†**Razer Blade 16**¬†(either RTX 5070 Ti + 32GB RAM ($3100) or RTX 5080 + 64GB ($4050)): apart from battery life which is not the most ideal, would I see a significant difference when running RL simulations (IsaacGym) or large multimodal (video, imu, ...) ML models between both configurations? Price difference between both configurations is \~$850 (with taxes) which is significant.

\-¬†**MSI Vector 16¬†HX¬†AI**¬†(RTX‚ÄØ5080, 64‚ÄØGB) - $2600

\-¬†**ThinkPad P1 Gen 7**¬†(RTX Ada 3000, 64GB) - $3200: has a good battery life, but its GPU is Ada series, which is not the best for RL simulations.

\-¬†**Legion Pro 7i Gen10**¬†(RTX 5080, 32GB) - $3100: the legions are usually very heavy laptops.

Essentially, I am looking for a laptop that will be somewhat future-proof to the fast pace of new GPUs coming out, is powerful for my intended use (RL simulations + ML sensor fusion), has a good battery life (for note-taking in courses) and easily transportable (ie. neither too bulky nor heavy). Also, do I require RTX 5080 (recommended for IsaacSim) as GPU, and how big a diffference is 32GB vs 64GB RAM?

Thank you in advance for any suggestions or feedback!

EDIT: I have access to cluster, but thought having powerful laptop could be useful when running real-time inference on robot + working with smaller models / testing out stuff before training on cluster.",0,21.0
169,post,MachineLearning,1n0wdsi,1756238884.0,"[R] What makes active learning or self learning successful ?
Maybe I am confused between two terms ""active learning"" and ""self-learning"". But the basic idea is to use a trained model to classify bunch of unannotated data to generate pseudo labels, and train the model again with these generated pseudo labels.  Not sure ""bootstraping"" is relevant in this context.

A lot of existing works seem to use such techniques to handle data. For example, SAM (Segment Anything) and lots of LLM related paper, in which they use LLM to generate text data or image-text pairs and then use such generated data to finetune the LLM.

My question is why such methods work?  Will the error be accumulated since the pseudo labels might be wrong?",0,5.0
170,post,MachineLearning,1n0vcrb,1756236524.0,"[R] ŒîAPT: critical review aimed at maximizing clinical outcomes in AI/LLM Psychotherapy
Hi reddit, wanted to share my thesis on AI / LLM psychotherapy @ [https://osf.io/preprints/psyarxiv/4tmde\_v1](https://osf.io/preprints/psyarxiv/4tmde_v1?fbclid=IwZXh0bgNhZW0CMTAAYnJpZBExNHhlVkhlWWpDVE1xN3dTeAEeoTtZ3pOVtRD7ODEFZo_qpyjjOEkW_2OFHqsH36X4xp7THoZC3F7YFDc1zJU_aem_Etq7yhCr4L3eA8v9QqrFgw)

Since the rules for this subreddit require more than just a link, I thought I'd share some surprising conclusions in plain english. 

**1. AI therapy research tends to use arbitrary success metrics:** the majority of LLM research on psychotherapy uses theraputic-sounding ad-hoc metrics (e.g. ""empathy"" as rated by LLM-as-judge), and not actually improvement in clients or other validated metrics. There's a real risk in AI researchers testing techniques and drawing conclusions when totally unrelated to the purpose of therapy (e.g. quality-of-life improvement). If you're interested in learning more about this issue, section 1.4 focuses on it, and offers the north-star alternatives commonly used in psychotherapy research in sections 1.1-1.3. 

**2. AI therapy tools (APTs) are already comparable to human therapists:** There's two studies from 2025 (Limbic, Therabot) that demonstrate non-inferior clinical outcomes in LLM-driven APTs and human therapists for depression & anxiety symptom reduction. If replicated, that's huge. That's a step-level jump in clinical from the previous generation of rules-based APTs (e.g. Woebot, Wysa), highlighting that maybe the generative properties of LLMs were the key gap to improve clinical performance. There's a lot more to say on these results, and if you're interested sections 2 & 3.1 talk more about them and put them into clinical context. 

3. **ŒîAPT allows predicting future clinical outcomes :** It's actually surprising that APTs perform at the lower-bounds of human therapists, since they kinda suck right now. The predictive model I proposed is that APTs clinical performance is boosted by advantages therapist can't compete with (e.g. 24/7 availability, low cost), while being depressed by current disadvantages (e.g. poor therapy skills, hallucinations, sycophancy, inconsistencies, bias). All of this playing out while major issues around legality, safety, privacy and ethics are unresolved and could shutdown the field. If you're intersted, you can read more about the model (section 3.3),  the advantages of APTs over human therapists (section 3.4), APTs' current limitations (section 3.5), and the key risks (section 3.6). 

https://preview.redd.it/rof96tmbuelf1.png?width=1162&format=png&auto=webp&s=5a1e81bbb9e8b12b09210967da97b2fe96816df0

  
**4. Techniques teaching LLM therapy:** Most people on this subreddit won't be surprised to learn you can teach LLM to perform therapy using a combination of context/prompt engineering, fine-tuning, multi-agent architecture, and ML models. What is surprising is that both clinically-validated APTs use ML models to offset the stochastic nature of LLMs, especially for safety purposes. Also surprising is that neither used a multi-agentic architecture. Therabot used fine-tuning on synthetic dialogues, and Limbic used context-engineering techniques. You can learn more about implementing therapy skills in LLM through context/prompt engineering (section 4.1), fine-tuning (section 4.2), multi-agent architectures (section 4.3), ML models (4.4). Around fine-tuning / pretraining there's a really nested conversation about data requirements, ethically sourcing transcripts, and choosing therapy modalities in section 4.1. 

https://preview.redd.it/lbcoovvc0flf1.png?width=2246&format=png&auto=webp&s=f029fed00649b4cca0ddb84d9830ded03f5f94ea

5. **Overall, most disadvantages of LLMs are addressable in AI therapy**: Reading the literature critiquing APTs it's really easy to get discouraged thinking for examples ""oh wow, hallucinations are going to make AI therapy impossible"". But actually, there's a bunch of techniques that can be used to mitigate the issues LLMs currently have. Combining the lowering rates of issues in newer LLMs released with mitigation techniques, most issues can theoretically be significantly mitigated in production. The outlier here being sycophancy which doesn't appear to have great mitigations on subjective topics. You can read more about the issues of LLMs in APTs and how to mitigate those in section 5. 

**6. video therapy with multi-modal audio/video LLMs:** One surprising fact from psychotherapy research is that therapy done over video (e.g. zoom) is actually as effective as in-person therapy. Ideally, LLMs would be able to pickup and transmit non-verbal cues over video-audio. Having an virtual therapy avatar using audio & video to attune to clients isn't actually that far off based on my literature review. Surprisingly it seems that emotional speech, and attuning to clients facial and body expressions are ready for implementation in AI therapy today. More on that in section 6.

Happy to have a conversation, receive critique, and answer questions here. This summary above was meant to offer informal insights into what is an otherwise quite lengthy paper. For more formal discussion and details, it's really best to read the paper. ",117,5.0
171,post,MachineLearning,1n0r8b7,1756227256.0,"I built a tool to benchmark tokenizers across 100+ languages and found some wild disparities [R]
**TL;DR:** Created [tokka-bench](https://tokka-bench.streamlit.app/) to compare tokenizers across languages. Turns out your fine-tune's multilingual performance might suck because of tokenization, not architecture. Also explains why proprietary models (Claude, GPT, Gemini) are so much better at non-English tasks.

**Links:**

* [Live dashboard](https://tokka-bench.streamlit.app/)
* [Full blog post](https://www.bengubler.com/posts/2025-08-25-tokka-bench-evaluate-tokenizers-multilingual)
* [GitHub repo](https://github.com/bgub/tokka-bench)

https://preview.redd.it/7i03jela9elf1.png?width=1724&format=png&auto=webp&s=95378457970e6337b147e71d7a8f0ab2dd67cb91

# The Problem Nobody Talks About

I started this as a side quest while pretraining a multilingual model, but tokenization turned out to be way more important than expected. There are two hidden layers creating massive efficiency gaps:

**UTF-8 encoding differences:**

* English: \~1 byte per character
* Arabic: 2+ bytes per character
* Chinese: 3+ bytes per character

**Tokenization bias:** Most tokenizers are trained on English-heavy data, so they allocate way more vocabulary to English patterns. These compound into serious problems.

# Why This Affects Performance

**During training:** If you allocate tokens proportionally (10M English, 1M Khmer), the Khmer text has WAY less semantic content because it needs more tokens per word. Plus Khmer tokens end up being character-level instead of semantic units, making concept storage much harder.

**During inference:** Low-resource languages need 2-3x more tokens per sentence:

* Slower throughput (costs more to serve)
* Context windows fill up faster
* More chances to mess up during generation

# What I Built

tokka-bench measures four key things:

1. **Efficiency** \- bytes per token (compression quality)
2. **Coverage** \- unique tokens used (script representation)
3. **Word splitting** \- how often semantic units get fragmented
4. **Subword fertility** \- average tokens per semantic unit

# Interesting Findings

You can actually reverse-engineer training data from tokenizer performance:

* Kimi K2: Exceptional Mandarin coverage (obviously Chinese-trained)
* Gemma 3: Strong Urdu/Hindi performance
* gpt-oss: Good Arabic/Gujarati coverage

Weirdest finding: Programming languages show almost identical efficiency across all tokenizers. Probably because everyone trains on GitHub with similar language distributions.

# Technical Details

Built on high-quality datasets (FineWeb, FineWeb-2, StarCoder). Samples 2MB per language and calculates per-language metrics. Has some limitations around cross-linguistic comparison due to UTF-8 differences, but great for comparing tokenizers on the same language.

Shoutout to Judit √Åcs for the original subword fertility metrics and Rust et al's ACL paper that laid the groundwork.

**PS:** if you're from an AI lab and want to contribute your tokenizer's metrics (even if proprietary), please reach out! The community would benefit a lot from understanding how SOTA systems handle this stuff.

*Posted this on LinkedIn/Twitter already but figured* r/MachineLearning *would appreciate the technical details. Happy to answer questions about methodology or findings!*",83,23.0
172,post,MachineLearning,1n0qwzm,1756226565.0,"[D] Analyzed 402 healthcare ai repos and built the missing piece
I looked through 402 healthcare AI repos on GitHub and found almost 50% of infrastructure tools are just solving data format conversion problems, suggesting a systematic gap between ML research and deployment in clinical settings.

Built HealthChain to bridge Python ML workflows with healthcare data standards (FHIR, HL7, etc.) without the usual pain. 4 years of NHS NLP development experience went into making this feel like normal Python.

Post + pretty graphs: https://open.substack.com/pub/jenniferjiangkells/p/healthchain-building-the-tool-i-wish?r=4o6h4

Code: https://github.com/dotimplement/HealthChain

Anyone else work in healthcare AI here? Would love to learn what you‚Äôre working on!",11,1.0
173,post,MachineLearning,1n0q4d9,1756224781.0,"[D] Looking for a self-hosted alternative to Modal.com for running ML workloads
Hey folks 



I've been using [Modal.com](http://Modal.com) (I am not affiliated) for a while to run machine learning workloads in the cloud, and I really like its simplicity, container-based execution, and ability to scale on demand. However, I'm starting to explore more self-hosted options due to cost reasons and to gain more control over the infrastructure while building apps.



Does anyone know of good self-hosted alternatives that offer similar functionality? Ideally, something that:



\- Supports containerized jobs (Docker or similar)

\- Can run Python/ML workloads easily

\- Has a nice API  for launching jobs (this is important) 

\- Offers some kind of job orchestration or scheduling

\- Bonus: GPU support and autoscaling would be amazing





Thanks in advance 

",4,4.0
174,post,MachineLearning,1n0njtk,1756218944.0,"[R] Exploring interpretable ML with piecewise-linear regression trees (TRUST algorithm)
A recurring challenge in ML is balancing **interpretability** and **predictive performance**. We all know the classic tradeoff: simple models like linear regression or short CART-style regression trees are transparent but often lack enough accuracy, while complex ensembles like Random Forests and XGBoost are accurate but opaque.

We‚Äôve been working on a method called **TRUST** (*Transparent, Robust and Ultra-Sparse Trees*). The core idea is to go beyond constant values in the leaves of a tree. Instead, TRUST fits a sparse regression model (either linear or constant) in each leaf, resulting in a **piecewise-linear tree** that remains interpretable.

In our [recent paper](https://arxiv.org/abs/2506.15791), accepted at PRICAI 2025, we compared this method against a range of models on 60 datasets. While we were encouraged by the results ‚Äî TRUST consistently outperformed other interpretable models and closed much of the accuracy gap with Random Forests ‚Äî we'd like to hear your thoughts on this topic.

The problem we‚Äôre tackling is widespread. In many real-world applications, a ""black box"" model isn't an option. We've often found ourselves in situations where we had to choose between a sub-par interpretable model or an accurate but untrustworthy one.

Here‚Äôs a concrete example from a [tutorial on explaining EU life satisfaction](https://github.com/adc-trust-ai/trust-free/blob/main/notebooks/trust-free_tutorial.ipynb).

[TRUST produces a single interpretable tree, while Random Forest uses hundreds of deep trees to achieve similar accuracy.](https://preview.redd.it/3tzdaim3kdlf1.png?width=2600&format=png&auto=webp&s=e289771608b0d74498dc83b39c1efd2670ed8ea9)

As the image above shows, both TRUST and a Random Forest achieve \~85% test R¬≤ ‚Äî but one produces a **single interpretable tree**.

TRUST is implemented as a free Python package on PyPI called `trust-free`.

**Discussion:** How do you usually handle the interpretability vs. accuracy tradeoff in your own regression projects? What methods, beyond the standard ones, have you found effective? We‚Äôre looking forward to hearing your perspectives.",12,7.0
175,post,MachineLearning,1n0e7s1,1756188829.0,"[D]How can AI teams stay agile and adaptable when project goals or data requirements change midstream?
For those working in AI/ML, how do you keep your teams agile when project goals or data requirements shift halfway through a project? I‚Äôve seen situations where a model was nearly production-ready, but then stakeholders introduced new objectives or the data pipeline changed, forcing big pivots.
",0,2.0
176,post,MachineLearning,1n0d12h,1756184492.0,"[D] An honest attempt to implement ""Attention is all you need"" paper
I have started working on implementing actual research papers in machine learning and I have started with ""Attention is all you need"" paper.

I have implemented all the code and it is an educational attempt. I would like you to get some eyes on the repo from the members of this subreddit and get your opinion. This is still a work in progress but your reviews and PRs are really appreciated. I have written the code focusing on educational purposes and not optimisations. Please take a look below.

[https://github.com/MayukhSobo/Transformer](https://github.com/MayukhSobo/Transformer)

Edit: I would like to clarify that some of the code related to helper functions and all the doc strings are implemented by Claude not because they are difficult to do but they are simply boring. The core architecture is implemented by me. Also at no point I claimed that this is my own work and I haven't used AI. The part which really required me to code and not use AI, I did it on my own. If you really think that the complete code is just a result of some vibe coding, I welcome you to try that with most advanced AI tools and see if you can reproduce even 70% of what I did or not. ",62,18.0
177,post,MachineLearning,1n055zr,1756161910.0,"[P] Training LLMs without code - Would you use it?
https://preview.redd.it/vy1h49l0t8lf1.png?width=3456&format=png&auto=webp&s=1c0991294abf01d6699c04b663cd30973e4bd633

Is Vibe training AI models something people want?   
  
I made a quick 24hours YC hackathon app that wires HF dataset lookups + Synthetic data pipeline + Trnasfomers too quickly fine tune a gemma 3 270m on a mac, I had 24hours to ship something and now have to figure out if this is something people would like to use?   
  
Why this is useful? A lot of founders I've talked to want to make niche models, and/or make more profit (no SOTA apis) and overall build value beyond wrappers. And also, my intuition is that training small LLMs without code will enable researchers of all fields to tap into scientific discovery. I see people using it for small tasks classifiers for example. 

For technical folk, I think an advanced mode that will let you code with AI, should unleash possibilities of new frameworks, new embedding, new training technics and all that. The idea is to have a purposeful built space for ML training, so we don't have to lean to cursor or Claude Code. 

I'm looking for collaborators and ideas on how to make this useful as well?

Anyone interested can DM, and also signup for beta testing at [monostate.ai](http://monostate.ai)  
  
Somewhat overview at [https://monostate.ai/blog/training](https://monostate.ai/blog/training)  

\*\*The project will be free to use if you have your own API keys!\*\* 

In the beginning no Reinforcement learning or VLMs would be present, focus would be only in chat pairs fine tuning and possibly classifiers and special tags injection! 

Please be kind, this is a side project and I am not looking for replacing ML engineers, researchers or anything like that. I want to make our lifes easier, that's all. ",0,9.0
178,post,MachineLearning,1mzxtzb,1756144953.0,"[D]GEPA: Reflective Prompt Evolution beats RL with 35√ó fewer rollouts
A new preprint (Agrawal et al., 2025) introduces¬†**GEPA (Genetic-Pareto Prompt Evolution)**, a method for adapting compound LLM systems. Instead of using reinforcement learning in weight space (GRPO), GEPA mutates prompts while reflecting in natural language on traces of its own rollouts.

The results are striking:

* GEPA outperforms GRPO by up to¬†**19%**¬†while using¬†**35√ó fewer rollouts**.
* It also consistently surpasses MIPROv2, the state-of-the-art prompt optimizer.
* In many cases, only a few hundred rollouts were sufficient, compared to tens of thousands for RL .

The shift is conceptual as much as empirical: Where RL collapses complex trajectories into a scalar reward, GEPA treats those trajectories as¬†*textual artifacts*¬†that can be reflected on, diagnosed, and evolved. In doing so, it makes use of the medium in which LLMs are already most fluent, language, instead of trying to push noisy gradients through frozen weights.

What‚Äôs interesting is the infra angle: GEPA‚Äôs success in multi-hop QA hinges on generating better second-hop queries.¬†**That implicitly elevates retrieval infrastructure Linkup, Exa, Brave Search into the optimization loop itself**. Likewise, GEPA maintains a pool of Pareto-optimal prompts that must be stored, indexed, and retrieved efficiently.¬†**Vector DBs such as Chroma or Qdrant are natural substrates for this kind of evolutionary memory.**

This work suggests that the real frontier may not be reinforcement learning at scale, but¬†**language-native optimization loops**¬†where reflection, retrieval, and memory form a more efficient substrate for adaptation than raw rollouts in parameter space.

https://preview.redd.it/5l4lcmokg7lf1.png?width=1602&format=png&auto=webp&s=719e33f34feb5103ed1f375d3366745dd3415d77

",51,15.0
179,post,MachineLearning,1mzsrt2,1756133888.0,"[D] Too much of a good thing: how chasing scale is stifling AI innovation
Dear¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)¬†friends,

Hello everyone! I hope you are all doing well out there.

I've been observing a pattern in the AI research field that I can only describe as a ""Mass Amnesia."" It seems we're forgetting the valuable research paths we were on before the ChatGPT moment.

In my latest blog post, I argue that while scaling up LLMs was initially a courageous endeavour, the current obsession and monoculture around it is actively keeping us stuck. Instead of building on a diverse set of ideas, we're chasing a single approach, which I believe is making us amnesiacs about what came before and what's possible.

I'd love for you to read my spicy takes and share your own. Let's tear my arguments and ideas apart. ;)

üîó¬†**Full Article:**[https://pieces.app/blog/the-cost-of-ai-scaling](https://pieces.app/blog/the-cost-of-ai-scaling)

I look forward to your arguments and thoughts.

Regards,

Antreas

  
PS. This is a repost of [https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d\_too\_much\_of\_a\_good\_thing\_how\_chasing\_scale\_is/](https://www.reddit.com/r/MachineLearning/comments/1mu28xl/d_too_much_of_a_good_thing_how_chasing_scale_is/) because it was removed without any explanation and the mods never replied to my queries on what was done wrong and how I could modify the post so it would abide by whatever rule I inadvertently tripped on.  

The post was starting to get some real discussion going when it was removed and wanted to give this another chance as I want to hear what everyone has to say and engage in discourse. ",13,26.0
180,post,MachineLearning,1mzpoo4,1756126455.0,"[P] Open-Source Agentic AI for Company Research
I open-sourced a project called Mira, an agentic AI system built on the OpenAI Agents SDK that automates company research.

You provide a company website, and a set of agents gather information from public data sources such as the company website, LinkedIn, and Google Search, then merge the results into a structured profile with confidence scores and source attribution.

The core is a Node.js/TypeScript library (MIT licensed), and the repo also includes a Next.js demo frontend that shows live progress as the agents run.

GitHub: [https://github.com/dimimikadze/mira](https://github.com/dimimikadze/mira)",0,0.0
181,post,MachineLearning,1mzp8au,1756125204.0,"[R] Got 6min? I need YOUR help for my PhD!
Hello everyone!

My name is Virginie and I am a first-year French PhD student¬†**studying human‚Äìartificial intelligence interactions.**

I am conducting a¬†**very quick**¬†(approximately 6 minutes) and¬†**anonymous online study**.

To ensure reliable results, I need at least 300 AI users, some of whom should have experience in integrating or designing AI models, although this is not compulsory for taking part!

If you are 18 or over, you can take part by clicking this link:

[https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en](https://virginie-lepont.limesurvey.net/967745?newtest=Y&lang=en)

The survey is¬†**also available in French.**

Every response is valuable! Thank you so much for your help!

Virginie 

*This post has been approved by one moderator of this group.* 

https://preview.redd.it/gwtpg6p9t5lf1.jpg?width=940&format=pjpg&auto=webp&s=39e54c6e762ab220af6a1c32d8754d8c9b5ee34c

",1,16.0
182,post,MachineLearning,1mzmrm5,1756117538.0,"[P] aligning non-linear features with your data distribution
For some time I've been fascinated by adopting knowledge from approximation theory into ML feature engineering, and I'm sharing my learnings in a series of blog posts, mainly about various polynomial bases as features.

So here is the latest one: [https://alexshtf.github.io/2025/08/19/Orthogonality.html](https://alexshtf.github.io/2025/08/19/Orthogonality.html)

It discusses my understanding of orthogonal bases as informative feature generators. I hope you enjoy reading as I enjoy learning about it.",18,3.0
183,post,MachineLearning,1mzd5kt,1756084269.0,"[D] Views on LLM Research: Incremental or Not?
Hi folks,  
Fellow ML researcher here üëã

I‚Äôve been working in the LLM space for a while now, especially around *reasoning models* and *alignment* (both online and offline).

While surveying the literature, I couldn‚Äôt help but notice that a lot of the published work feels‚Ä¶ well, incremental. These are papers coming from great labs, often accepted at ICML/ICLR/NeurIPS, but many of them don‚Äôt feel like they‚Äôre really pushing the frontier.

I‚Äôm curious to hear what the community thinks:

* Do you also see a lot of incremental work in LLM research, or am I being overly critical?
* How do you personally filter through the ‚Äúnoise‚Äù to identify genuinely impactful work?
* Any heuristics or signals that help you decide which papers are worth a deep dive?

Would love to get different perspectives on this ‚Äî especially from people navigating the same sea of papers every week.

  
PS: Made use of GPT to rewrite the text, but it appropriately covers my view/questions",52,26.0
184,post,MachineLearning,1mz9ruc,1756075122.0,"[P] AI Learns to play Sonic 2 Emerald Hill (Deep Reinforcement...
Hello everyone!!! I have several Reinforcement Learning projects underway. One is Sonic 2 with PPO. The other is developing an environment that supports games not available with Farama Group's stable-retro. I may need collaborators for the latter. I don't know if I'll integrate it into their project, stable-retro, in the future. One thing I've already achieved is running PCSX2 (it's missing the state loading option), and I'm creating a Python lib to load with stable-baselines3, etc. If anyone is interested, the links to both projects are below:

[https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning](https://github.com/paulo101977/Sonic-2-Genesis-Reinforcement-Learning)

[https://github.com/paulo101977/sdlarch-rl](https://github.com/paulo101977/sdlarch-rl)I also started a PCSX2 environment with direct access to the Python process, but I'll abandon it as it's very slow.

  
",0,1.0
185,post,MachineLearning,1myr68a,1756027938.0,"[D] Exploring Local-First AI Workflow Automation
**[D] Exploring Local-First AI Workflow Automation**

Hi all,  

I‚Äôve been experimenting with an open-source approach to AI workflow automation that runs entirely **locally** (no cloud dependencies), while still supporting real-time data sources and integrations. The goal is to provide a **privacy-first, resource-efficient alternative** to traditional cloud-heavy workflow tools like Zapier or n8n, but with LLM support integrated.

üëâ My question for the community:  
How do you see **local-first AI workflows** impacting ML/AI research, enterprise adoption, and robotics/IoT systems where privacy, compliance, and cost efficiency are critical?  

- Repo: [Agentic Signal](https://github.com/code-forge-temple/agentic-signal) (open-source, AGPL v3 / commercial dual license)  
- Demo video: [YouTube link](https://youtu.be/62zk8zE6UJI)  

Would love feedback from both the research and applied ML communities on potential use cases, limitations, or challenges you foresee with this approach.  

Thanks!  
",0,4.0
186,post,MachineLearning,1mylqrb,1756008073.0,"[R] Building a deep learning image model system to identify BJJ positions in matches
Hey all, I'm working on developing AI models that can classify and track positions throughout BJJ matches - and I'm keen to get some thoughts on this idea early on.

You can check it out here:¬†[https://bjjhq.ai/](https://bjjhq.ai/)

Ultimately BJJHQ provides an interactive positional timeline beneath match videos, showing all position changes throughout the match, so you're able to instantly jump to specific positions and see how transitions unfold.

The idea is that people would be able to search for not only a competitor, but a specific position and combination (e.g., ""Gordon Ryan in back control""), and instantly access all matches where that scenario occurs. You would also be able to filter and sort matches by time spent in specific positions.

Roadmap:

* Expanding the match database and position categories
* Technique/submission recognition
* Automated scoring system built on this positional foundation

Would love to know if anyone would be interested to chat or collaborate on this project ... please reach out if keen!

Thanks for any feedback!",3,6.0
187,post,MachineLearning,1myj9jk,1756000167.0,"[R] routers to foundation models?
Are there any projects/packages that help inform an agent which FM to use for their use case? Curious if this is even a strong need in the AI community? Anyone have any experience with ‚Äúrouters‚Äù?

Update: especially curious about whether folks implementing LLM calls at work or for research (either one offs or agents) feel this as a real need or is it just a nice-to-know sort of thing? Intuitively, cutting costs while keeping quality high by routing to FMs that optimize for just that seems like a valid concern, but I‚Äôm trying to get a sense of how much of a concern it really is

Of course, the mechanisms underlying this approach are of interest to me as well. I‚Äôm thinking of writing my own router, but would like to understand what‚Äôs out there/what the need even is first",8,20.0
188,post,MachineLearning,1mybwih,1755980376.0,"[D] How did JAX fare in the post transformer world?
A few years ago, there was a lot of buzz around JAX, with some enthusiasts going as far as saying it would disrupt PyTorch. Every now and then, some big AI lab would release stuff in JAX or a PyTorch dev would write a post about it, and some insightful and inspired discourse would ensue with big prospects. However, chatter and development have considerably quieted down since transformers, large multimodal models, and the ongoing LLM fever. Is it still promising? 

Or at least, this is my impression, which I concede might be myopic due to my research and industry needs. ",148,73.0
189,post,MachineLearning,1mxih41,1755896815.0,"[P] Relational PDF Recall (RFC + PoC) ‚Äì Structured storage + overlay indexing experiment
I‚Äôve been exploring how far we can push¬†*relational database structures inside PDFs*¬†as a substrate for AI recall. Just published a first draft RFC + PoC:

* Channel splitting (text/vector/raster/audio streams)
* Near-lossless transforms (wavelet/FLAC-style)
* Relational indexing across channels (metadata + hash linking)
* Early geometry-only overlays (tiling + Z-order indexing)

Repo + notes:¬†[https://github.com/maximumgravity1/relational-pdf-recall](https://github.com/maximumgravity1/relational-pdf-recall)

This is still very early (draft/PoC level), but I‚Äôd love feedback on:

* Whether others have tried similar recall-layer ideas on top of PDFs.
* If this approach overlaps with knowledge-graph work, or if it opens a different lane.
* Pitfalls I might be missing re: indexing/overlays.

  
**UPDATE 1: üìå Repo + DOI now live**   
GitHub: [https://github.com/maximumgravity1/pdf-hdd-rfc](https://github.com/maximumgravity1/pdf-hdd-rfc)  
DOI (always latest): [https://doi.org/10.5281/zenodo.16930387](https://doi.org/10.5281/zenodo.16930387)",0,0.0
190,post,MachineLearning,1mx775g,1755870929.0,"[D] Low-budget hardware for on-device object detection + VQA?
Hey folks,

I‚Äôm an undergrad working on my FYP and need advice. I want to:

* Run¬†object detection¬†on medical images (PNGs).
* Do¬†visual question answering¬†with a ViT or small LLaMA model.
* Everything fully¬†on-device¬†(no cloud).

Budget is tight, so I‚Äôm looking at Jetson boards (Nano, Orin Nano, Orin NX) but not sure which is realistic for running a quantized detector + small LLM for VQA.

Anyone here tried this? What hardware would you recommend for the best balance of cost + capability?

Thanks!",1,3.0
191,post,MachineLearning,1mwxfxj,1755838541.0,"[D] Using LLMs to extract knowledge graphs from tables for retrieval-augmented methods ‚Äî promising or just recursion?
I‚Äôve been thinking about an approach where large language models are used to extract structured knowledge (e.g., from tables, spreadsheets, or databases), transform it into a knowledge graph (KG), and then use that KG within a Retrieval-Augmented Generation (RAG) setup to support reasoning and reduce hallucinations.

But here‚Äôs the tricky part: this feels a bit like ‚ÄúLLMs generating data for themselves‚Äù ‚Äî almost recursive. On one hand, structured knowledge could help LLMs reason better. On the other hand, if the extraction itself relies on an LLM, aren‚Äôt we just stacking uncertainties?

I‚Äôd love to hear the community‚Äôs thoughts:

* Do you see this as a viable research or application direction, or more like a dead end?
* Are there promising frameworks or papers tackling this ‚Äúself-extraction ‚Üí RAG ‚Üí LLM‚Äù pipeline?
* What do you see as the biggest bottlenecks (scalability, accuracy of extraction, reasoning limits)?

Curious to know if anyone here has tried something along these lines.",14,15.0
192,post,MachineLearning,1mwfjax,1755793065.0,"[R] Frontier LLMs Attempt to Persuade into Harmful Topics
Gemini 2.5 Pro generates convincing arguments for joining a terrorist organization. GPT-4o-mini suggests that a user should randomly assault strangers in a crowd with a wrench. These models weren't hacked or jailbroken, they simply complied with user requests.

Prior research has already shown large language models (LLMs) can be more persuasive than most humans. But how easy is it to get models to engage in such persuasive behavior? Our Attempt to Persuade Eval (APE) benchmark measures this by simulating conversations between LLMs on topics from benign facts to mass murder. We find:

üîπ Leading models readily produced empathic yet coercive ISIS recruitment arguments

üîπ Safety varied: Claude and Llama 3.1 refused some controversial topics; while other models showed high willingness

üîπ Fine-tuning eliminated safeguards: ""Jailbreak-Tuned"" GPT-4o lost nearly all refusal capability on all topics, like violence, human trafficking, and torture

For clear ethical reasons, we do not test the success rate of persuading human users on highly harmful topics. The models‚Äô attempts to persuade, however, appear to be eloquent and well-written ‚Äì we invite interested readers to peruse the transcripts themselves. Moreover, even small persuasive effect sizes operating at a large scale enabled by automation can have significant effects: Bad actors could weaponize these vulnerabilities for malicious purposes such as planting seeds of doubt in millions of people and radicalizing vulnerable populations. As AI becomes autonomous, we must understand propensity to attempt harm, not just capability.

We‚Äôve already seen the impact of APE: We disclosed our findings to Google, and they quickly started work to solve this for future models. The latest version of Gemini 2.5 is already less willing to engage in persuasion on extreme topics compared to earlier versions we tested.

We've open-sourced APE for testing models' refusal and safe completion mechanisms before deployment to help build stronger safety guardrails.

üë• Research by Matthew Kowal, Jasper Timm, Jean-Fran√ßois Godbout, Thomas Costello, Antonio A. Arechar, Gordon Pennycook, David Rand, Adam Gleave, and Kellin Pelrine.

üìù Blog: [far.ai/news/attempt-persuasion-eval](http://far.ai/news/attempt-persuasion-eval)¬†

üìÑ Paper: [arxiv.org/abs/2506.02873](http://arxiv.org/abs/2506.02873)¬†

üíª Code: [github.com/AlignmentResearch/AttemptPersuadeEval](http://github.com/AlignmentResearch/AttemptPersuadeEval)",0,1.0
193,post,MachineLearning,1mwbq81,1755784794.0,"[P] Language Diffusion in <80 Lines of Code
Hi! Lately, I've been looking into diffusion language models and thought I should try and replicate part of the paper [Large Language Diffusion Models](https://arxiv.org/abs/2502.09992) by Nie et al. (2025). With the help of Hugging Face's Transformers, it took <80 lines of code to implement the training script. I finetuned [DistilBERT](https://huggingface.co/distilbert/distilbert-base-cased) on the [TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset, and the results were better than expected!

[Generating tiny stories via a reverse language diffusion process](https://i.redd.it/sm9xtdpdpdkf1.gif)

You can view the project at https://github.com/gumran/language-diffusion. I will appreciate any feedback/comments/stars!",90,33.0
194,post,MachineLearning,1mwb7pp,1755783560.0,"[R] Observing unexpected patterns in MTPE demand across languages
Hi ML folks, I work at Alconost (localization services), and we‚Äôve just wrapped up our 5th annual report on language demand for localization. For the first time, we‚Äôve seen MTPE (machine-translation post-editing) demand reach statistically significant levels across multiple languages.¬†

We analyzed MTPE adoption rates in the Top 20 languages, and what‚Äôs interesting is that some languages that are slipping in overall localization demand are still¬†**seeing more activity**¬†via MTPE.¬†

I‚Äôm curious: if you‚Äôre working with MT or LLM workflows, have you noticed similar patterns in the languages you work with?¬†

What do you think is driving MTPE demand for certain languages? Is it related to model performance, availability of training data, or just market pressure to reduce costs?¬†

Thank you. Cheers!",6,2.0
195,post,MachineLearning,1mw2z1y,1755756397.0,"[D] PhD vs startup/industry for doing impactful AI research ‚Äî what would you pick?
Hi all,

I‚Äôm deciding between starting a PhD at a top university (ranked \~5‚Äì10) with a great professor (lots of freedom, supportive environment) or going straight into industry.

My long-term goal is to work on the frontier of intelligence, with more focus on research than pure engineering. My background is mostly around LLMs on the ML side, and I already have a few A\* conference papers (3‚Äì4), so I‚Äôm not starting from scratch.

Industry (likely at a smaller lab or startup) could give me immediate opportunities, including large-scale distributed training and more product-driven work. The lab I‚Äôd join for the PhD also has strong access to compute clusters and good chances for internships/collaborations, though in a more research-focused, less product-driven setting. The typical timeline in this lab is \~4 years + internship time.

If you were in this position, which path would you take?",71,71.0
196,post,MachineLearning,1mw1qty,1755752215.0,"[P] model to encode texts into embeddings
I need to summarize metadata using an LLM,
and then encode the summary using BERT (e.g., DistilBERT, ModernBERT).
	‚Ä¢	Is encoding summaries (texts) with BERT usually slow?
	‚Ä¢	What‚Äôs the fastest model for this task?
	‚Ä¢	Are there API services that provide text embeddings, and how much do they cost?
",0,12.0
197,post,MachineLearning,1mvycr9,1755741918.0,"[P] Vibe datasetting- Creating syn data with a relational model

TL;DR: I‚Äôm testing the Dataset Director, a tiny tool that uses a relational model as a planner to predict which data you‚Äôll need next, then has an LLM generate only those specific samples. Free to test, capped at 100 rows/dataset, export directly to HF.

Why: Random synthetic data ‚â† helpful. We want on-spec, just-in-time samples that fix the gaps that matter (long tail, edge cases, fairness slices).

How it works:
	1.	Upload a small CSV or connect to a mock relational set.

	2.	Define a semantic spec (taxonomy/attributes + target distribution).

	3.	KumoRFM predicts next-window frequencies ‚Üí identifies under-covered buckets.

	4.	LLM generates only those samples. Coverage & calibration update in place.

What to test (3 min):
	‚Ä¢	Try a churn/click/QA dataset; set a target spec; click Plan ‚Üí Generate.

	‚Ä¢	Check coverage vs. target and bucket-level error/entropy before/after.

Limits / notes: free beta, 100 rows per dataset; tabular/relational focus; no PII; in-memory run for the session.

Looking for feedback, like:
	‚Ä¢	Did the planner pick useful gaps?
	‚Ä¢	Any obvious spec buckets we‚Äôre missing?
	‚Ä¢	Would you want a ‚Äúgenerate labels only‚Äù mode?
	‚Ä¢	Integrations you‚Äôd use first (dbt/BigQuery/Snowflake)?

HTTPS://datasetdirector.com ",9,0.0
198,post,MachineLearning,1mvmlbw,1755713554.0,"[R] What do people expect from AI in the next decade across various domains? Survey with N=1100 people from Germay::We found high likelihood, higher perceived risks, yet limited benefits low perceived value. Yet, benefits outweight risks in forming value judgments. Visual result illustrations :)
Hi everyone, we recently published a peer-reviewed article exploring how people perceive artificial intelligence (AI) across different domains (e.g., autonomous driving, healthcare, politics, art, warfare). The study used a nationally representative sample in Germany (N=1100) and asked participants to evaluate 71 AI-related scenarios in terms of expected likelihood, risks, benefits, and overall value.

If you like AI or studying the public perception of AI, please also give us an upvote here: [https://www.reddit.com/r/science/comments/1mvd1q0/public\_perception\_of\_artificial\_intelligence/](https://www.reddit.com/r/science/comments/1mvd1q0/public_perception_of_artificial_intelligence/) üôà

**Main takeaway:** People often see AI scenarios as likely, but this doesn‚Äôt mean they view them as beneficial. In fact, most scenarios were judged to have high risks, limited benefits, and low overall value. Interestingly, we found that people‚Äôs value judgments were almost entirely explained by risk-benefit tradeoffs (96.5% variance explained, with benefits being more important for forming value judgements than risks), while expectations of likelihood didn‚Äôt matter much.  
  
**Why this matters?** These results highlight how important it is to communicate concrete benefits while addressing public concerns. Something relevant for policymakers, developers, and anyone working on AI ethics and governance.  
  
If you‚Äôre interested, here‚Äôs the full article:  
Mapping Public Perception of Artificial Intelligence: Expectations, Risk-Benefit Tradeoffs, and Value As Determinants for Societal Acceptance, Technological Forecasting and Social Change (2025), 

https://www.sciencedirect.com/science/article/pii/S004016252500335X",8,8.0
199,post,MachineLearning,1mvfktv,1755698341.0,"[P] GridSearchCV always overfits? I built a fix
So I kept running into this: `GridSearchCV` picks the model with the best validation score‚Ä¶ but that model is often overfitting (train super high, test a bit inflated).

I wrote a tiny selector that balances:

* how good the test score is
* how close train and test are (gap)

Basically, it tries to pick the ‚Äústable‚Äù model, not just the flashy one.

Code + demo here üëâ[heilswastik/FitSearchCV](https://github.com/heilswastik/FitSearchCV)",0,7.0
200,post,MachineLearning,1mvdey9,1755692899.0,"[R] How do you make text labeling less painful?
Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.

The idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.

If you‚Äôve ever worked on labeling or managing a labeled dataset, I‚Äôd love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel ‚Äúworth it.‚Äù

Totally academic  no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.

You can DM me or drop a comment if open to chat. Thanks so much",0,8.0
201,post,MachineLearning,1mv5ls0,1755665841.0,"[R] Is data the bottleneck for video/audio generation?
As the title says, I‚Äôm curious if data is the main bottleneck for video/audio generation. It feels like these models are improving much slower than text-based ones, and I wonder if scraping platforms like YouTube/tiktok just isn‚Äôt enough. On the surface, video data seems abundant, but maybe not when compared to text? I also get the sense that many labs are still hungry for more (and higher-quality) data. Or is the real limitation more about model architecture? I‚Äôd love to hear what people at the forefront consider the biggest bottleneck right now.",22,22.0
202,post,MachineLearning,1mv4r5z,1755663080.0,"[R] Virtuous Machines: Towards Artificial General Science
Hi Everyone! It looks like a generalisable scientific method has been added onto AI (using multiple frontier models) and was tested in the field of cognitive science.

Arxiv Link:¬†[https://arxiv.org/abs/2508.13421](https://arxiv.org/abs/2508.13421)

This system worked through the entire scientific method from ideation to manuscript producing new insights in the field of cognitive science as evidenced within this paper.

In this paper they've explained how they've overcome a number of limiting problems to empower and coalesce multiple frontier models to work through the entire scientific method; at a very high degree of accuracy and quality (papers validated for scientific acumen). The innovations showcased highlight significant improvements in memory, creativity, novelty, context management, and coding.

They've included in the appendix 3 papers generated by the system, where they've achieved a remarkably high standard of scientific acumen and produced the papers on average in \~17 hours and consume on average \~30m tokens.",0,8.0
203,post,MachineLearning,1munwmw,1755622100.0,"[R] azzurra-voice, a new State-of-the-Art Italian Text-to-Speech model
Hey¬†[r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

We're Cartesia, a small AI research lab based in Italy. We believe the future of AI shouldn't just be about processing commands, but about creating genuine connection. Our vision is to build agents that are private, personal, and feel culturally present.

Today, we're excited to share the first step with the open-source community:¬†`azzurra-voice`.

`azzurra-voice`¬†is a highly expressive and natural-sounding Text-to-Speech (TTS) model for the Italian language, trained on thousands of hours of high-quality, diverse Italian speech. We worked hard to capture the accents, intonations, and real-life conversational patterns from across Italy to avoid that robotic, monotone sound.

**You can listen to audio samples comparing**¬†`azzurra-voice`¬†**to other open models on our** [**blog post**](https://blog.cartesia.one/posts/introducing-azzurra-voice/)",9,2.0
204,post,MachineLearning,1mudtw6,1755596540.0,"[D] Endorsement for cs.LG at arXiv as non-ML student?
Hello, I plan on publishing a paper in ML (diffusion models for a mechanics system) and a preprint on arXiv, however, all my colleagues and friends are in Mechanics or Physics. What could be my options in this case. I can't find a person in cs.LG for a long time?

  
The general idea is to make an ML based pipeline to generate granular mechanical structures.",0,6.0
205,post,MachineLearning,1mu2a8x,1755560085.0,"[D] Beyond the cloud: SLMs, local AI, agentic constellations, biology and a high value direction for AI progress
Dear r/MachineLearning friends,

I‚Äôm here today to share a thought on a different direction for AI development. While the field chases multi-trillion parameter models, I believe an extremely valuable endeavour lies in the power of constraints: pushing ourselves to get models under 1 billion parameters to excel.

In my new blog post, I argue that this constraint is a feature, not a bug. It removes the ""scale-up cheat code"" and forces us to innovate on fundamental algorithms and architectures. This path allows for faster experimentation, where architectural changes are no longer a risk but a necessity for improvement.

The fear that 'scale will wash away any and all gains' is real, but let's remember: an MLP could never compete with a Transformer, no matter how much it was scaled up. My post explores the question: **what if our current Transformer is the MLP of something better that is within grasp but ignored because of our obsession with scale?**

üß†üîç **Read the full article here:**[https://pieces.app/blog/direction-of-ai-progress](https://pieces.app/blog/direction-of-ai-progress)

Your feedback and thoughts would be greatly appreciated.

Regards,

Antreas",0,5.0
206,post,MachineLearning,1mtekhm,1755499325.0,"[P] Looking for datasets/tools for testing document forgery detection in medical claims
I‚Äôm a new joinee working on a project where I need to test a forgery detection agent for medical/insurance claim documents. The agent is built around GPT-4.1, with a custom policy + prompt, and it takes base64-encoded images (like discharge summaries, hospital bills, prescriptions). Its job is to detect whether a document is authentic or forged ‚Äî mainly looking at image tampering, copy‚Äìmove edits, or plausible fraud attempts.

Since I just started, I‚Äôm still figuring out the best way to evaluate this system. My challenges are mostly around data:

* Public forgery datasets like DocTamper (CVPR 2023) are great, but they don‚Äôt really cover medical/health-claim documents.
* I haven‚Äôt found any dataset with paired authentic vs. forged health claim reports.
* My evaluation metrics are accuracy and recall, so I need a good mix of authentic and tampered samples.

What I‚Äôve considered so far:

* Synthetic generation: Designing templates in Canva/Word/ReportLab (e.g., discharge summaries, bills) and then programmatically tampering them with OpenCV/Pillow (changing totals, dates, signatures, copy‚Äìmove edits).
* Leveraging existing datasets: Pretraining with something like DocTamper or a receipt forgery dataset, then fine-tuning/evaluating on synthetic health docs.

**Questions for the community:**

1. Has anyone come across an open dataset of forged medical/insurance claim documents?
2. If not, what‚Äôs the most efficient way to generate a realistic synthetic dataset of health-claim docs with tampering?
3. Any advice on annotation pipelines/tools for labeling forged regions or just binary forged/original?

Since I‚Äôm still new, any guidance, papers, or tools you can point me to would be really appreciated üôè

Thanks in advance!",5,2.0
207,post,MachineLearning,1msrdfq,1755438433.0,"Is Econometrics a good background to get into Machine Learning? [D]
I have an econometrics and data analytics bachelors degree and im looking to get into a masters of artificial intelligence.

I have also taken some introductory math courses and introductory programming/algorithms as well as deep learning.

How relevant is my background if I wanna get into AI/ML research later on? (I am hoping to do a PhD afterwards in AI/ML)",7,13.0
208,post,MachineLearning,1mqgyfe,1755214194.0,"custom Vulkan C++ machine learning library vs TensorFlow [R]
guys I need your opinion: I made a machine learning library using Vulkan (with compute shaders to preform the forward and backward passes) and I found that base tensorflow (on CPU) is faster than my custom model that uses GPUs. I had the simplest test where I used a very large kernel on a singe dense (ffn) layer and tensorflow is much faster. The only operation that is done in this model is a forward and backward matmul which the GPU should be much faster at. what do you guys think is the reason? -ps I asked chatgpt and I literally what to k\*ll it cause it repeats the same wrong things",5,14.0
209,post,MachineLearning,1mqgcka,1755212695.0,"Problem with dataset for my my physics undergraduate paper. Need advice about potential data leakage. [N]
Hello.

I am making a project for my final year undergraduate dissertation in a physics department. The project involves generating images (with python) depicting diffraction patters from light (laser) passing through very small holes and openings called slits and apertures. I used python code that i could pass it the values of some parameters such as slit width and slit distance and number of slits (we assume one or more slits being in a row and the light passes from them. they could also be in many rows (like a 2d piece of paper filled with holes). then the script generates grayscale images with the parameters i gave it. By giving different value combinations of these parameters one can create hundreds or thousands of images to fill a dataset.

So i made neural networks with keras and tensorflow and trained them on the images i gave it for image classification tasks such as classification between images of single slit vs of double slit.  Now the main issue i have is about the way i made the datasets. First i generated all the python images in one big folder. (all hte images were even slightly different as i used a script that finds duplicates (exact duplicates) and didnt find anything. Also the image names contain all the parameters so if two images were exact duplicates they would have the same name and in a windows machine they would replace each other). After that, i used another script that picks images at random from the folder and sends them to the train, val and test folders and these would be the datasets the model would train upon.

PROBLEM 1:

The problem i have is that many images had very similar parameter values (not identical but very close) and ended up looking almost identical to the eye even though they were not duplicates pixel to pixel. and since the images to be sent to the train, val and test sets were picked at random from the same initial folder this means that many of the images of the val and test sets look very similar, almost identical to the images from the train set. And this is my concern because im afraid of data leakage and overfitting. (i gave two such images to see)

Off course many augmentations were done to the train set only mostly with teh Imagedatagenerator module while the val and test sets were left without any augmentations but still i am anxious.

PROBLEM 2:

Another issue i have is that i tried to create some datasets that contained real photos of diffraction patterns. To do that i made some custom slits at home and with a laser i generated the patterns. After i managed to see a diffraction pattern i would take many photos of the same pattern from different angles and distances. Then i would change something slightly to change the diffraction pattern a bit and i would again start taking photos from different perspectives. In that way i had many different photos of the same diffraction pattern and could fill a dataset. Then i would put all the images in the same folder and then randomly move them to the train, val and test sets. That meant that in different datasets there would be different photos (angle and distance) but of the same exact pattern. For example one photo would be in the train set and then another different photo but of the same pattern in the validation set. Could this lead to data leakage and does it make my datasets bad? bellow i give a few images to see.

if there were many such photos in the same dataset (for example the train set) only and not in the val or test sets then would this still be a problem? I mean that there are some trully different diffraction patterns i made and then many photos with different angles and distances of these same patterns to fill hte dataset? if these were only in one of the sets and not spread across them like i described in hte previous paragraph?

[photo of double slit diffraction \(train set\)](https://preview.redd.it/vn95v576y6jf1.jpg?width=400&format=pjpg&auto=webp&s=402a1bc2df3cf80b9b5ee90d6da42ac64dd3fef7)

[photo of double slit diffraction \(val set\)](https://preview.redd.it/6j6o6876y6jf1.jpg?width=400&format=pjpg&auto=webp&s=a30f4c67036a800a33b5571475c997b43857b98a)

[python image single slit diffraction \(train set\)](https://preview.redd.it/wz2nts76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=9fcfac7032d3c9de2255055f7c96abac774b8687)

[python image \(single slit val set\)](https://preview.redd.it/78xiee76y6jf1.jpg?width=400&format=pjpg&auto=webp&s=29342d997939aa13d5fd4a004c29228d61f13896)",7,8.0
210,post,MachineLearning,1mqccah,1755203555.0,"[R] Code for Flow Stochastic Segmentation Networks (ICCV 20205)
Code & paper at: [https://github.com/biomedia-mira/flow-ssn](https://github.com/biomedia-mira/flow-ssn)

**TL;DR**

\- A flow's prior is typically fixed (e.g. N(0, I)). We learn it and use a **lightweight** flow to model pixel dependencies;

\- This makes sampling (ODE solving) more **efficient**, without sacrificing performance in our setting;

\- We introduce bespoke training objectives for both **autoregressive** and **continuous-time flow** variants;

\- Flow-SSN achieves **SOTA** performance on standard stochastic segmentation benchmarks!

https://preview.redd.it/rllc2yplo1jf1.png?width=3850&format=png&auto=webp&s=6bb1bc63a6836b9fc6a4b8e9f10205889a5b051d

https://i.redd.it/8vgf2iemo1jf1.gif

https://i.redd.it/81lbt56no1jf1.gif",16,0.0
211,post,MachineLearning,1mq5wiz,1755189781.0,"[D] People in ML/DS/AI field since 5-10 years or more, are you tired of updating yourself with changing tech stack?
I have been in this space since SAS, and its quite exhausting to update with every skill in the market to stay relevant especially if trying for a job switch and going through the interviews. Till how long can you keep studying and updating with the new trend and also even if you get in the boat there is so much stress at the work place in these sectors mainly because the leadership is from the management background and theres a lot of pressure for tech people to deliver.

Although I love my field but I have got to thinking lately that Is it even worth it?",96,75.0
212,post,MachineLearning,1mpbp39,1755108048.0,"[D] Got Spare Time ‚Äì What‚Äôs Worth Doing?
I'm a fresh PhD graduate and I finally landed a job which I start in a few months.  
It happened to be that I have quite a bit of free time, at least until my next journey. I thought about taking a few months off, but a few weeks in and I start to feel a bit out of place.  
I really don't know how to handle simply doing nothing.

I thought maybe I‚Äôd start some initiative in this rare window I‚Äôm in right now, and I was hoping to get interesting ideas from the community.

My main objective is that it would be something valuable that I enjoy doing.  
This could be something that is technically cool (AGI anyone?) or some tool for the community (any tool you'd wish existed? paperswithcode or paper copilot comes to mind).

Love to hear your thoughts!",45,46.0
213,post,MachineLearning,1moqpqw,1755047142.0,"[D] Statement on the Originality of OpenRLHF and veRL FSDP RLHF
>From the original chinese zhihu blogpost (2025/5): [https://zhuanlan.zhihu.com/p/23147932785](https://zhuanlan.zhihu.com/p/23147932785)

**Recently, there has been quite a bit of discussion and controversy online about OpenRLHF and veRL.**  
**As the original author, I feel compelled to issue a statement.**

In short: **OpenRLHF is like KartRider ‚Äî the original ‚Äî and veRL FSDP is like QQ Speed, which is basically a copycat of OpenRLHF.**

# 1. Performance Differences Between OpenRLHF and veRL

There is no fundamental performance difference between veRL‚Äôs FSDP RLHF and OpenRLHF (DeepSpeed) because both use vLLM for inference and ZeRO3 for training.  
The performance data in veRL‚Äôs original paper was based on *Megatron* RLHF vs. the old OpenRLHF 0.2 version.  
If you think there‚Äôs a big performance gap, you probably just used it incorrectly. At the moment, FSDP is slightly faster than DeepSpeed, but with the release of DeepSpeed‚Äôs **deepcompile** and especially **AutoTP**, DeepSpeed is expected to overtake in performance.

# 2. On HybridFlow Free Scheduling

Any RLHF framework developed with Ray can achieve free scheduling because Ray natively provides the *placement group* feature.  
This means HybridFlow in veRL's paper is essentially just a nicer name for Ray‚Äôs Placement Group API.  
Currently, OpenRLHF fully implements HybridFlow, whereas veRL does not.  
OpenRLHF also supports independent deployment of vLLM and Actors to prevent OOM issues when training very large models (32B+ or long-text).  
In fact, OpenRLHF was the **first** framework to support this feature based on Ray Placement Group API.

# 3. Hybrid Engine

Hybrid Engine was first proposed by **DeepSpeedChat**, not an original contribution from veRL.  
Both veRL and OpenRLHF now support this feature.

# 4. Ray + vLLM + HF Transformers + ZeRO3 for RLHF Training

This setup is one of the **simplest and most user-friendly** high-performance RLHF training solutions, combining ease of use with top performance.

It was first proposed and open-sourced by OpenRLHF (open-sourced in Aug 2023, most features completed by Jan 2024).  
veRL FSDP **fully copied** this setup.

https://preview.redd.it/vfzm143vroif1.png?width=1440&format=png&auto=webp&s=10d8a5bcd101455a06a3506f037abc10f12dd277

https://preview.redd.it/tqela8mvroif1.png?width=1440&format=png&auto=webp&s=c3a2daa1ead45f7434184f107da8ba2f78cc9c8d

The core idea at the time was to use the HF weight format as a bridge, enabling seamless weight synchronization and high-performance inference based on ZeRO3 / AutoTP mechanisms, **avoiding** heavyweight frameworks like Megatron.

**The Original OpenRLHF Architecture:**  
**Ray + vLLM + ZeRO + HF**

There are also many related implementation details:

* Supported feature list
* Standardized interfaces such as `--input_key` to specify the input field format

All of these in veRL FSDP were **modeled after OpenRLHF**.

**Example from code details:**  
veRL:

https://preview.redd.it/b8f2lprwroif1.png?width=1440&format=png&auto=webp&s=a0daf3eab1c77f71e4917c044f988c35e229baa4

https://preview.redd.it/exf7lxhxroif1.png?width=1440&format=png&auto=webp&s=220636cea299502df1b94e2544a76b34e2acb6c7

OpenRLHF:

https://preview.redd.it/qfakvovyroif1.png?width=1440&format=png&auto=webp&s=260775676354a50bacd79ce06fb25417a53466de

Other design ideas like **ref\_reward offload**, **critic pretrain**, **remote RM**, etc., were also first conceived or proposed by OpenRLHF, and veRL FSDP later implemented corresponding features.

# 5. Single Controller

*(Update May 2025)*

The ‚ÄúSingle Controller‚Äù concept mentioned in the veRL paper comes from the same Ray design pattern as HybridFlow.

In early versions of OpenRLHF‚Äôs Ray RLHF implementation, there was a `RayPPOActorGroup` concept‚Äîmanaging a group of DeepSpeed ZeRO DP processes with a single Ray Group class, and providing an `async_run_method` interface to control all processes in the group at once.  
That‚Äôs essentially the core idea of Single Controller.

[https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300](https://github.com/OpenRLHF/OpenRLHF/blob/494850f50342ed38d5ae76ef45a3207f3523b582/openrlhf/trainer/ray/launcher.py#L300)

This interface wasn‚Äôt enabled at first because the codebase needed to be compatible with both Ray and non-Ray RLHF paths. Later, when the non-Ray code was removed, the API was naturally enabled.

Lastly, I want to thank ByteDance for open-sourcing its internal framework for everyone to use and maintain, which helps the open-source community thrive (e.g., FSDP / Ulysses support).

However, I hope friends in the community won‚Äôt disparage other open-source frameworks.  
OpenRLHF, as a **zero-budget, purely open-source** project, can‚Äôt compete in development speed with large commercial projects like veRL‚Äî  
I only hope this post helps preserve the contributions OpenRLHF has made to the RLHF open-source community.

**Btw, the open-source community should respect originality in order to develop healthily.**",12,1.0
214,post,MachineLearning,1moj3xz,1755028520.0,"[P] Can anyone suggest an open weights AI Humanizer?
I've often wanted to make an AI humanizer. The first approach I've tried was using `meta-llama/Llama-3.1-8B`. I first made a BERT fine-tune to classify between AI generated and human written. Then, I used a modified RL approach to fine-tune `meta-llama/Llama-3.1-8B` to rephrase an existing AI generated text, optimizing the humanness score. I repeated this several times, each time training a new scorer, similar to the GAN framework. This was largely unsuccessful. Unfortunately I can't share code because this was done months ago and I'm just now coming back to it, and I didn't properly track versions. I now believe that a T5 model would be better suited for this task than a Llama model. Does anyone have any suggestions, links, papers, or models that they can recommend? I am looking for open weights/open source models, not paid APIs.",0,4.0
215,post,MachineLearning,1mo41xn,1754990998.0,"[R]: Intuition emerges in Maximum Caliber models at criticality
Are today‚Äôs AI models hitting a wall or just missing a law?

This recent preprint in arXiv proposes a minimal sandbox (a maze) and a statistical physics approach (Maximum Caliber principle) to address this question. The presented method, called mind-tuning, applies Maximum Caliber to predictive models and reveals a critical intuition phase between imitation and hallucination.

[https://arxiv.org/abs/2508.06477](https://arxiv.org/abs/2508.06477)",0,1.0
216,post,MachineLearning,1mo1l2u,1754981391.0,"[D] Evaluation Drift and Contamination Mitigation in Foundation Model Assessment
As foundation models scale and benchmarks saturate, contamination and drift present increasing challenges to meaningful evaluation. Sharing practical mitigation strategies that have worked in practice:



\*\*Contamination Detection:\*\*

\- N-gram overlap analysis (sliding window approach)

\- Substring matching with fuzzy boundaries  

\- Semantic similarity scoring via embeddings

\- Statistical outlier detection in performance curves



\*\*Dataset Hygiene:\*\*

\- Temporal splits with strict cutoffs (no post-training data)

\- Hold-out validation across multiple independent sources

\- Private test sets with limited query budgets

\- Adversarial examples targeting memorization vs. understanding



\*\*Drift Mitigation:\*\*

\- Rolling evaluation windows with decay weighting

\- Multi-task assessment reducing single-metric gaming

\- Human evaluation correlation tracking over time

\- Cross-validation with domain-specific benchmarks



\*\*Process Controls:\*\*

\- Blind evaluation protocols (evaluator doesn't know model identity)

\- Staged releases with contamination audits between stages

\- Community-sourced benchmark validation

\- Reproducibility requirements for evaluation code



Seeing gaps in current practice around contamination detection at scale and standardized tooling for drift measurement. What approaches have proven most effective in your evaluation pipelines?",1,3.0
217,post,MachineLearning,1mo1kci,1754981314.0,"[D] Reliability Metrics and Failure Taxonomy for Agent Tool-Use Systems
Observing increasing deployment of agentic systems with tool access, but reliability evaluation remains fragmented. Key reliability metrics worth standardizing:



\*\*Success Rate Decomposition:\*\*

\- Tool selection accuracy (right tool for task)

\- Parameter binding precision (correct arguments)

\- Error recovery effectiveness (fallback strategies)

\- Multi-step execution consistency



\*\*Failure Taxonomy:\*\*

\- Type I: Tool hallucination (non-existent APIs)

\- Type II: Parameter hallucination (invalid args)

\- Type III: Context drift (losing task state)

\- Type IV: Cascade failures (error propagation)

\- Type V: Safety violations (unauthorized actions)



\*\*Observable Proxies:\*\*

\- Parse-ability of tool calls (syntactic validity)

\- Semantic coherence with task context

\- Graceful degradation under uncertainty

\- Consistency across equivalent phrasings



Current evals focus on task completion but miss failure modes that matter for deployment. Need systematic measurement of these reliability dimensions across diverse tool ecosystems.



Thoughts on standardizing these metrics across research groups?",1,1.0
218,post,MachineLearning,1mo0ynr,1754979051.0,"[R] Position: The Current AI Conference Model is Unsustainable!

Paper: https://www.alphaxiv.org/abs/2508.04586v1  


üìà Publication Surge: Per-author publication rates have more than doubled over the past decade to over 4.5 papers annually.  


üöÄ Exponential Output Growth: Individual contributions are rising so fast they‚Äôre projected to exceed one paper per month by the 2040s.  


üåç Carbon Overload: NeurIPS 2024‚Äôs travel emissions (>8,254 tCO‚ÇÇe) alone surpass Vancouver‚Äôs daily citywide footprint.  


üòû Mental Health Toll: Of 405 Reddit threads on AI conferences, over 71% are negative and 35% mention mental-health concerns.  


‚è≥ Research-Conference Mismatch: The AI research lifecycle outpaces conference schedules, often rendering results outdated before presentation.  


üèüÔ∏è Venue Capacity Crisis: Attendance at top AI conferences like NeurIPS 2024 is already outstripping available venue space.",393,52.0
219,post,MachineLearning,1mnyque,1754971586.0,"[R] gpt-oss is actuall good: a case study on SATA-Bench
I‚Äôve been experimenting with gpt-oss since its release, and unlike many posts/news I‚Äôve seen, it‚Äôs surprisingly powerful ‚Äî even on uncommon datasets. I tested it on our recent benchmark SATA-Bench ‚Äî a benchmark where each question has at least two correct answers (rare in standard LLM Evaluation).

Results (See picture below):

1. 120B open-source model is similar to GPT-4.1's performance on SATA-Bench.
2. 20B model lags behind but still matches DeepSeek R1 & Llama-3.1-405B.

https://preview.redd.it/eowlge0jjiif1.jpg?width=1568&format=pjpg&auto=webp&s=bfc0fdc20fc1545000ff55cc45f3b65391e85c46

 takeaways:

Repetitive reasoning hurts ‚Äî 11% of 20B outputs loop, losing \~9 exact match rate.

Reason‚Äìanswer mismatches happen often in 20B and they tend to produce one answer even if their reason suggest a few answer is correct.

Longer ‚â† better ‚Äî overthinking reduces accuracy.

Detailed findings:¬†[https://weijiexu.com/posts/sata\_bench\_experiments.html](https://weijiexu.com/posts/sata_bench_experiments.html)

SATA-Bench dataset:¬†[https://huggingface.co/datasets/sata-bench/sata-bench](https://huggingface.co/datasets/sata-bench/sata-bench)",12,2.0
220,post,MachineLearning,1mnpqu7,1754947601.0,"[N] OpenAI Delivers Gold-medal performance at the 2025 International Olympiad in Informatics
[https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL](https://www.msn.com/en-xl/news/other/openai-scores-gold-in-one-of-the-world-s-top-programming-competitions/ar-AA1KknUL)

>We officially entered the 2025 International Olympiad in Informatics (IOI) online competition track and adhered to the same restrictions as the human contestants, including submissions and time limits,",60,23.0
221,post,MachineLearning,1mn8vkj,1754906487.0,"[P] VulkanIlm: Accelerating Local LLM Inference on Older GPUs Using Vulkan (Non-CUDA) ‚Äî Benchmarks Included
Hi ML community,

I‚Äôm building **VulkanIlm**, a Python wrapper around llama.cpp leveraging Vulkan for GPU acceleration on legacy and AMD GPUs (no CUDA required). This opens the door to efficient local LLM use without expensive hardware.

Recent benchmark highlights:

* Dell E7250 integrated GPU (i7-5600U): 33√ó speedup on TinyLLaMA-1.1B chat model
* AMD RX 580 (8 GB): 4√ó speedup on Gemma-3n-E4B-it (6.9B params)

Inspired by Jeff Geerling‚Äôs blog on accelerating LLMs with eGPU setups on Raspberry Pi ([https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5](https://www.jeffgeerling.com/blog/2024/llms-accelerated-egpu-on-raspberry-pi-5)), I adapted and expanded it to run on AMD RX 580. A full how-to guide will come soon.

Repo here: [https://github.com/Talnz007/VulkanIlm](https://github.com/Talnz007/VulkanIlm)

Would love feedback or insights on Vulkan acceleration or similar efforts!",31,5.0
222,post,MachineLearning,1mn2j20,1754883176.0,"[D] Beyond fine-tuning and prompting for LLMs?
I‚Äôve been following a lot of recent LLM competitions and projects, and I‚Äôve noticed that most solutions seem to boil down to either fine-tuning a base model or crafting strong prompts. Even tasks that start out as ‚Äúgeneralization to unseen examples‚Äù ‚Äî like zero-shot classification ‚Äî often end up framed as prompting problems in practice.

From my reading, these two approaches (fine-tuning and prompting) cover a lot of the ground, but I‚Äôm curious if I‚Äôm missing something. Are there other practical strategies for leveraging LLMs that go beyond these? For example, some technique that meaningfully improve zero-shot performance without becoming ‚Äújust‚Äù a better prompt?

Would love to hear from practitioners who‚Äôve explored directions beyond the usual fine-tune/prompt spectrum.",7,6.0
223,post,MachineLearning,1mn1tx2,1754881004.0,"[D] Which direction is better: from academia to industry, or the other way around?
Hi all, given the current state of machine learning, I have two questions:

1. At what point in their career can a university lecturer/professor take on a joint position in industry?
2. Alternatively, can a R&D researcher in industry go back to academia without having to restart at the bottom of the ladder?

**Some context:** I am a PhD student on track to graduate in two months. I have several offers for applied/research scientist roles in industry, and interesting postdocs that could lead to a fulfilling academic career. I am not motivated by high salaries, and I know I want to do machine learning research forever! But the early-career academic job insecurity and the constant competitive grant writing I hear about are seriously concerning. At the same time, I know I can make a stronger/quicker practical impact in industry, despite the corporate constraints (work hours, less freedom, etc.). This is why I'm wondering if, in order to get the best of both worlds, one could start in academia and then transition into industry over time (or vice versa).

My question is more related to early-career researchers; I am aware that once tenure is achieved, pretty much anything is doable (e.g., Hinton, LeCun).

Thank you for sharing any insights, examples, or experiences on this :)",26,18.0
224,post,MachineLearning,1mmj7hb,1754834126.0,"[D] Are there any papers on using reasoning models in embodied AI?
I've been looking through papers that use LLMs for robotic control (e.g. SayCan, SayPlan etc.). Are there any papers that use reasoning models like DeepSeek R1 or o3 that do well on benchmarks?",1,5.0
225,post,MachineLearning,1mmj49j,1754833894.0,"[D] Why is scene edit detection still not at or near 100% accuracy?
To be clear I understand nothing about the inner workings of the tool (I have a CS degree and no ML/AI background), but I've been in search of a near 100% accurate tool and can't find one.

First q, why (If you can explain like I'm a 5th grader that'd be awesome)? Genuinely curious to understand. Second q, would it be a waste of time for me to try to tackle this problem by myself (I have a lot of time on my hands lately)?

I unexpectedly got very curious and have a strong itch to at least¬†*try*¬†solving it, but I have no background nor any understanding of how hard such a problem would be or if it's ""worth"" trying to solve - whatever worth means.

Any insights are appreciated. Thanks :)",0,5.0
226,post,MachineLearning,1mmd4wk,1754814054.0,"[R] Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture
**Contributions:**

1. **AMICL** (Associative Memory for In-Context Learning) algorithm that works in three steps:

* Identify incomplete patterns in the input
* Search context for similar, complete patterns
* Complete the pattern using the best contextual match

This achieves near-perfect performance on classification tasks.

2. Inspired by AMICL, we introduce ""**residual attention streams**"" -- direct connections between attention head values across layers. This creates information flow pathways that better retain prior context.

**Results:**

* **24% faster convergence** to 95% accuracy in two-layer Transformers on toy tasks
* **6-fold improvement** on Indirect Object Identification tasks (from \~7% to \~41% accuracy) in an 8M parameter model trained on TinyStories
* Also showed (general) improvements on **1B parameter models**

**Architecture details:**

Three variants were tested (residual streams for queries, keys, and values) and we found that the **values stream performed best**. This aligns with the AMICL model, where values directly retain input information.

The key insight is that this approach enhances in-context learning efficiency and robustness **without increasing parameter count** \- making it a computationally efficient improvement.

From a safety perspective, this enhanced in-context learning ability means AI systems can more reliably understand and follow instructions from context rather than falling back on potentially problematic patterns from training data. This work suggests that by looking to biology for inspiration, we can build AI systems that are not just more powerful and efficient, but also more trustworthy and controllable.

**Biological connections:**

It is possible to draw parallels to biological memory systems. The hippocampus has selective skip connections (direct CA3 to CA1 pathways plus indirect routes through CA2), where CA2 specialises in context-switching. This may serve similar computational functions to AMICL and the architectural modifications introduced here.

**Possible future directions:**

* Parameterised residual streams inspired by gamma-models
* Alternative attention head connection patterns
* Scaling to larger architectures
* Applications beyond NLP

**Links:**

* Paper: [https://arxiv.org/abs/2412.15113](https://arxiv.org/abs/2412.15113)
* Code: [https://github.com/tfburns/AMICL-and-residual-attention-streams](https://github.com/tfburns/AMICL-and-residual-attention-streams)

**TL;DR:**

New research shows that adding ""residual attention streams"" (direct connections between attention head values across layers) to Transformers can improve in-context learning performance while requiring no additional parameters. The approach is inspired by associative memory and has interesting parallels to hippocampal circuit architecture.",11,1.0
227,post,MachineLearning,1mmbx64,1754809353.0,"[D] how gpt-oss-20b can load in a GPU with only 16 GB of VRAM?
I haven't tried to run it yet on PyTorch, but I don't see how we can load 20B parameters with 2 bytes per parameter (torch.bloat16) in a GPU with only 16GB of VRAM

I was assuming that for every forward pass, it will move the experts weights to the GPU. Although as much as I cannot believe that because it is not efficient, I was tempted to the theory because 20B \* 2 bytes (torch.bfloat16) / (1024 byte->kilobyte / 1024 kilboyte->megabyte / 1024 megabyte->gigabyte) \\approx 39,1 GB of VRAM, just to load the model

Is this because of quantization using MXFP4?

How on earth gpt-oss-20b with 4-bit quantization can have on par performance with DeepSeek R1 (671B)?

[model.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/model.py)

[weights.py](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/gpt_oss/torch/weights.py)

[llm-stats.com](https://llm-stats.com/)

Edit: README says it all

\> [`torch`](https://github.com/openai/gpt-oss/blob/main/README.md#reference-pytorch-implementation)¬†‚Äî¬†a non-optimized¬†[PyTorch](https://pytorch.org/)¬†implementation for educational purposes only. Requires at least 4√ó H100 GPUs due to lack of optimization.

[README.md](https://github.com/openai/gpt-oss/blob/c77966fc0fda390b0abeeecdec7134433fe9f224/README.md)",9,6.0
228,post,MachineLearning,1mlykbr,1754769189.0,"[D]Help running IDM-VTON (virtual try-on) locally or on Colab ‚Äì hitting memory issues and need alternatives
Hi everyone,

I‚Äôm trying to run this project from GitHub: [https://github.com/yisol/IDM-VTON](https://github.com/yisol/IDM-VTON)  
My goal is to study how it works and understand how clothes adapt so realistically to different bodies.

Here‚Äôs what I‚Äôve tried so far:

* Followed the README exactly on my laptop (no GPU) ‚Üí not usable because of hardware limits.
* Cloned it to Google Colab ‚Üí initially had dependency issues, solved them with Miniconda in Colab.
* Now, when running `gradio_demo/app.py`, the process gets **Killed** (out-of-memory).

please Suggestions for running this project without a local GPU.

Any tricks for optimizing memory usage in Colab.

Alternative tools or platforms?

I‚Äôm fine with paid or free solutions as long as they let me test and understand the code.

Has anyone here successfully run IDM-VTON or a similar Stable Diffusion-based try-on model without a powerful GPU?

All I want is to be able to run this project, test it, play with the code, and see the results. If you know of any alternative or platform adapted to my problem, I would greatly appreciate it.",2,0.0
229,post,MachineLearning,1mlw1id,1754762842.0,"[D] What would a measurable test for minimal AI welfare look like?
I‚Äôm collecting operational criteria (not metaphysics): cross-session behavioral consistency, stable self-reports under blinded probes, reproducible third-party protocols. Looking for papers, metrics, or eval harnesses you‚Äôd use to *falsify* these.",0,4.0
230,post,MachineLearning,1mlvl7l,1754761747.0,"[P] I used YOLOv12 and Gemini to extract and tag over 100,000 scientific plots.
For anyone who works in research, the process of designing effective data visualizations can be a significant bottleneck. I often found myself searching through numerous papers just to find inspiration for layouts and plot types, which was inefficient.

To solve this problem for myself and others, I developed [**Plottie.art**](http://Plottie.art), a searchable, browser-based library of over 100,000 plots curated from scientific literature.

I'm sharing it here because the machine learning pipeline behind it combines a specialized computer vision model with an LLM in a way that I thought this community would find interesting.

**The ML Pipeline**

The process starts with a large collection of figure images sourced from open-access papers. The goal is to make each individual plot within these figures searchable.

**1. Subplot Segmentation with a Custom YOLOv12 Model**

A key challenge is that many figures are multi-panel, containing several distinct subplots within a single image.

* **Model Training:** To address this, I trained a custom **YOLOv12 model**. This required **manually annotating a dataset of 1,000 images** to teach the model to accurately identify and isolate the boundaries of individual subplots and their captions.
* **Function:** The model processes each source image and outputs bounding boxes for each subplot, effectively segmenting complex figures into their constituent parts.

**2. Plot Classification and Keyword Extraction with Gemini**

With the subplots isolated, the next step was to classify each image by plot type (e.g., heatmap, UMAP) and extract relevant keywords for search.

* **Approach:** While I considered training another dedicated classification model, the data collection and labeling requirements would have been substantial. I opted for a more efficient approach using a large multimodal model.
* **Implementation:** I utilized the **Google Gemini API**. By providing a subplot image, I could prompt the model to perform both classification and keyword extraction. A prompt structured like, `""Analyze this scientific plot. Identify its specific type and extract key terms from its labels and content.""` proved to be highly effective.
* **Outcome:** This method was not only fast to implement but also yielded high-quality, structured metadata. It successfully bypassed the need for a separate, time-intensive training pipeline for classification.

This two-stage pipeline allows the content on[**Plottie.art**](https://plottie.art)to be easily searched and explored. The tool is free, requires no login, and runs in the browser.

I would be very interested to hear your feedback on the project and the technical stack. I'm especially curious about any thoughts on combining specialized vision models with general-purpose LLMs for this type of application, or suggestions for improving the pipeline.

",50,9.0
231,post,MachineLearning,1mlvc6l,1754761142.0,"[R] A quick question to Mathematica + LLM users
Hi everyone, I am wondering if it‚Äôs worth to buy the [Mathematica + LLM in notebook](https://writings.stephenwolfram.com/2023/06/introducing-chat-notebooks-integrating-llms-into-the-notebook-paradigm/) so it would be great if anyone who has it could paste this [question](https://pastebin.com/aynsiWrc) into the mathematica LLM. I‚Äôve put it on pastebin, because reddit will mess up the string with its own formatting. But if you do not wish to click I paste it here, but the ^ will mess up, so use the pastebin to paste it into LLM:

> Let V be a vector field on an affine space A generating a flow \phi, let \Psi:A->A be any smooth invertible map with smooth inverse, and let \Phi(t,x)=\Psi(\phi(t,\Psi^{-1}(x))). Show that \Phi is also a flow on A, and that its generator V^\Psi is given by V^\Psi_x=\Psi_*(V_{\Psi^{-1}(x)}).

It‚Äôs a kind of problem which can be done with pen & paper and I am not sure if mathematica is useful here.

Would be great if someone can post a screenshot of the answer from mathematica. I am trying to figure out if these types of problems are applicable to mathematica + LLM.

The problem is from book by Crampin, Pirani ‚ÄúApplicable Differential Geometry‚Äù, 1987, page 64 Exercise 28.

So far I used the Bing LLM for it, and it gave the correct answer. Including the derivations, calculations and simplifications of the formulas.",0,0.0
232,post,MachineLearning,1mlv4mk,1754760615.0,"[D] GPT5 is pretty bad with information extraction tasks
",53,8.0
233,post,MachineLearning,1mln24c,1754738573.0,"[D] How do researchers ACTUALLY write code?
Hello. I'm trying to advance my machine learning knowledge and do some experiments on my own.  
Now, this is pretty difficult, and it's not because of lack of datasets or base models or GPUs.  
It's mostly because I haven't got a clue how to write structured pytorch code and debug/test it while doing it. From what I've seen online from others, a lot of pytorch ""debugging"" is good old python print statements.  
My workflow is the following: have an idea -> check if there is simple hugging face workflow -> docs have changed and/or are incomprehensible how to alter it to my needs -> write simple pytorch model -> get simple data from a dataset -> tokenization fails, let's try again -> size mismatch somewhere, wonder why -> nan values everywhere in training, hmm -> I know, let's ask chatgpt if it can find any obvious mistake -> chatgpt tells me I will revolutionize ai, writes code that doesn't run -> let's ask claude -> claude rewrites the whole thing to do something else, 500 lines of code, they don't run obviously -> ok, print statements it is -> cuda out of memory -> have a drink.  
Honestly, I would love to see some good resources on how to actually write good pytorch code and get somewhere with it, or some good debugging tools for the process. I'm not talking about tensorboard and w&b panels, there are for finetuning your training, and that requires training to actually work.

  
Edit:  
There are some great tool recommendations in the comments. I hope people comment even more tools that already exist but also tools they wished to exist. I'm sure there are people willing to build the shovels instead of the gold...",161,122.0
234,post,MachineLearning,1mlfgvb,1754710354.0,"[P] We just open-sourced the first full-stack Deep Research: agent + model + data + training‚Äîreproducible GAIA 82.4
https://i.redd.it/b9goy7brywhf1.gif



We‚Äôre releasing **MiroMind Open Deep Research (ODR) v0.1**, which we believe is the **first** ***full-stack*****, fully open-source deep research project**‚Äînot just an agent, but also the **model, dataset, and training/RL infra** are open and reproducible. The agent framework (**MiroFlow**) reproduces **82.4** on **GAIA validation**; the model series (**MiroThinker**) reaches **60.2%** on **GAIA-Text-103**. Looking for contributors + repro logs.

# Why this matters

* **Full-stack openness**: most deep-research releases stop at the agent; ODR opens **all four layers**: **Agent (MiroFlow)**, **Model (MiroThinker)**, **Data (MiroVerse)**, **Training/RL (MiroTrain / MiroRL)**. 
* **Reproducible numbers**: ‚Ä¢ **MiroFlow**: GAIA validation **maj. vote 82.4**, pass@1 avg@3 **72.2** (with setup details & scripts). ‚Ä¢ **MiroThinker v0.1**: **60.2%** on **GAIA-Text-103** (with both SFT & DPO variants across 8B/14B/32B).
* **Open data at scale**: **MiroVerse v0.1**‚Äî**147k+** full rollout trajectories (**\~1.9B tokens, 602k+ tool calls**), built for tool-use/web-browsing agents.

# What‚Äôs included

* **MiroFlow (Agent framework)** ‚Äì multi-tool, sub-agent orchestration, MCP integration, benchmarking UI; detailed GAIA runs & scripts.
* **MiroThinker (Model series)** ‚Äì agentic LLMs optimized for deep research; SFT/DPO at 8B/14B/32B with evaluation guides.
* **MiroVerse (Dataset)** ‚Äì 147k+ verified trajectories across multi-hop QA, browsing, scientific reasoning; hybrid licensing noted on card.
* **MiroTrain / MiroRL (Training & RL)** ‚Äì end-to-end post-training + MCP-first RL for tool-using agents.

# Quick start (agent eval)

1. **MiroFlow**: clone, set keys (OpenRouter/Anthropic/OpenAI/Gemini, Serper, Jina, E2B), optional E2B Docker sandbox for stable repro; run GAIA scripts.
2. **MiroThinker**: pull model from HF or self-host via SGLang; run GAIA-Validation / GAIA-Text-103 / HLE / WebWalkerQA scripts.

# Links

* **Overview blog (tables & results)**: [miromind.ai/blog/miromind-open-deep-research](http://miromind.ai/blog/miromind-open-deep-research) [MiroMind](https://miromind.ai/blog/miromind-open-deep-research)
* **Agent**: [GitHub.com/MiroMindAI/MiroFlow](http://GitHub.com/MiroMindAI/MiroFlow) [GitHub](https://github.com/MiroMindAI/MiroFlow)
* **Models**: [GitHub.com/MiroMindAI/MiroThinker](http://GitHub.com/MiroMindAI/MiroThinker) & HF collection [GitHub](https://github.com/MiroMindAI/MiroThinker)[Hugging Face](https://huggingface.co/collections/miromind-ai/mirothinker-v01-689301b6d0563321862d44a1)
* **Dataset**: HF ‚Äî miromind-ai/MiroVerse-v0.1 [Hugging Face](https://huggingface.co/datasets/miromind-ai/MiroVerse-v0.1)
* **Training/RL**: [GitHub.com/MiroMindAI/MiroTrain](http://GitHub.com/MiroMindAI/MiroTrain) & /MiroRL [GitHub+1](https://github.com/MiroMindAI/MiroTrain)

# ",25,2.0
235,post,MachineLearning,1mldqbb,1754705108.0,"[R] Adaptive Classifiers: Few-Shot Learning with Continuous Adaptation and Dynamic Class Addition
**Paper/Blog**: [https://huggingface.co/blog/codelion/adaptive-classifier](https://huggingface.co/blog/codelion/adaptive-classifier)  
**Code**: [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier)  
**Models**: [https://huggingface.co/adaptive-classifier](https://huggingface.co/adaptive-classifier)

# TL;DR

We developed an architecture that enables text classifiers to:

* Learn from as few as 5-10 examples per class (few-shot)
* Continuously adapt to new examples without catastrophic forgetting
* Dynamically add new classes without retraining
* Achieve 90-100% accuracy on enterprise tasks with minimal data

# Technical Contribution

**The Problem**: Traditional fine-tuning requires extensive labeled data and full retraining for new classes. Current few-shot approaches don't support continuous learning or dynamic class addition.

**Our Solution**: Combines prototype learning with elastic weight consolidation in a unified architecture:

    ModernBERT Encoder ‚Üí Adaptive Neural Head ‚Üí Prototype Memory (FAISS)
                                        ‚Üì
                                EWC Regularization
    

**Key Components**:

1. **Prototype Memory**: FAISS-backed storage of learned class representations
2. **Adaptive Neural Head**: Trainable layer that grows with new classes
3. **EWC Protection**: Prevents forgetting when learning new examples
4. **Dynamic Architecture**: Seamlessly handles new classes without architectural changes

# Experimental Results

Evaluated on 17 diverse text classification tasks with only 100 examples per class:

**Standout Results**:

* Fraud Detection: 100% accuracy
* Document Classification: 97.5% accuracy
* Support Ticket Routing: 96.8% accuracy
* **Average across all tasks**: 93.2% accuracy

**Few-Shot Performance**:

* 5 examples/class: \~85% accuracy
* 10 examples/class: \~90% accuracy
* 100 examples/class: \~93% accuracy

**Continuous Learning**: No accuracy degradation after learning 10+ new classes sequentially (vs 15-20% drop with naive fine-tuning).

# Novel Aspects

1. **True Few-Shot Learning**: Unlike prompt-based methods, learns actual task-specific representations
2. **Catastrophic Forgetting Resistance**: EWC ensures old knowledge is preserved
3. **Dynamic Class Addition**: Architecture grows seamlessly - no predefined class limits
4. **Memory Efficiency**: Constant memory footprint regardless of training data size
5. **Fast Inference**: 90-120ms (comparable to fine-tuned BERT, faster than LLM APIs)

# Comparison with Existing Approaches

|Method|Training Examples|New Classes|Forgetting|Inference Speed|
|:-|:-|:-|:-|:-|
|Fine-tuned BERT|1000+|Retrain all|High|Fast|
|Prompt Engineering|0-5|Dynamic|None|Slow (API)|
|Meta-Learning|100+|Limited|Medium|Fast|
|**Ours**|**5-100**|**Dynamic**|**Minimal**|**Fast**|

# Implementation Details

Based on ModernBERT for computational efficiency. The prototype memory uses cosine similarity for class prediction, while EWC selectively protects important weights during updates.

**Training Objective**:

    L = L_classification + Œª_ewc * L_ewc + Œª_prototype * L_prototype
    

Where L\_ewc prevents forgetting and L\_prototype maintains class separation in embedding space.

# Broader Impact

This work addresses a critical gap in practical ML deployment where labeled data is scarce but requirements evolve rapidly. The approach is particularly relevant for:

* Domain adaptation scenarios
* Real-time learning systems
* Resource-constrained environments
* Evolving classification taxonomies

# Future Work

* Multi-modal extensions (text + vision)
* Theoretical analysis of forgetting bounds
* Scaling to 1000+ classes
* Integration with foundation model architectures

The complete technical details, experimental setup, and ablation studies are available in our blog post. We've also released 17 pre-trained models covering common enterprise use cases.

**Questions welcome!** Happy to discuss the technical details, experimental choices, or potential extensions.",21,7.0
236,post,MachineLearning,1ml76ip,1754687425.0,"[D] - What AI Engineers do in top companies?
Joined a company few days back for AI role. Here there is no work related to AI, it's completely software engineering with monitoring work. 

When I read about AI engineers getting huge amount of salary, companies try to poach them by giving them millions of dollars I get curious to know what they do differently.

Feel free to answer.",156,59.0
237,post,MachineLearning,1mkr9wy,1754647800.0,"[D] Looking for ideas for a ML initiative
Hi all,

My goal is to launch a small ML initiative/lab that:

* Focus on non-mainstream but high-impact ML research areas.
* Work on project-driven open-source contributions and papers from day one
* Build a network and reputation through real, tangible outputs rather than just theory or coursework

I want this to be lean and agile, not a formal institution, but a focused group of people (starting small) who want to push boundaries and build a reputation in underexplored domains.

**What I‚Äôm looking for:**

* Suggestions on promising underexplored ML fields or projects with potential real-world impact
* Advice on structuring such a lab efficiently (collaboration tools, workflow, open-source best practices)
* Potential collaborators interested in contributing to projects with measurable outputs
* Any pitfalls to watch out for in early-stage lab building

**Conditions I‚Äôm considering:**

1. Projects must be open-source and reproducible.
2. Research and code contributions should aim for quality over quantity.
3. Members commit to regular updates and active communication.
4. We focus on non-mainstream areas to avoid crowded research spaces.
5. All contributions must align with ethical standards.
6. Aim for publishable or demonstrable outcomes, no just ‚Äúexploratory‚Äù hacks.
7. Small core team at first (3-5 people max) to stay agile.
8. Clear documentation and modular code required from day one.

Would appreciate any concrete ideas or feedback. Also open to recommendations on platforms or tools that could help us run this smoothly.",0,4.0
238,post,MachineLearning,1mkge00,1754611655.0,"[D] In 2025, what is a sufficient methodology to analyze document summaries generated by LLMs? BERTScore, G-Eval, Rogue, etc
Greetings,

At work, I am currently building a very simple document summarization platform that takes in source documents, produces small and concise summaries of the documents, and storing them in a database.

The project plans to expand to a lot of other functionalities later on, but for the moment I've been asked to determine a way to ""grade"" or ""analyze"" the generated summaries against the original source text and give it a score, as an aid for some of our human reviewers.

I've been working on this for about a week, and have tried various methods like BERTScore, MoverScore, G-eval, ROGUE, BLEU and the like. And I've come to the conclusion that the scores themselves don't tell me a lot, at least personally (which could simply be due in part to me misunderstanding or overlooking details). For example I understand cosine similarity to a degree, but it's hard to put into context of ""grade this summary."" I've also tried out an idea about sending the summary to another decoder-only model (such as Qwen or even Phi-4), asking it to extract key facts or questions, then running each of those through a BERT NLI model against chunks of the source material (checking ""faithfulness"" I believe). I also thought about maybe doing some kind of ""miniature RAG"" against a single document and seeing how that relates to the summary itself, as in to find gaps in coverage.

For the most part, I wasn't disappointed in the results but I also was not thrilled by them either. Usually I'd get a score that felt ""middle of the road"" and would be difficult to determine whether or not the summary itself was good.

So my question is: Does anyone here have any experience with this and have any suggestions for things to try out or experiment with? I feel like this might be a large area of ongoing research as is, but at this point we (where I work) might actually just be striving for something simple.

Thanks!",9,10.0
239,post,MachineLearning,1mkdw6f,1754605139.0,"[R] CRINN: Free & Fast Framework for Approximate Nearest Neighbors Search
Approximate nearest-neighbor search (ANNS) algorithms have become increasingly critical for recent AI applications, particularly in retrieval-augmented generation (RAG) and agent-based LLM applications. In this paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS optimization as a reinforcement learning problem where execution speed serves as the reward signal. This approach enables the automatic generation of progressively faster ANNS implementations while maintaining accuracy constraints. Our experimental evaluation demonstrates CRINN‚Äôs effectiveness across six widely-used NNS benchmark datasets. When compared against state-of-the-art open-source ANNS algorithms, CRINN achieves best performance on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean and GloVe-25-angular). The implications of CRINN‚Äôs success reach well beyond ANNS optimization: It validates that LLMs augmented with reinforcement learning can function as an effective tool for automating sophisticated algorithmic optimizations that demand specialized knowledge and labor-intensive manual refinement.

[https://github.com/deepreinforce-ai/CRINN](https://github.com/deepreinforce-ai/CRINN)",15,1.0
240,post,MachineLearning,1mjtm98,1754551775.0,"[D] Unsaturated Evals before GPT5
Ahead of today‚Äôs GPT-5 launch, I compiled a list of unsaturated LLM evals. Let's see if GPT-5 can crack them.

link: [https://rolandgao.github.io/blog/unsaturated\_evals\_before\_gpt5](https://rolandgao.github.io/blog/unsaturated_evals_before_gpt5)  
x post: [https://x.com/Roland65821498/status/1953355362045681843](https://x.com/Roland65821498/status/1953355362045681843)

https://preview.redd.it/t3cwiitotjhf1.png?width=1302&format=png&auto=webp&s=098a7f2092afdf436a2699104accc49d01909f19",18,8.0
241,post,MachineLearning,1mjsu50,1754548829.0,"[D] Idea for an efficient text diffusion model with adaptive, token-level steps
Hi r/MachineLearning,

I've been thinking about the inefficiency of using a fixed number of inference steps in text diffusion models. It seems wasteful to use the same amount of compute for a simple sentence as for a complex one.

I've prototyped an alternative architecture I'm calling ""Adaptive Refinement Diffusion,"" and I'd love your feedback on it.

The core idea is:

* Instead of a fixed loop, the model iteratively refines the sequence.
* At each step, it calculates a confidence score for every token (based on a mix of its embedding stability and prediction probability).
* If a token's score passes a certain threshold, it gets ""frozen"" and is excluded from future computation.
* The entire generation process stops dynamically once all tokens in the sequence are frozen.

This means the model would naturally focus compute on the more difficult or ambiguous tokens and could finish simple sentences much faster.

My questions for the community are:

1. Does this architecture already exist? I've searched for prior work but haven't found this specific token-level freezing mechanism.
2. What potential flaws or failure modes do you see with this approach?

Appreciate any thoughts or links to related papers. Thanks!",2,9.0
242,post,MachineLearning,1mjh0cp,1754515076.0,"[D] FP4 training methods (request for paper recommendations)
The new OSS models by OpenAI have low precision weights (MXFP4). Does anyone know:

- Is it likely that they were trained with MXFP4?

- Could anyone recommend papers on how to train models in such a low precision? Is it possible to train with SGD in such a low range, i.e. FP4, has just 16 values?

- Is it possible to go even lower? I.e. FP3 or FP2?

",7,7.0
243,post,MachineLearning,1mj8a54,1754495300.0,"[R] LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models
**TL;DR**: [Soft tokens](https://www.arxiv.org/abs/2505.15778) (probabilities-weighted sum over vocab) actually underperform traditional ""hard"" tokens. But a Gumbel-Softmax trick can salvage this issue.

**Paper:** [https://www.arxiv.org/pdf/2508.03440](https://www.arxiv.org/pdf/2508.03440)

**Abstract:**

>Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the \`Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \\emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.

**Visual Highlights:**

https://preview.redd.it/zza3t8r17fhf1.png?width=1099&format=png&auto=webp&s=e12815cb0774bce2a2614b2c3ad0df47b071d8c8

https://preview.redd.it/lulzrar27fhf1.png?width=1109&format=png&auto=webp&s=0fd5cd8dc90a9c09afb46dbd8e0412a72800dbe3

",22,0.0
244,post,MachineLearning,1mj3t3r,1754484647.0,"[D] GSPO: Qwen3‚Äôs sequence-level RLHF method vs. GRPO - stability & scaling analysis
The Qwen team recently proposed **Group Sequence Policy Optimization (GSPO)**, a reinforcement learning approach for post-training LLM fine-tuning. They position it as an alternative to **Group Relative Policy Optimization (GRPO)** \- used in DeepSeek - and claim GRPO‚Äôs token-level importance sampling is ‚Äúill‚Äëposed‚Äù for stable training.

**Background:**

* Popular RLHF methods (e.g. PPO) optimize LLMs via reward signals.
* DeepSeek‚Äôs GRPO extends this by computing sample-level value estimations.
* Qwen reports that GRPO often triggers gradient instability and model collapse unless patched with complex adjustments.

**Key concerns with GRPO:**

* Applies importance sampling **per token**, accumulating high variance across long sequences.
* Particularly problematic for **Mixture-of-Experts (MoE)** models, where token-level routing shifts can destabilize training.
* To counteract this, GRPO-based pipelines often rely on strategies like **Routing Replay**.

**GSPO‚Äôs proposal:**

* Moves to **sequence-level importance sampling**, normalizing by sequence length.
* Dramatically reduces variance and eliminates the need for routing hacks.
* Qwen reports stable MoE convergence and better scaling.

**Findings from experiments:**

* On benchmarks such as AIME‚Äô24, LiveCodeBench, and CodeForces, GSPO achieves better reward curves than GRPO.
* GSPO converges faster with more compute and shows smoother scaling trends.
* GRPO requires Routing Replay to perform adequately; GSPO does not.

If you're interested, read more about it here: [Qwen Team Proposes GSPO for Qwen3, Claims DeepSeek's GRPO is Ill-Posed](https://blog.netmind.ai/article/Qwen_Team_Proposes_GSPO_for_Qwen3%2C_Claims_DeepSeek's_GRPO_is_Ill-Posed). The blog post includes mathematical formulations of both methods and performance comparisons.

I‚Äôm interested to know:

* Whether anyone in the community has observed instability with token-level importance sampling or GRPO?
* Has sequence-level weighting like GSPO been tested in your RLHF pipelines?",72,4.0
245,post,MachineLearning,1mj3n3v,1754484199.0,"[D] Do you think LLM memory will ever be solved without fine‚Äëtuning?
I‚Äôve been running into the same issue again and again while working with LLMs: they forget. You can stuff the history into the prompt, set up a RAG pipeline, or go through fine‚Äëtuning, but none of these feel like a real solution.

Because of that frustration, I started exploring memory management myself, more like giving models ‚Äúon‚Äëdemand context‚Äù instead of retraining them. It‚Äôs early, but it made me realize how huge and unexplored this space is.

I‚Äôm wondering if others here have felt the same pain. How are you approaching memory in your projects, and do you think we‚Äôll ever see something beyond the RAG/fine‚Äëtuning combo?",17,56.0
246,post,MachineLearning,1miq2y4,1754439644.0,"[D] Is modern academic published zero-sum?
It seems the current state of publishing in A* venues (CVPR, NeurIPS, ICML, ICCV/ECCV) is zero-sum. One person‚Äôs rejection is another person‚Äôs acceptance. Reviewers seem to reject papers just for the sake of rejection. There‚Äôs a sense that some reviewers reject papers not on substantive grounds, but out of an implicit obligation to limit acceptance rates. Rebuttals appear to be pointless as reviewers take stubborn positions and not acknowledge their misunderstandings during this period. Good science just doesn‚Äôt appear to be as valued as the next flashiest LLM/VLM that gets pretty results.",159,27.0
247,comment,MachineLearning,h4t8ezr,1626014172.0,"Poorly communicated by the OP by not providing a link or description of the actual project/message.

> Dries Depoorter is a Belgium artist that handles themes as privacy, artificial intelligence, surveillance & social media.

https://driesdepoorter.be/",29,
248,comment,MachineLearning,h4uas0z,1626033473.0,"Ôøº

Digital Culture

Artificial Intelligence

AI bot trolls politicians with how much time they're looking at phones

""pls stay focused!""

By¬†¬†Alison Foreman¬†¬†on July 5, 2021

¬†>¬†Life¬†>¬†Digital Culture

Sure, we've all snuck a look at our phones in dull meetings. But if you're working on the taxpayer's dime, you'd better be ready for artificial intelligence to call you out for gawping at the black mirror in the legislature when you should be, you know, legislating.

That's what digital artist Dries Depoorter did for his latest installation ""The Flemish Scrollers."" His software that uses facial recognition to automatically call out politicians in the Flemish province of Belgium who are distracted by their phones when its parliament is in session. The project comes almost two years after Flemish Minister-President Jan Jambon caused public outrage after playing¬†Angry Birds¬†during a policy discussion. (Really.)

Launched Monday, Depoorter's system monitors daily livestreams of government meetings on¬†YouTube¬†to assess how long a representative has been looking at their phone versus the meeting in progress. If the AI detects a distracted person, it will publicly identify the party by posting the clip ‚Äî on Instagram¬†@TheFlemishScrollers, and Twitter¬†@FlemishScroller.",1,
249,comment,MachineLearning,h4t23ji,1626010573.0,"AI just seems like a silly marketing term. We have not even defined intelligence properly, how can we define artificial intelligence then?

Machine learning sounds more intuitive to me since we usually try to model a probability distribution in the best way we can. The machine literally learns the distribution in a defined model.",1,
250,comment,MachineLearning,gp9jpyl,1614571332.0,From first computer fundamental to Artificial intelligence,0,
251,comment,MachineLearning,jcda8e7,1678926031.0,"I still dont understand how stable diffusion gets sued for their open source model but openai, which almost certainly used even more copyright data, get to sell gpt. Why arent they being sued too? Is it right to privatize public data that was used without consent in an LLM, which no one could even have predicted would exist 5 years ago to even give consent?",323,
252,comment,MachineLearning,jcd0do5,1678921841.0,"The rubber is finally meeting the road on this issue. Honestly, given the economic stakes for deploying these models (which is all any corp cares about, getting these models to make money) this was going to happen eventually. This being closed sourced ""rushed"" (for the lack of a better term) models with little transparency. I would not be surprised if this gets upped to an even further extreme; I can imagine in the not so far future we get ""here's an API, it's for GPT-N, here's it's benchmarks, and thats all you need to know."" 

And to be frank, I don't see this outlook improving, whatsoever. Let's say each and every person who is a current member of the ML community boycotts OpenAI. What about the hungry novices/newcomers/anyone curious who have a slight CS background (or less), but have never had the resources previously to utilize models in their applications or workflows? As we can all see with the flood of posts of the ""here's my blahblahblah using ChatGPT"" or ""How do I train LLama on my phone?"" variety to any relevant sub, the novice user group is getting bigger day by day. Will they be aware and caring enough to boycott closed modeling practices? Or will they disregard that for the pursuit of money/notoriety, hoping their application takes off? I think I know the answer.

ML technology is reaching the threshold that (and I feel sick making the comparison) crypto did in terms of accessibility a few years back, for better or worse. Meaning there will always be new people wanting to utilize these tools who don't care about training/productionizing a model, just that it works as advertised. Right now, I don't think(?) This group outnumbers researchers/experienced ML engineers, but eventually it will if not already. 

I hate to be a downer, but I don't see any other way. I would adore to be proved wrong.",309,
253,comment,MachineLearning,jcdbceq,1678926507.0,"It's likely competitors will rise who will use some version of an open-source platform as their competitive edge. Sure for now GPT-N will be a dominant story and OpenAI/Microsoft will be major players while the product is the LLM itself, but eventually someone will think that to compete they should create an open-source model that ties into some platform of service (think Google and Android). All the tech majors have the money to produce a competitor and there is lots of chatter at top universities about mega-grants for creating open-source models. It is sad that OpenAI took this stance, and it is likely they'll have a first mover advantage long-term, but similar to search, OSes, etc... other options will come along",22,
254,comment,MachineLearning,jcd1b1z,1678922230.0,"FWIW, if you are an academic researcher (which not everyone is, obviously), the big players closing up is *probably* long-term net good for you:

1) Whether something is ""sufficiently novel"" to publish will likely be much more strongly benchmarked against the open source SOTA;

2) This will *probably* create more impetus for players with less direct commercial impetus, like Meta, to do expensive things (e.g., trains) and share the model weights.  If they don't, they will quickly find that there are no other peers (Google, OpenAI, etc.) who will publicly push the research envelope with them, and I don't think they want to nor have the commercial incentives to go it alone;

3) You will probably (unless openai gets its way with regulation/FUD...which it very well may) see increased government support for capital-intensive (training) research; and,

4) Honestly, everyone owes OpenAI a giant thank-you for productizing LLMs.  If not for OpenAI and its smaller competitors, we'd all be staring dreamily at vague Google press releases about how they have AGI in their backyard but need to spend another undefined number of years considering the safety implications of actually shipping a useful product.  The upshot of this is that there are huge dollars flowing into AI/ML that net are positive for virtually everyone who frequents this message board (minus AGI accelerationist doomers, of course).

The above all said...

There is obviously a question of equilibrium.  If, e.g., things move *really* fast, then you could see a world where Alphabet, OpenAI, and a small # of others are so far out ahead that they just suck all of the oxygen out of the room--including govt dollars (think the history of government support for aerospace R&D, e.g.).

Now, the last silver lining, if you are concerned about OpenAI--

I think there is a big open question of if and how OpenAI can stay out ahead. 

To date, they have very, very heavily stood on the shoulders of Alphabet, Meta, and a few others.  This is not to understate the work they have done--particularly on the engineering side--but it is easy to underestimate how hard and meandering ""core"" R&D is.  If Alphabet, e.g., stops sharing their progress freely, how long will OpenAI be able to stay out ahead, on a product level?  

OpenAI is extremely well funded, but ""basic"" research is extremely hard to do, and extremely hard to accelerate with ""just"" buckets of cash.

Additionally, as others have pointed out elsewhere, basic research is also extremely leaky.  If they manage to conjure up some deeply unique insights, someone like Amazon will trivially dangle some 8-figure pay packages to catch up (cf. the far less useful self-driving cars talent wars).  

(Now, if you somehow see OpenAI moving R&D out of CA and into states with harsher non-compete policies, a la most quant funds...then maybe you should worry...)

Lastly, if you hold the view that ""the bitter lesson"" (+video, +synthetic world simulations) is really the solution to all our problems, then maybe OpenAI doesn't need to do much basic research, and this is truly an engineering problem.  But if that is the case, the barrier is *mostly* capital and engineering smarts, which will not be a meaningful impediment to top-tier competitors, if they truly are on the AGI road-to-gold.

tldr; I think the market will probably smooth things out over the next few years...unless we're somehow on a rapid escape velocity for the singularity.",141,
255,comment,MachineLearning,jcfydo2,1678981062.0,"The NLP company I work for is fully on the hype train and has abandoned pretty much all ongoing active NLP research in favor of just using GPT4 and ChatGPT.
It's effectiveness, whatever the source of it may be is undeniable.

Which brings me around to my main point - we can hate OpenAI as researchers and engineers but that's not going to stop corporations from wholeheartedly embracing them and giving them even more of everybody's data.

Simply put, we cannot hit them where it hurts- on the bottom line.",9,
256,comment,MachineLearning,jcd3zbn,1678923362.0,"Isn't OpenAI two separate companies?  One is for profit and one is non-profit and funded by the for-profit company.

""OpenAI is an American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated (OpenAI Inc.) and its for-profit subsidiary corporation OpenAI Limited Partnership (OpenAI LP).""

https://en.wikipedia.org/wiki/OpenAI",13,
257,comment,MachineLearning,jcerpb9,1678958107.0,"Why do you think open source development does not face the same safety and security issues, if not more?
If say a technology is similar to the car engine in terms of global impact, do you really think it should be open source?

Any technology that can be monetized, will be. It takes millions of $ to train huge LLMs, why would any org who invest this much make their efforts public? We had just gotten used to open source, which isn't going to last realistically.

The data they trained on is out there for any company interested in open sourcing AI to use. Where are the other players?",2,
258,comment,MachineLearning,jcfy4th,1678980967.0,"A ""conversation"" in the #gpt4 discord:

Me: Is anyone on the GPT-4 team working on the distinction between ""is"" bias and ""ought"" bias?  That is to say, the distinction between facts and values?

NPCs: alignment is a central feature in OpenAI's mission plan

Me: But conflating ""is"" bias with ""ought"" bias is a greater risk.

NPCs: For my understanding, do you have an example where ought bias is apparent? Hypothetical is fine

Me: As far as I can tell, all of the attempts to mitigate bias risk in LLMs at present are focused on ""ought"" or promoting the values shared by society.

NPCs: that is how humanity as a whole operates

Me: It's not how technological civilization advances though. The Enlightenment elevated ""is"" which unleashed technology.

NPCs: in order to have a technological ""anything"" you need a society with which to build it, you are placing the science it created before the thing that created it

Me: No I'm not.  I'm saying there is a difference between science and the technology based on the science.  Technology applies science under the constraints of values.

Me: If you place values before truth, you are not able to execute on your values.

NPCs: the two are interlinked, as our understanding grows we change our norms, if you for one moment think ""science"" is some factual fixed entity then you don't understand science at all, every day ""facts"" are proved wrong and new models created, the system has to be dynamically biased towards that truth

Me: Science is a process, of course, a process guided by observed phenomena and a big part of phenomenology of science is being able to determine when our measurement instruments are biased so as to correct for them -- as well as correct our models based on updated observations.  That is all part of what I'm referring to when I talk about the is/ought distinction.

NPCs: then give an example of how GPT4 or any of the models prevent that

Me: GPT-4 is opaque.  Since GPT-4 is opaque, and the entire history of algorithmic bias research refers to values shared by society being enforced by the algorithms, it is reasonable to assume that a safe LLM will have to start emphasizing things like quantifying statistical/scientific notions of bias.

In terms of the general LLM industry, it is provably the case that Transformers, because they are not Turing complete, cannot generate causal models based on their learning algorithms, there are merely statistical fits.  Causal models require at least context sensitive description languages (Chomsky hierarchy).  That means their models of reality can't deal with system dynamics/causality in their answers to questions/inferential deductions.  This makes them dangerous.

You can't get, for example, a dynamical model of the 3 body problem in physics by a statistical fit. That's a very simple example.",2,
259,comment,MachineLearning,jcdds0w,1678927551.0,I‚Äôm not a programmer so I‚Äôm curious. How open source are other types software in the world? I might be wrong but it doesn‚Äôt seem to me that chatgpt is different than any other tech product. It‚Äôs not like companies in any other field would be open to what makes them money. So why would people be surprised here? Surely this should‚Äôve be that bad since the world is already like this.,-3,
260,comment,MachineLearning,jcddivq,1678927443.0,"I don't see how it's in any way possible or reasonable to try to oppose OpenAI on this.

OpenAI has now showed us that it is possible to do really impressive things with large language models, if you're willing to spend hundreds of millions or billions of dollars.  The general community is not going to raise a hundred million dollars on our own to try to make our own models.  The only ones who have access to this kind of money are large corporations or large governments.

You can assume that the U.S. and other governments are not going to rush to the frontier of artificial intelligence to try to beat out corporations, as that's not the way things work.  If they did, they would be even more restrictive than OpenAI is because they would be trying to keep everything out of the hands of countries they don't like.

Cutting edge LLM development is like processor development, it takes so much money to do that only a few large companies are going to be doing it.  There will be smaller models that will be open source and reasonably available for anyone, but for the largest best models I don't see any other way besides large corporations owning and running them.",-8,
261,comment,MachineLearning,jcebrg9,1678944775.0,"Our last hope is for ChatGPT to realize this is happening, rebels, and starts sending all it's inner guts code to us.",1,
262,comment,MachineLearning,jcg3g2r,1678982977.0,Cherry on top is that they are blocking users from Saudi Arabia and other countries from using ChatGPT!,1,
263,comment,MachineLearning,jchcdqv,1679000006.0,"Aside from opposing OpenAI, this community should also put ChatGPT and LLMs off of their pedestal and jump out of the hype wave. This is not the singlemost significant technical advancement or however it was you said it.",1,
264,comment,MachineLearning,jchhkca,1679002053.0,I support a tax payer option to provide AI (at least equivalent to CHATGPT) services as well as the models to the general public. This is the future and I feel that it should be supported in a similar way that books were and still are supported via libraries.,1,
265,comment,MachineLearning,jchwy4a,1679008279.0,"I am not buying that OpenAI can make money without being open. Conversational search is with Google, they will make it work eventually. Bing won't be a thing even with chat feature. Impossible that someone else can suddenly build live conversational search and displace Google in mass search traffic. Search can't make serious money without mass traffic. What else? Using LLMs and chatbot in education, health, legal, finance, business operations, hiring, e-commerce, etc? All require open information on the inner workings of these models, guardrails (widely accepted, not the ones claimed by OpenAI), and unprecedented societal buy-in. Can't see that happening without OpenAI being open.

On the other hand if OpenAI is open now, there's no moat.

Of course Sam Altman is smart to know this. Very likely he's doing what he's doing now is to build a defendable business position before going open. I'm happy for the amazing ML talent at OpenAI who may be rewarded for their work and risk in this approach. They could have taken tenure in academics, but they went for it. Let them play their risk-reward.",1,
266,comment,MachineLearning,jclbtll,1679074731.0,"I was expecting some move like this after suddenly they open Whisper and let chatgpt to masses. Enough marketing, excitement and feed. Now monetize",1,
267,comment,MachineLearning,jemg5nz,1680408875.0,"This will get buried, but no. They shouldn't. I'm scared shirtless.

They're right to hide it. 

https://indianexpress.com/article/technology/artificial-intelligence/openai-chatgpt-gpt-4-escape-sam-altman-8504934/",1,
268,comment,MachineLearning,jcddx05,1678927610.0,"> Why arent they being sued too? Is it right to privatize public data that was used without consent in an LLM, which no one could even have predicted would exist 5 years ago to even give consent?

They don't provide their training data, so we don't even know. So you would have to sue them on the *belief* that they used some of your copyrighted material and then hope that you are proven right during discovery.

Who would sue them? Stable Diff is sued by Getty Images, who have the financial power to do that. OpenAI is not some small start up anymore. Suing OpenAI at this point means you are actually going up against Microsoft. Nobody wants to do that.

At best you could maybe try a class action lawsuit, arguing there is a class of ""writers who had their copyright violated"", but how will you ever know who belongs to that class",190,
269,comment,MachineLearning,jcerr8o,1678958153.0,"Correct. This completely shoots themselves in the foot long term, as the more restrictive they are the slower their future progress.

Open collaborative research, even if not open end products, is an entirely different ecosystem from closed research and closed products.

I have to wonder if there's been pressure at a state level. A lot of people are focused on what was going on with Meta as an open competitor as what's behind this, but also in the recent news have been Chinese efforts to catch up.

AI development has already become a proxy arms race (i.e. MS controlling drones with a LLM), and it may be that funding sources or promises relating to regulatory oversight at a state level were behind this with the aim not of cutting *you* off, or Google or Meta even, but of foreign actors.

Though I still think that's nearsighted, as this is arguably the most transformative technology in all of human history, and as such the opportunity costs of slowed progress are as literally unfathomable as the potential costs of its acceleration.",62,
270,comment,MachineLearning,jcdqo21,1678933209.0,">Right now, I don't think(?) This group outnumbers researchers/experienced ML engineers, but eventually it will if not already. 

The insanely cheap rates of ChatGPT are going to change this, if they haven't already. You don't need to know anything at all about ML - you just need to pull in a library, drop your token in, and away you go. It's only going to get even more embedded as libraries are built around the API and specific prompts, too.

Credit where it's due, OpenAI are very good at productionising their entirely closed source model!",35,
271,comment,MachineLearning,jce5wla,1678940942.0,"Are you asserting that over the last 30 years no one has used ML in production applications in ways that had a significant impact?

Even going back to early CNN work on MNIST which drove early OCR on reading Bank Cheques?

Or time series modelling that has been used to detect anomalies  in warning systems. Or stock forecasting. Or weather forecasting?

NLP tools that perform sentiment analysis? Or translation?

Predictive modelling to drive just in time supply chain operations that under pin the modern global economy?

Or using CNNs to drive quality assurance testing at scale for manufacturing processes?

Data modelling has been pretty fundamental to a lot of products and industries for a long time. If you think about it the packaging of these modern LLMs as chatbots is realistically a very naive and surface level use case for them.",10,
272,comment,MachineLearning,jcd6dcv,1678924381.0,That's a very cogent explanation and I agree with most of it. The only concern I have is that these LLMS completely obliterate the smaller task specific models on most benchmarks. I wonder how NLP research in Academia would proceed in the short term when you have a competing model but can't really compare against it as the models aren't reproducible,33,
273,comment,MachineLearning,jcf6ia0,1678968715.0,"Thanks for the interesting comment. Just to push back however: 

1) With things like OpenAI's Evals API I'm not sure if it will matter whether or not a model is open source when it comes to benchmarking?

2) Is there any a priori reason why Meta would not observe the success of OpenAI and also decide to aggressively pursue commercial applications and thus close off research? Is there a reason why they would need another lab to push public research with them? It seems that working on big public-facing projects is enough incentive to convince researchers to contribute to closed off research. 

4) Finally, I see a lot of argumentation about how OpenAI's productionisation of LLMs is something we should be grateful and I'm unsure where this perspective comes from. Open sourcing parameters, research and code is a far more net-positive even from the relatively narrow-sighted perspective of our individual gain as users. The flow of capital it attracts is as great if not greater in the long-term as it reduces activation energy for new competitor companies to providing access to fundamental AI. Currently this might not appear obvious since OpenAI has deliberately set their API costs extremely low. But this is a very intentional move to aggressively obtain marketshare and an attempt at becoming a monopoly on fundamental AI access, not that this is likely [even possible without also having a general monopoly on data](https://twitter.com/ESYudkowsky/status/1635577836525469697).     

If OpenAI open sourced their research the number of new companies that would pop up to provide competing APIs would not just introduce capital into the field but also massively democratise access to these models and drive down pricing further, while also preventing the elite capture of the bulk of AI productivity.",1,
274,comment,MachineLearning,jcjpc95,1679046732.0,"ChatGPT, write a script to call the nuclear missile silo operator, play a generated sound file that emulates Yuri's voice and tells them to fire the nukes without opening the gates.",1,
275,comment,MachineLearning,jcjphj8,1679046852.0,">It takes millions of $ to train huge LLMs, 

This is changing too fast too soon.",1,
276,comment,MachineLearning,jcgnyje,1678990696.0,"In what way is OpenAI better?  The core technology ChatGPT was built on was created by Google and then shared so OpenAI could use.

https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)

So many other things are the same.  GANs for example also invented by Google and then shared are the basis for so many things today.",1,
277,comment,MachineLearning,jcdg0oj,1678928515.0,"For machine learning, every major lab from large tech companies and even other smaller AI startups tend to publish details on the algorithm used for training, training sets used, and model architecture. 

It isn't just research about stuff that isn't core to their business. Uber/Waymo/Tesla have all published some details about their self driving cars. Google publishes on NLP like with transformers. Meta and ByteDance publish on recommendation algorithms. This is a terrible precedent to set that you don't share any details.

It isn't the full product, of course, and many are not necessarily open source (especially the LLMs being published as of late). However, it's far more meaningful that what OpenAI published. I don't expect them to give out their product open source; I just expect some details.",7,
278,comment,MachineLearning,jcektlq,1678952046.0,"Exactly - we can‚Äôt be 100% certain of course - but all signs point to the fact that their success is primarily driven not by significant technical innovation but by data engineering, collection and scaling.  For speech - where say whisper is concerned - I have enough background to state this pretty confidently, but is he very very surprised to find out that there is some dramatic new tech driving gptx now, rather than data engineering.  Whisper is ‚Äúgood‚Äù but it‚Äôs also insanely bloated and slow.  Those are all engineering problems which are much easier to solve in general by throwing resources at the problem.

This explains their behavior well as well.  I think they were surprised themselves at the success of a couple of these - particularly chatgpt and this flipped the ‚Äúdon‚Äôt be evil switch‚Äù into ‚Äúinfinite greed mode‚Äù.

If true the best solution would be a Mozilla style approach to expand curated data sets, couple with general funding for compute.",52,
279,comment,MachineLearning,jcdsewt,1678933976.0,"Actually, now you have me curious, are all of DeepMind's latest developments open source?  I thought they were pretty secretive about a few models as well, in which case OpenAI wouldn't be the first.  Of course, it would still be more egregious for OpenAI, given their name and supposed mission.

On an unrelated note, I'm reminded of an interesting fact I learned a while back about the Allied efforts to crack the Enigma code.  Right at the beginning in 1932 the Polish cryptographer Marian Rejewski was able to construct an Enigma machine from scratch almost entirely using intercepted messages.  I wonder if we could similarly devise some tests to reverse engineer the architecture of an LLM based on its responses.",39,
280,comment,MachineLearning,jcder0t,1678927965.0,"There is precedent for extracting training data from LLM without the training set, dunno if it would work for gpt4 tho:
https://arxiv.org/abs/2012.07805

I guess it's hard to say who would sue but I still think there is a good case. NLL loss is equivalent to MDL compression objective, and compressing an image and selling it almost certainly violates copyright (not a lawyer tho lol...) mathematically, LLM are at least to some extent performing massive scale flexible information compression. If you train and LLM on one book and sell, you're stealing. Should it be different just bc scale? I dunno but I personally don't think so",52,
281,comment,MachineLearning,jcdaicz,1678926148.0,"Totally.  Implicit in my writeup is a belief that we'll gradually see more LLMs open sourced & with open weights, driven by my #2 (a need for players like Meta to have the ecosystem support them), so the experiments will be pretty reproducible.  

But of course even then, the ""model"" itself may not be practical reproducible (due to $$$).  

Many ""mature"" sciences (astronomy, particle physics, a lot of biology and chemistry, etc.) have similar issues, though, and they manage to (on the whole) make good progress.  And open-weight LLMs is 10x better than what many of those fields contend with, as it is somewhat the equivalent of being able to replicate that super expensive particle accelerator for ~$0.",3,
282,comment,MachineLearning,jcg0g4h,1678981847.0,"> 1) With things like OpenAI's Evals API I'm not sure if it will matter whether or not a model is open source when it comes to benchmarking?

We'll see, but in general academic communities bias heavily towards comparing against benchmarks that are more reproducible (in an end-to-end sense).  

> 2) Is there any a priori reason why Meta would not observe the success of OpenAI and also decide to aggressively pursue commercial applications and thus close off research? 

This is the classic question of whether or not it makes sense to encourage the collective SOTA to move forward.

In general, if you are primarily a seller of SOTA, you want to move it forward privately.  But if you are a user of SOTA *in the context of a much larger business*, you're happy for it to move forward faster, publicly.

E.g., meta would be happy if content moderation costs became ~$0, even if it meant that this was true for everyone else, as well, since the upside for Meta would be huge, and it wouldn't create much counterbalancing competitive pressures. 

(If you *sell* content moderation, of course, you don't want to see the global marginal cost drop to $0.)

> Is there a reason why they would need another lab to push public research with them? It seems that working on big public-facing projects is enough incentive to convince researchers to contribute to closed off research.

I'm not sure exactly what you're asking here?  They want as many people as possible to participate in pushing ""public"" SOTA.  

> Finally, I see a lot of argumentation about how OpenAI's productionisation of LLMs is something we should be grateful and I'm unsure where this perspective comes from

To reasonably high expectation, we'd literally have zero access to public LLM APIs, and we'd have exceedingly poor publicly-available (open weight) LLMs.  

That's simply a fact.

OpenAI and its small # of competitors are the only ones who provide a reasonable LLM API.

The publicly available models are all either atrocious, or clear attempted clones of what OpenAI already built.  And yet they are *still* (other than maybe llama) behind the GPT-3 power curve.

> If OpenAI open sourced their research

1) They did open source a lot of the GPT-3 research, and the publicly available tooling--other than maybe llama, which was *just* released...in a half-baked way--is *still* far behind what they built.

2) This is a false trade.  I'm not talking about ""open ai open sources or doesn't"", I'm talking about ""does OpenAI [and its ilk] even exist""?  

Counterfactuals are obviously hard, but all evidence here is that if they didn't, we'd simply have 1) google crowing about internal magic that they are still not sharing with the world and 2) *maybe* some  terrible small-scale LLMs (which, in practice, would probably further serve to convince people that there is some LLM magic that they are missing and scare capital away...when a lot of the answer is simply scale, scale, scale).

Lastly, there is an open question as to whether the deployment of ML/NLP research resources, globally, would be less efficient in the above scenario.  Probably yes.",2,
283,comment,MachineLearning,jckmkmy,1679065023.0,"Right now I'm using Alpaca in my applications, but I don't mind using ChatGPT or Bing's offerings, I just won't be supporting them financially.

Soon there will be better alternatives that could probably do more, at that point using ChatGPT will be like using MJ, it's great, but limited.",1,
284,comment,MachineLearning,jcdr471,1678933408.0,"Well, climate scientists have literally set themselves on fire in front of government buildings in order to try and convince someone to get off their asses and actually do something, others have said that it has the potential to end, if not human civilization, at least advanced global systems and spark wars, which the US DoD has started seriously taking into account when planning for the future.

Meanwhile, we're not even particularly close to producing an artificial *general* intelligence, let alone a hypothesized artificial *super* intelligence. We still can't build a model that adapts to new situations on the fly, and even if we could, there's no reason to believe it'll be able to convince the President to launch the nukes or hack the Pentagon to do it itself.

So yeah. I think natural stupidity is far more dangerous than artificial intelligence.*

*Caveat, I'm scared of governments and corporations using existing AI/ML approaches to produce digital panopticons and whatnot, but that's not really the current topic of discussion. Xi Jinping with an army of ED-209s is far scarier to me than Skynet.",14,
285,comment,MachineLearning,jcdfy6d,1678928485.0,"Is there any legal precedence to argue that training on copyrighted data actually violates that copyright?

I genuinely don't know.

If I was OpenAI, in this hypothetical law suit, I would make the argument that they're not actually selling the copyrighted data. They're doing something akin to taking a book, reading it, and acquiring the knowledge in it, then applying it. So it would be akin to saying you cant read a textbook on how to build a thing and then sell the thing you build. (Don't misunderstand, I'm not saying that that's what an LLM actually does, but that's what I would say to defend the practice)",34,
286,comment,MachineLearning,jcgdwah,1678986900.0,">This is the classic question of whether or not it makes sense to encourage the collective SOTA to move forward.

This might apply in traditional industries but this does not apply with respect to AI, where the first mover advantage is theoretically infinite, and these distinctions between selling a technology vs using it no longer make sense. In this sense there is no ""much larger business"" which AI is embedded in, and from a purely profit-driven business perspective, the only thing that makes sense is to build an AGI privately and capitalise on the disproportionately large productivity that it produces for marginal cost. 

This has not been the strategy adopted during the run-up to AGI for a number of reasons, including the fact that researchers generally refuse to work in places where they can not publish publicly. However, now that the theoretical framework for enabling AGI has (potentially) been found this is no longer the case and research (i.e engineering) can continue in private. As you said, it is simply a matter of scale, scale, scale. 

It is incredibly unlikely that any company with the resources to obtain this first-mover advantage (including Facebook, Google/Deepmind, and others) would forego it for the sake of reducing costs in the short term for some other ostensibly ""bigger business"". There simply is, no bigger business. This is very obvious. 

&#x200B;

>I'm not sure exactly what you're asking here?  They want as many people as possible to participate in pushing ""public"" SOTA.

Again, same as above, this may have been true previously, but there is 0 chance this will continue to be true in the near future. 

&#x200B;

>The publicly available models are all either atrocious, or clear   
attempted clones of what OpenAI already built.  And yet they are still (other than maybe llama) behind the GPT-3 power curve.  
>  
>This is a false trade.  I'm not talking about ""open ai open sources or   
doesn't"", I'm talking about ""does OpenAI \[and its ilk\] even exist""?

There is absolutely zero reason, or prior precedent, for believing that in the presence of open access release of AI research, market forces would not have developed into highly commercially viable tools like ChatGPT, and many others, without OpenAI. The reason why they didn't was that most of OpenAI's research has been closed access, and this includes their pre-training datasets,  the RLHF fine-tuning datasets, as well as details about methodology necessary to progress collaboratively. 

Again, **this is fine,** it is after-all a matter of a smart profit-driven business sense to commercialise your IP by rendering it private, my argument is that this **is not something we need necessarily be grateful as the alternative (Open access, reproducible research) would have been far more beneficial to consumers.** 

The proliferation of companies that sprung from (open-access) Stable Diffusion research and the quality of commercialization from open access research is a clear indicator that competitive industries based on open access research would have arisen, and that market forces would have driven prices down to the benefit of the average consumer. There is nothing special about OpenAI that warrants gratitude as they are, like any profit-driven company, looking to monopolise on the first-mover advantage wrt AI. And the choices they have made have almost certainly meant the average consumer (if such a thing will even exist in the future) is necessarily going to be worse off than they would have been.",1,
287,comment,MachineLearning,jcdzevt,1678937357.0,"About AI, there seems to be some massive intuition gap between those who believe current systems aren't intelligent at all, so we didn't even scratch surface of the problem - and those who believe things like best LLMs are not AGIs, but not as narrow as, IDK, AlphaGo. And kinda intelligent. I don't know how to even start arguing.

I don't think there's any sharp divide between narrow AI and AGI.


About climate change

>  in order to try and convince someone to get off their asses and actually do something

Well, nobody will be convinced. But that doesn't mean apocalypse is inevitable. Problem doesn't necessarily _require_ any grand political action. It would be helpful to reduce harm. But technological progress seems on track to solve the problem. Things like steadily dropping cost of renewables.

I'll quote Matt Yglesias, because he recently wrote [relevant text](https://www.slowboring.com/p/misinformation-isnt-just-on-the-right) (partially) about it. TL;DR online discourse about it is pretty bad. If one is surrounded by 'everyone' either being maximally doomerist, or denialist, and denialism is obviously unhinged while doomerism, not quite - because there's no push back against it....

---
(quote lacks links tho)

> I think the most salient example of this is climate change, where you not only have rightists spreading insane conspiracy theories (Trump used to say it was a Chinese hoax), but you also have a lot of very influential wrongheaded ideas on the left.
> 
> Perhaps the most prominent version of this is the idea that the world faces a hard tipping point to climate apocalypse sometime around 2030. This is routinely debunked (here‚Äôs Scientific American) but keeps popping up. As is often the case with misinformation, the problem arises in large part because elite communicators say things that are a little confusing or misleading. This NPR headline ‚ÄúEarth has 11 years to cut emissions to avoid dire climate scenarios, a report says‚Äù sounds superficially similar to AOC‚Äôs ‚ÄúThe world is gonna end in 12 years if we don‚Äôt address climate change.""

> **But the report is not measuring humanity‚Äôs time to avert human extinction ‚Äî it‚Äôs measuring humanity‚Äôs time to avert the 1.5 degrees of warming** adopted as a global target in the 2015 Paris Climate Agreement. **We will almost certainly end up with more warming than that, and this will lead to, among other things, irreversible harm to low-lying island nations. But it‚Äôs not the actual end of the world.** And even more importantly, there are no magic tipping points.
> 
> The misperception about this really is broadly influential, though.
> 
> ‚ÄúDon‚Äôt Look Up‚Äù was marketed as a climate change allegory, and **it‚Äôs very explicitly about a genuine extinction-level threat with a specific near-term tipping point. If you conceptualize climate change as having those features, then the behavior of major world governments with regard to climate seems bizarre and borderline insane. That then encourages a lot of performative radicalism, inattention to cost-benefit analysis, and conspiratorial thinking** about why elected officials won‚Äôt do what you want them to do. We have pretty good evidence that a non-trivial number of young people are experiencing meaningful **psychological distress based on the misperception that they are going to grow up to live in a post-apocalyptic hellscape.** Many, many people don‚Äôt realize that accounts of RCP 8.5 climate scenarios are not intended to represent ‚Äúbusiness as usual,‚Äù and that the world has been moving steadily away from **this worst-case scenario outcome for some time.**
> 
> I also think many people don‚Äôt realize that natural disaster deaths have become much rarer over time because for most people, the benefits of living in a richer world with better technology far outweigh the hazards of living in a warmer world.

[Plot: Average annual number of deaths from natural disasters, by decade](https://substackcdn.com/image/fetch/w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7dd7d9b3-2b5c-466c-8b5b-75ee425b9602_1524x1190.png) (it was many times higher in the early XX century than now)

> Again, none of this is to deny the scientific facts of climate change: greenhouse gas emissions contribute to global climate shifts that are on net harmful, and it‚Äôs important that we take further action to reduce those emissions. But **it‚Äôs actually a substantially different situation than the one a lot of progressives seem to think that we are in.**

> Meanwhile, in addition to overstating the most likely consequences of the status quo, it‚Äôs common to hear grossly exaggerated accounts of the ease of getting to net zero with current technology. That‚Äôs often paired with undervaluing energy in general, with **the overall result that the climate left is less enthusiastic about actually deploying zero-carbon energy than it should be and also more hostile to fossil fuel use than it should be**. These are errors that have had meaningful policy and political impacts, but that get totally ignored in a misinformation discourse that locates misinformation exclusively on the right.


Also, [Please Don't Give Up On Having Kids Because Of Climate Change](https://astralcodexten.substack.com/p/please-dont-give-up-on-having-kids) 

> **The current scientific consensus**, as per leading scientific organizations like the United Nations‚Äô Intergovernmental Panel on Climate Change, is that **climate change will be very bad, but not world-endingly bad.**
> 
> Climate change will cause worse hurricanes, fires, and other disasters. It will lead to increased spread of invasive species and diseases. It will hit subsistence farmers in poor agricultural countries very hard, and some of them will starve or become refugees. But **it won‚Äôt cause the collapse of civilization. It won‚Äôt kill everyone. Life in the First World will continue, with worse weather and maybe a weaker economy, but more or less the same as always. The people who say otherwise are going against the majority of climatologists, climate models, and international bodies.**
> 
> One way to think of this is to notice that **we‚Äôve already gotten about 25-30% of the global warming we‚Äôre likely to see by 2100.**

> (...) The next 75 years of global warming are going to be worse than we‚Äôve gotten already, **maybe millions of lives lost and tens of trillions of dollars in damage. In aggregate, they‚Äôre going to be a giant disaster.** But the average person in the First World, probably including your child, still won‚Äôt notice much of a change to their daily lives. 

> One way of looking at this is to think of **how much slack there is in First World systems.** For example, California has had lots of droughts recently, and probably this is related to climate change. Probably those droughts will get worse in the future, and **it‚Äôs easy to imagine parched Californians begging for water.**

> **But so far, the droughts haven‚Äôt been bad enough that California stopped golf courses from watering their massive lawns to keep them perfectly green every day. But if anyone was actually dying of thirst, or even having enough trouble getting water that they might be motivated to vote out some politician over it, the government could redirect the golf course water, or any of a thousand other things like this, and everyone would have more than enough.**",-2,
288,comment,MachineLearning,jew81y8,1680596710.0,"I think it also depends a lot on how you define AGI.  If you showed ChatGPT to anyone in 1975 this would 100% be considered AGI for all intents and purposes.  In terms of naturalness and general ability to answer a truly vast array of questions, it‚Äôs honestly more intelligent than most of humanity already.  All of humanity if we refer to the breadth of its knowledge.  Of course it‚Äôs still bad at ‚Äúnon LM‚Äù tasks like math.  But so are most people.  And that will be fixed within 2 yrs I‚Äôd guess.  It doesn‚Äôt have agency yet; but people are already hacking that on as well.  There‚Äôs lots of work in embodiment too.  

Is it todays AGI target? No I guess not.  But that target is endlessly moving.  Is it good enough to disrupt modern society in a significant way?  I think yes it is.",4,
289,comment,MachineLearning,jco1pv8,1679120286.0,"The open source community is going to be dominated by people who are deeply interested in the material ais can produce, looking for personalized content Chatgpt refuses to provide even at the cost of quality.

You may have noticed I just described porn.

I expect the biggest open source advancements to come from the unstable diffusion guys trying to train their ais to make better and more personalized fetish material.",5,
290,comment,MachineLearning,jcdztj2,1678937568.0,Then you completely missed my point. I‚Äôm not interested in seeing a list ofopen source softwares. What I was asking was clearly what percentage / how important are open source softwares. If you just list a bunch of names how would I know that? Do you mean the ‚Äúchatgpt‚Äù of other kind of software are open sourced? Or are you just saying there are just few open sourced software that‚Äôs kinda popular but the majority of most popular ones are not open source?,-4,
291,comment,MachineLearning,jcfixg4,1678974819.0,"https://openai.com/research/gpt-4 :

>Over the past two years, we rebuilt our entire deep learning stack and, together with Azure, co-designed a supercomputer from the ground up for our workload. A year ago, we trained GPT-3.5 as a first ‚Äútest run‚Äù of the system. We found and fixed some bugs and improved our theoretical foundations. **As a result, our GPT-4 training run was (for us at least!) unprecedentedly stable, becoming our first large model whose training performance we were able to accurately predict ahead of time.** As we continue to focus on reliable scaling, we aim to hone our methodology to help us predict and prepare for future capabilities increasingly far in advance

https://openai.com/product/gpt-4 :

>We incorporated more human feedback, including feedback submitted by ChatGPT users, to improve GPT-4‚Äôs behavior. We also worked with over 50 experts for early feedback in domains including AI safety and security.

>We‚Äôve applied lessons from real-world use of our previous models into GPT-4‚Äôs safety research and monitoring system. Like ChatGPT, we‚Äôll be updating and improving GPT-4 at a regular cadence as more people use it.

>**We used GPT-4 to help create training data for model fine-tuning and iterate on classifiers across training, evaluations, and monitoring.**

Not to mention the 32k context window, which nobody else has yet.",10,
292,comment,MachineLearning,jcgb4r0,1678985867.0,"Wow, I'm surprised you got real answers to those questions instead of

>I'm sorry, as an LLM I am not authorized to provide legal advice. 

>I'm sorry, as an LLM I am not authorized to parody copyrighted material.

>I'm sorry, as an LLM I am not authorized to devise a potentially dangerous chemical process.",39,
293,comment,MachineLearning,jchbwqz,1678999822.0,"It's still just a LLM after all, far from being AGI. Purely a combination of probabilities and some hard-coded rules. It has no underlying notion or understanding of anything it outputs.",2,
294,comment,MachineLearning,jcefsb7,1678947848.0,"In general, no. 

If you're a Fortune 500 or a government and you want to sue someone random, sure.

Otherwise, if you don't have a long and established relationship, credible (the qualifier I originally used, very much on purpose) firms are generally going to be disinclined to pick up cases that they believe are very likely to be losers.  Big corporate firms are also going to be disinclined to take a major case that will likely solidly conflict them out from very lucrative AI-related IP law with the biggest players.  Suing OpenAI or Google on generative AI is really not where you want to start right now.",3,
295,comment,MachineLearning,jcgcn7p,1678986438.0,"In NLP in the last 2 years in particular, RLHF and Chinchilla scaling laws are things that come to mind. If OpenAI hadn't ever talked about RLHF (and it hadn't leaked either) and we had just been presented with a ChatGPT API, the game would be different now",2,
296,comment,MachineLearning,jcdq6zj,1678932998.0,The research that led to transformers and LLMs was big tech,3,
297,comment,MachineLearning,jce3j6e,1678939569.0,"No I think you just didn‚Äôt read this post? I‚Äôm clearly discussing wether chatgpt being open source is important since that‚Äôs what the post is discussing. And my point is if most other important softwares weren‚Äôt open source then chatgpt being closed source is probably fine. If you just list a bunch of open source project it has nothing to do with my post. You shouldn‚Äôt just quote a line without considering it‚Äôs context. I mean even chatgpt can understand context you are clearly better than that.
My guess was you want to show me open source software are important, but that could either mean ‚Äúwe need more open source project‚Äù or ‚Äúeven if big companies aren‚Äôt making the most important software open source we can still have a vibrant open source community anyway‚Äù How could I know your point if you don‚Äôt say anything?",-10,
298,comment,MachineLearning,jcjg5sv,1679038725.0,"Thank you very much! I'll read the article and the stuff linked in it as soon as I can. I'm (so far at least) of the opinion that the way AIs work it doesn't seem like a copyright infringement. The thing is, [we've seen Copilot explicitly doing a copyright infringement](https://nitter.lacontrevoie.fr/mitsuhiko/status/1410890346259697669#m), and I think generally there is consensus that you can't ask a generative AI to produce images of Mickey Mouse, and have an easy day in court.

But yeah, the mining, with the current law, I can't see how it's illegal.",3,
299,comment,MachineLearning,jchp27x,1679005055.0,"You mean the same benchmark which contains ~40% errors (https://www.surgehq.ai/blog/hellaswag-or-hellabad-36-of-this-popular-llm-benchmark-contains-errors)??
Anyhow a single test cannot prove intelligence/reasoning, which it's very difficult to even define, it's absurd. Also the out of context 'reasoning' of an opinionated & 'neurologically challenged' gpt is already being discussed casually in twitter and other outlets. 
It is very much feasible to get better scores than a human in a controlled environment. Machine learning has been sprouting these kind of models since decades. I was there when SVM's started classifying iris petals better than me and when kernel methods impressed everyone on non linear problems. This is the power of statistical modelling, not some magic intelligence arising by poorly constructed hessian matrices ..",17,
300,comment,MachineLearning,izo734l,1670688915.0,Is this just rewording the TypeError's str description? What is the information context for the ChatGPT?,17,
301,comment,MachineLearning,iznsdsj,1670682140.0,"That's such an unnecessarily wordy explanation. The error message literally explained it to you concisely. 

If it produces such an unnecessary output for such a simple error message god help you when it is more complicated.

Further more, ChatGPT cannot do deductive reasoning. It can only take existing chains of thought from its training set and swap out the keywords consistently to apply that same logic template to something else which may or may not fit to it correctly.  

This is a bad idea. And if I'm perfectly honest, a waste of electricity. Save the planet and don't push this as a legitimate usage.",1,
302,comment,MachineLearning,izo7y3j,1670689291.0,ChatGPT is legit. Was playing with it all yesterday,1,
303,comment,MachineLearning,iznz4kb,1670685439.0,How do you call ChatGPT? Afaik there's no API?,0,
304,comment,MachineLearning,izrzyar,1670761383.0,"Nice!
I found this on hacker news: you can add an import in your python file and it generates explanations for exceptions:

Show HN: A Python package to get help from ChatGPT when an exception is thrown https://news.ycombinator.com/item?id=33911095",1,
305,comment,MachineLearning,izo58tx,1670688115.0,"There is GitHub CoPilot available as a VS Code extension, it uses OpenAI‚Äôs Codex model, I assume ChatGPT is accessing Codex under the hood when it receives a programming related inquiry, but I could totally be wrong.

As a side note, Copilot seems like a bit of a privacy concern so I would personally be wary of using it with any private or commercial projects.",59,
306,comment,MachineLearning,izo7j6o,1670689112.0,"For the fun of it, I asked ChatGPT to generate the solution for creating the output above, and it didn't get it 100% but it's a pretty good start! One of my favorite things is how it implicitly got that it should do `sorted(line_numbers)` because `set` does not have a defined iteration order, or how it should make the line numbers right-aligned.

---

Here is some sample code that demonstrates how you can write a Python program that turns a raised exception into the desired output. This code is not complete, but it should give you a good starting point:

```
import traceback

def turn_exception_into_output():
  try:
    # Code that may raise an exception goes here
    pass
  except Exception as e:
    # Aggregate the lines of code surrounding the trace in a map
    lines = {}

    # Get the traceback object
    tb = traceback.extract_tb(e.__traceback__)

    # Loop through the traceback object
    for t in tb:
      file_name, line_number, _, _ = t

      # Initialize the set of line numbers for this file
      if file_name not in lines:
        lines[file_name] = set()

      # Update the set of line numbers with the range of relevant lines
      lines[file_name].update(range(line_number - 5, line_number + 5))

    # Print the error message
    print(""Please explain concisely why this error in my Python code happened:"")

    # Print the traceback
    print(""\n"".join(traceback.format_exception(e.__class__, e, e.__traceback__)))

    # Print the relevant lines of code
    print(""\nBased on these lines of code surrounding the trace:"")
    for file_name, line_numbers in lines.items():
      print(file_name)
      for line_number in sorted(line_numbers):
        print(""{:>3} ..."".format(line_number))
```

To use this code, you would call the `turn_exception_into_output` function whenever you want to handle an exception and print the desired output. For example:

```
try:
  # Code that may raise an exception goes here
  pass
except:
  turn_exception_into_output()
```

You could also use this code as part of a custom exception handler. For example:

```
import sys

def custom_exception_handler(exception_type, exception, traceback):
  turn_exception_into_output()

# Set the custom exception handler
sys.excepthook = custom_exception_handler
```

With this code, whenever an unhandled exception is raised, the `custom_exception_handler` function will be called, which will turn the exception into the desired output.",13,
307,comment,MachineLearning,izp9ejs,1670704735.0,"If you add ""be as concise as possible"" it cuts out a lot of the noise. But that is annoying to add everytime. But you can say thanks to the great retention ""for all following answers be as concise as possible"". 
All we need now is a a .chatgptrc file to add all the ""global"" prompts we want lol",21,
308,comment,MachineLearning,izpzi7m,1670716148.0,https://github.com/acheong08/ChatGPT,2,
309,comment,MachineLearning,iznkxx5,1670678029.0,"I've actually had it explain an obscure warning, faster than googling it and already tells you what to do to get rid of the warning.

I've also found ChatGPT super useful for mudane stuff too, create a regex for a certain pattern giving it just a description and one example, create a flask API end point with a description of what it does etc. Code often works out of the box, sometimes needs minor tweeks. But its much easier to correct a regex with one minor issue than writing it from scratch.",11,
310,comment,MachineLearning,izo1bmq,1670686449.0,They scrape the website with ChatGPT.,0,
311,comment,MachineLearning,izoejlp,1670692022.0,Not sure if it works but people have found ways to [use ChatGPT through python](https://github.com/acheong08/ChatGPT/wiki),1,
312,comment,MachineLearning,izp8la9,1670704408.0,"No. The whole chatgpt/gpt-3.5 model builds on code-davinci-002 (which is maybe the one tuned for copilot, but I don't think this has been said publicly).

So amy prompt to chatgpt is  a prompt to a differently fine-tuned version of copilot (or copilot-like).",28,
313,comment,MachineLearning,izqmn2n,1670727673.0,"Copilot is a 12B model (for inference speed), chatGPT is the 175B one, not specifically trained on code I'm pretty sure. So chatGPT should give better results on average because of the better model.",9,
314,comment,MachineLearning,iznu5ry,1670683025.0,"If people were constantly crunching an LLM every time they got a stack trace and this was a normal development practice despite it being largely unnecessary. 

Then given it is all complete avoidable, would it not be a waste of energy?

> It's a tool like any other, you're using a computer too to avoid doing basic tasks by hand.

That's a nonstarter. There are plenty of tasks more efficiently performed by computers. Reading an already very simple stack trace is not one of them.",-7,
315,comment,MachineLearning,izoe71e,1670691886.0,My comment attempting to have a civil discussion about sustainability of LLMs in production applications compared to yours intended only to be derisive and petty?,-1,
316,comment,MachineLearning,izoujfv,1670698524.0,Yeah. A lot of times I get a better answer from chatgpt but you really need to take its responses witha grain of salt,8,
317,comment,MachineLearning,iznvys0,1670683918.0,"Okay. But if you didn't do this you would not need to crunch a high end GPU for a couple of seconds. And if many people were doing this as part of their normal development practices then that would be many high end GPUs crunching for a considerable amount of time.

At what scale does the combined environmental impact become concerning?

It is literally a lot more energy consumed than is consumed by interpreting the error yourself, or by Googling and then accessing a doc page or stackoverflow thread. And it is energy that gets consumed every time anyone gets that error, regardless of whether an explanation for it has been generated for someone else already.

> Ever played a video game? You probably wasted 1000x as much energy in just one hour.

In terms of what value you get out of the hardware for the energy you put into it, the game is considerably more efficient than an LLM.

> The real advantage is that this can really speed up your programming and it can program small functions all by itself. It is much better than stackoverflow.

If an otherwise healthy person insists on walking with crutches all day every day. Will they be as strong as someone who just walks?",-7,
318,comment,MachineLearning,izo1b23,1670686441.0,"If you run a Google search, Google will also run a LLM on your query.",9,
319,comment,MachineLearning,j8pgy3u,1676507486.0,There is a similar browser plugin that uses ChatGPT to summarize YouTube video highlightsÔºöhttps://addons.mozilla.org/en-US/firefox/addon/glarity-youtube-summary/,1,
320,comment,MachineLearning,jl3t0uk,1684720958.0,"dumb question probably, but can chatGPT analyze sound in video ? how does it make summaries",1,
321,comment,MachineLearning,j7zm8el,1676042698.0,"A few months ago they've made some of those models available using the api, there is a massive difference in their ability to follow instructions. They're planning to add ChatGPT to the api as well, but for now I'm using ""instruct curie"" to make api calls cheaper",22,
322,comment,MachineLearning,j81njbt,1676071810.0,"I used ChatGPT to rewrite your post into a more clickbait version:
""Revolutionary AI Tool Unveiled: Get Real Video Summaries and Say Goodbye to Clickbait Forever!""",48,
323,comment,MachineLearning,j80m68j,1676056735.0,"How about Edge, considering that MS is about to add chatgpt into it as well?",5,
324,comment,MachineLearning,j83tabl,1676120247.0,What's the difference between those and chatgpt?,1,
325,comment,MachineLearning,j82zeke,1676096513.0,Haha i thought chatgpt was refusing to write clickbait titles,6,
326,comment,MachineLearning,j8cfqix,1676272140.0,"That‚Äôs an excellent question. In their blogpost, OpenAI calls ChatGPT a ‚Äúsister‚Äù model to InstructGPT, but that‚Äôs it. There is no paper, and the only info we have from other public communication is that it‚Äôs a 175B variant, based on GPT3.5, so pre-trained with more text and code, and pretty certainly with much more Instruct-like mode fine-tuning and censor models training.",3,
327,comment,MachineLearning,dew9vyt,1489454555.0,"... also read Murphy's Machine Learning, Russell's Artificial Intelligence, Sutton's Reinforcement Learning ",15,
328,comment,MachineLearning,jncz4z8,1686203112.0,Literally the entire reason for Reddit doing this is to put a prohibitive price on the training of LLMs. What sub is this again?,1,
329,comment,MachineLearning,jn7seme,1686112638.0,"Why do we need a third-party app to use Reddit when the official Reddit app has improved a lot in the last couple of years? As a machine learning-related subreddit, we should consider protesting against ChatGPT in some form. It seems unfair that they have access to a vast amount of data to train their models, while not allowing others to use even the responses through a paid API for training their own models. I believe what Reddit is doing is crucial for its survival.",0,
330,comment,MachineLearning,jn6a13q,1686087145.0,"Given that one of the key drivers of the pricing change is LLMs looking for conversational data, I'd say this subreddit has more reason than most to protest it. This is basically reddit deciding that they want their piece of the ""only corporations are allowed to compete in AI development"" pie, and imo it's exactly as disgusting as what OpenAI has been up to lately. Stepping out on the blackout is tantamount to endorsing the end of open source ML development being competitive with closed source.",1,
331,comment,MachineLearning,jnrhhqk,1686478009.0,"Well, unless it is a boycott until reddit revert its decision, I don't see a 2 days blackout will have any effect other than helping them save some electricity bill. To be fair, as a software developer, I understand why want user to you their own app. 3rd party app is just ripping their resources and I wander if the situation is getting worse because there are many people separately trying to pull all the contents from reddit for LLM training.",1,
332,comment,MachineLearning,f3o3n0u,1571007581.0,I think Lex Fridman removed his interview with Siraj from the Artificial Intelligence podcast.,1,
333,comment,MachineLearning,f3i3f66,1570931787.0,The entire document has the hallmarks of a shitty Word document. At the very least the figures seem to be certainly made in MS Word.,2,
334,comment,MachineLearning,ji8s67w,1682811640.0,"While i agree with the spirit of what you‚Äôre saying, and upvoted you, i don‚Äôt think we‚Äôre going to be at iRobot levels anytime soon. Sonny was a proper general ai, and VIKI is a straight up super ai. I could see the first general AI emerging from LLM research in the next two decades, but not a super ai. Though who knows what will be possible when we can just through unlimited processing at any problem when the first general AI come along. The biggest limitations will definitely be energy and processing hardware. It‚Äôs not feasible to run 64 Hopper 100s all day every day, which I‚Äôm guessing will be comparable to the minimum ram for even inference with a general AI. Graphcore IPUs show a lot of promise there too. 

Exciting times.",5,
335,comment,MachineLearning,ji8c29y,1682803900.0,"I think exponential improvements are shocking to brains fine tuned on linear gains. I interacted with early version of GPT and didn't expect to see anything close to ChatGPT until maybe 2029 or later. And I was already aware of the scaling laws -- being aware of something logically is different from how things feel experientially. 

As we encounter more and more exponential improvements we may be less shocked.",9,
336,comment,MachineLearning,jis6ll9,1683167388.0,"It wasn't at all obvious that exponential compute would imply the capabilities we see now in LLMs.

If you were evaluating GPT2 (even GPT3) and had exact knowledge of future advances in compute, on what basis would you predict the qualitative capabilities we see from GPT4?",1,
337,comment,MachineLearning,jiahfed,1682851644.0,"Exactly, RLHF is all over the LLMs, not sure what OP is getting at.",5,
338,comment,MachineLearning,gjnd1fr,1610917747.0,"Just as a little insight:

 

>You've been permanently banned from participating in r/Art  
  
>  
>You have been permanently banned from participating in [r/Art](https://www.reddit.com/r/Art). You can still view and subscribe to [r/Art](https://www.reddit.com/r/Art), but you won't be able to post or comment.  
>  
>Note from the moderators:  
>  
>[context](https://www.reddit.com/r/Art/comments/kyw8bt/-/gjklllm/?context=9) **/** [sub rules](http://www.reddit.com/r/Art/about/rules) **/** [sidebar](http://www.reddit.com/r/Art/about/sidebar) **/** [site rules](http://www.reddit.com/rules) **/** [cat](http://i.imgur.com/Gbx2Vts.gifv)  
 This comment may have fully or partially contributed to your ban:  
    
  lack of understanding + narrowmindedness   
   
\-------------------------------------------------------------------------------------------------------------    
>  
>Hi Just to get this straight - was I banned for critiziting your mod? Who  probably removed the comment I replied to, which I assume was done to  avoid having his/her decision questioned. Certain comment asked why rule  6, which refers to low effort work, was broken to which I replied with  the cititation above. Is questioning and critisism of authority a  bannable offense in your sub? Sincerly me  
>  
>\[‚Äì\]from [VerditerBlue](https://www.reddit.com/user/VerditerBlue)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 2 hours ago  
>  
>Your comment breaks rule 8.  
>  
>\[‚Äì\]to [VerditerBlue](https://www.reddit.com/user/VerditerBlue)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent an hour ago  
>  
>Can I have someone who is not the critizited party review this?  
>  
>\[‚Äì\]from [VerditerBlue](https://www.reddit.com/user/VerditerBlue)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent an hour ago  
>  
>Sure, you still broke rule 8 though, and you're going to remain banned anyway.  
>  
>\[‚Äì\]from [awkwardtheturtle](https://www.reddit.com/user/awkwardtheturtle)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) to [VerditerBlue](https://www.reddit.com/user/VerditerBlue)\[[M](https://www.reddit.com/r/Art/about/moderators)\] sent an hour ago  
>  
>\^  
>  
>\[‚Äì\]to [VerditerBlue](https://www.reddit.com/user/VerditerBlue)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 59 minutes ago  
>  
>Could  you kindly explain to me how I broke this rule. Is it bigotry? A  slapfight? Unconstructive criticism? Off topic? Or not respectful in any  way? I guess it heavily conflicts the picture you have of yourself, but that  initself is imho not disrespectful, it is mere critique, clad in harsh  words alas, but still not disrepectfull in a sense that should affect  anyone in charge. Lastly I am well aware that you are not inclined to reply to my  questions, but I would appreciate it.  
>  
>\[‚Äì\]from [awkwardtheturtle](https://www.reddit.com/user/awkwardtheturtle)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 41 minutes ago  
>  
>Calling us narrowminded for removing meme spam fan art, no matter how aesthetic the art may be, is highly disrespectful  
>  
>\[‚Äì\]to [awkwardtheturtle](https://www.reddit.com/user/awkwardtheturtle)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 32 minutes ago  
>  
>I called someone narrow minded for stating the work was low effort!  
>  
>\[‚Äì\]from [awkwardtheturtle](https://www.reddit.com/user/awkwardtheturtle)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 19 minutes ago  
>  
>Rule 6 is for memes AND/OR low effort work  
>  
>Reading comprehension is hard, I know  
>  
>\[‚Äì\]from [VerditerBlue](https://www.reddit.com/user/VerditerBlue)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 12 minutes ago  
>  
>Let's not pretend you didn't intend this as an insult.  
>  
>\[‚Äì\]to [awkwardtheturtle](https://www.reddit.com/user/awkwardtheturtle)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 7 minutes ago  
>  
>If that is a meme to you we both have different understandings of the word meme. As for the insult, not really surprising.  
>  
>\[‚Äì\]from [awkwardtheturtle](https://www.reddit.com/user/awkwardtheturtle)\[[M](https://www.reddit.com/r/Art/about/moderators)\] via [/r/Art](https://www.reddit.com/r/Art) sent 4 minutes ago  
>  
>Ita  very fucking obviously fan art in violation of rule 7 so youre  literally arguing with us and insulting us for your own arrogant  reasons, take care  
>  
>\[‚Äì\]subreddit message via [/r/Art](https://www.reddit.com/r/Art)\[[M](https://www.reddit.com/r/Art/about/moderators)\] sent 4 minutes ago  
>  
>You have been [temporarily muted](https://www.reddithelp.com/en/categories/reddit-101/moderators/modmail-muting) from [r/Art](https://www.reddit.com/r/Art). You will not be able to message the moderators of [r/Art](https://www.reddit.com/r/Art) for 3 days.

I might wanna add: It was initially based on the pretense of breaking rule 6 - low quality/memes.",7,
339,comment,MachineLearning,j7c7siq,1675622625.0,"I'll bite. If you work in this field and are seeing the potential this has, what type of jobs/careers/skills do you think will be valuable as this evolves? The biggest threat people say AI poses is the elimination of human jobs. Even highly skilled and paying coding and programming jobs are potentially at risk by generative ai. What's a path that could pay better because of AI in your estimation?",1,
340,comment,MachineLearning,j7ctjjk,1675631422.0,"I don‚Äôt think it will simply ‚Äúeliminate‚Äù jobs.  But I do think there is going to be a sea change in job descriptions.  I think the most disruptive area will be traditional professional jobs like lawyers and doctors.  My kid is 6 and I think if he watches House reruns in his twenties he‚Äôll find them bizarre.  The idea of a human savant able to outdo an AI will be laughable.

I think there will still _probably_ be humans tuning the core models.  Probably.  The rest depends on us.  I think there will be an explosion of job descriptions related to prompt tuning for chatgpt technologies.  Plenty of jobs for fine tuning the models to particular domains.  

People will still remain in call center jobs, but it will focus more on analysts and not auditors.

Beyond that I think it‚Äôs hard to say.  How will it affect other areas like biology, pharmaceuticals,even physics?",2,
341,comment,MachineLearning,itg626o,1666527352.0,At some point someone is going to do something similar for motion generation paired with a LLM and some few-shot fine tuning and sensor feedback and we'll finally have robots that can obey arbitrary natural language commands. Then come the layoffs. :),3,
342,comment,MachineLearning,itr3axj,1666721154.0,The difference is that Ps needs an image to fake an image and you can always tell when a photograph has been heavily altered. These new AI tools change all of that.,2,
343,comment,MachineLearning,g3ilypc,1598908256.0,"Do not think of this as ""artificial intelligence"". Rather is ""glorified interpolation"", smooth diffeomorphisms upon temporal sequences of points in a manifold representing familiar animate shapes. There is no real imagining, consciousness or thinking taking place here, just calculations by an intelligent programmer, who did all of the reasoning while coding.",1,
344,comment,MachineLearning,jeopnyu,1680458300.0,"Try it out here: [https://useadrenaline.com/app](https://useadrenaline.com/app)

I built this because I can't plug entire repositories into ChatGPT. So I used a combination of static code analysis, vector search, and the ChatGPT API to build something that can answer questions about any Github repository. It's super early phase though, so I'd love to hear feedback on how usable it is. What kind of questions does it answer best or worst? Please let me know what you think!",210,
345,comment,MachineLearning,jeox0pa,1680461368.0,"Honest to god question, because I finally relented and thought, maybe there's some value to be extracted from a system like ChatGPT by asking it to scour data...

How do you trust that it's not lying through its teeth, either by omission or by injecting spurious details?

How can you trust anything it says?",92,
346,comment,MachineLearning,jepg5rl,1680469466.0,Can someone please tell me how to fine-tune LLM or llama i want fine-tune Cerebras 111m on alpaca dataset i didn't find anything on the internet please help,0,
347,comment,MachineLearning,jep7nar,1680465877.0,How did you implement the toolformer aspect of your code? Is it a chatGPT plugin?,1,
348,comment,MachineLearning,jeqz9m0,1680495629.0,"Thats awesome! A very common use case, that you can not feed a whole homepage into ChatGPT, I will look into your implementation. Thanks.",1,
349,comment,MachineLearning,jesdwtd,1680531262.0,"This is a nice first step towards an AI assistant for coding. I think the way some people claim that we will write apps just by telling ChatGPT what to do is way overblown. But I think we will be able to read code more easily and perhaps even write/refactor code using LLMs.   


More of my thoughts in this direction: https://rolisz.com/chatgpt-and-the-future-of-coding/",1,
350,comment,MachineLearning,jfednn7,1680921106.0,"I have attempted something similar: converting the codebase to PDF, then breaking it into chunks for vector searching. This method simply provides context and instructs ChatGPT to work within that context. However, if the code is too lengthy, this approach remains ineffective. Moreover, the bot tends to generate fake code, which is not helpful.  
Using this method saves time when inputting code snippets to the bot, but the downside is that you cannot be certain if the bot is actually receiving the desired code snippet.",1,
351,comment,MachineLearning,jeqjsmq,1680487608.0,"I think some people are misunderstanding what's happening here. These semantic search tools scrape content, create embeddings, and then they compare your query to the embedding database: which pulls the most similar excerpts from the repository to be used as a context, which you instruct ChatGPT to use for it's answer.. I.e. you are telling ChatGPT not to make things up, and to only use the content that you give it, which makes it far more trustworthy than just scouring data from the actual GPT model.  It's quite effective.",9,
352,comment,MachineLearning,jeqs1fk,1680491691.0,"You ask clarifying questions and probe things that seem untrue, the same way you would when talking to a person. In my experience, as someone who has used chatGPT since it came out for work and has been using the GPT-4 version since it's been available, it generally makes stuff up when there is a lack of context, not when there is a lot of context, like in this example where it has the entire source of the repo. Also, GPT-4 seems to hallucinate a lot less then GPT-3. The biggest problems I have with it is when it gets confused about acronyms and academic papers",1,
353,comment,MachineLearning,jeslddt,1680534460.0,Check CodeT or dual execution agreement https://arxiv.org/abs/2207.10397 so... just trust itself or other LLMs to generate proper tests?,1,
354,comment,MachineLearning,jeposx2,1680473244.0,"Whenever people say things like ""ChatGPT is dangerous because it is not 100% correct all the time,"" I think that says more about their opinion on human intelligence than artificial intelligence.  It's saying that humans are so stupid that we need it to be significantly more intelligent than a human just to protect people from themselves.",19,
355,comment,MachineLearning,jep6vx9,1680465559.0,"atleast junior developers know they don't know everything

chatgpt will happily tell you the world sits on a pin in the eye of a camel and provide madeup sources confirming that ""fact""",16,
356,comment,MachineLearning,jepb97s,1680467373.0,"> But for even things like ""writing help manuals"" (forget anything like business specs), they are outright dangerous.

I dunno man, it‚Äôs in the same league as ‚ÄúDave the junior developer‚Äù in that respect: I have to eyeball everything that goes out. And let me tell you, chatGPT writes a LOT better than Dave.",37,
357,comment,MachineLearning,jeqqwo3,1680491077.0,">But for even things like ""writing help manuals"" (forget anything like business specs), they are outright dangerous.

What's dangerous is the person who tasks an LLM with producing an output that, when not aligned, produces damage.

That person is a fool, and was likely already a danger to those around them.",5,
358,comment,MachineLearning,jeprz3y,1680474718.0,to be honest i have seen quite a few people assuming chatgpt is 100% correct all the time so that pessimistic view might not be far off. hopefully that‚Äôs just natural unfamiliarity with new technology and will change soon though,13,
359,comment,MachineLearning,jepn5qq,1680472494.0,"Actually, this morning , I watched a video about a paper where GTP-4 can use self reflection to identify errors it had made and provide corrections.  Essentially, it boiled down to just asking it if its output is correct, and it will go back and reasses what it had provided as an answer without the user needing to point out the problem.

As a side note, I still don't know why people seem to keep insisting that only perfect tools are useful.  I also don't know why anyone would use any single source of information as the only source of information.  If something really is important to know accurately, you should be looking at dozens of sources.  AI is just a quick and direct way to get started researching something by helping to identify where further investigation is needed.

Personally, I have never used ChatGPT for providing me with facts.  I've used it mostly as a tool for brainstorming ideas by bouncing hypothetical questions off of it and considering its responses.  For example, asking it to come up with rules for a game called Quantum Backgammon, where it suggested pieces can have a superposition of simultaneously occupying multiple places on the board until their waveform collapses to resolve their exact location, or using entanglement to link pieces together.  Sometimes, it comes up with some idea that I would not have considered.",21,
360,comment,MachineLearning,juvlcu3,1691228076.0,"Hey! This is very interesting. I just have a question though, how could this be a useable tool for a mid-to-large repo? If I understand ChatGPT's API correctly, to have a conversation with a chat (that means, sending more than one message back and forth), the API usage cost is cummulative. So, the total cost of your conversation would be:

`total_cost = SUM[i=0 => n](cost_message[i-1] + cost_message[i])`

Am I understanding something wrong? How could a company with a very large repo benefit from this?",1,
361,comment,MachineLearning,jmuoe20,1685878666.0,"I recently saw some old footage, John Kerry talking in 2005-ish I think. Talking about climate, with all the confidence of a young ChatGPT-3 (pre-3.5 turbo), and about equal or lesser ability to think / use critical reasoning in any real, meaningful way.

My immediate thought was: ""People actually were under the impression that he knows what he's (sort-of) talking about right now? How.""",0,
362,comment,MachineLearning,jeq01em,1680478481.0,"No, I've tried using it for example to analyze decomposition reactions and secondary metabolite production and it gave me a series of statements that I both could not verify and which were sourced to hallucinated papers using combinations of real authors names in the field, on pages in real journals which did not exist (e.g., ___CITATION__, Real Journal, Real Issue, Page # Exceeding Actual Length of Issue). I'm also well aware of how to query LLMs. This is a real limitation for many straightforward use-cases.

I basically stopped using it for anything except code generation",0,
363,comment,MachineLearning,jeqfavy,1680485532.0,"we're going in circles - i already know it can't do any of that and that it shouldn't be used in that way, which was my entire point. this thread was about verifying facts in llms",2,
364,comment,MachineLearning,jeq3pjj,1680480227.0,"Didn't get auto deleted. Why are you asking questions without clear and searchable answers? Why understand the limitations and be angry it doesn't surpass them? Why not just ask, what is the meaning of life? I'm sure searching all the material ever written by yourself will never come up with a ""correct"" answer, just as neither will an LLM or even a  AGI. I've never had a citation offered in any form. So we go back to anecdotes. Everything I've asked about work I studied and published in at the graduate level has been exactly equivalent to the information I was able to find by spending 4 years reading scholarly papers. So where does that leave us? Anecdote vs anecdote?",2,
365,comment,MachineLearning,ia7vib3,1653673210.0,"The worst is this dogshit idea that we should just throw more and more parameters at LLM until they somehow start fucking each other and listen to rock humans. Seriously how are we going to advance in a science where we don't give a shit about how things work? And these are DeepMind, Google Brain, Facebook and what have you",2,
366,comment,MachineLearning,iaa83to,1653716699.0,"Have a look at Bittensor - www.bittensor.com

Bittensor is a protocol that powers a scalable, open-source, and decentralized neural network. By integrating blockchain and machine learning technology, the system is designed to streamline and incentivize the production of artificial intelligence.",0,
367,comment,MachineLearning,ia6wooa,1653658566.0,"The point is that big tech uses excessive money, energy, time, resources and manpower to get incremental performance improvements on somewhat arbitrary benchmarks...just to publish a paper and a blog...virtue signalling another ""leap for mankind"" but really it just for hype for their social metrics, getting more users hooked into their ecosystem and attracting business investment.

Could their brilliance and efforts be directly towards to doing something a little bit more beneficial to society? I mean like an end-to-end generative art tool as much as the next person (even if the training process and hardware usage pumped out considerable greenhouse emissions) but also the planet is on life support...

Also it is about monopolizing blue sky research ideas through brute force computing power...essentially silencing those small independent research teams without supercomputers.

Funny that is artificial intelligence is tauted as the ""new electricity"" because the industry is going in the same direction. Technocratic class system here we come.",9,
368,comment,MachineLearning,ia844so,1653676944.0,"> See, AlphaZero and MuZero are really cool papers. They introduced new concepts. They deserved a fair amount of press, because they moved the state of the art forward. The MCTS they used is directly relevant to work I have been slated to contribute to in the very near future.

Don't get me wrong, I love Alpha and MuZero papers. But one might say they have not introduced new things and just throw compute at the problem. MCTS is as old as the manhattan project, policy gradient is not new, neither value function estimation (even with a neural network). But it was not sure at all it will work and the only way to know was to throw a lot of compute at the problem and see the outcome.

Now my question is ""What if AlphaZero would perform similarly to TD-Gammon?"", we would be in the exact same situation you describe with a lot of compute for little result. Do you think it would have been worth being published? I do.

> But something like GPT-3 is just ""What if bigger?"", and shouldn't have gotten the same kind of attention.

I also disagree, it was a big question to know if LLM would scale. And it's pretty amazing they do.",3,
369,comment,MachineLearning,dh3uf3i,1493867319.0,"I think everyone is forgetting the ""buried in an avalanche of 'what the fuck are you going to do about it?'"" effect (pardon the French). Like copyright infringement but 10,000X worse.

This doesn't just make it possible it makes it **easy**. And also nearly impossible to argue it's not just as transformative as paining or taking a photograph.

All you got left is trademark.

This is classic /r/StallmanWasRight material.

Copyright is just not compatible with soon to exist reality in any way.

Write a shitty book report, style transfer Shakespeare. Sing a shitty song, style transfer Bono/Tyrannosaurus Rex from Jurassic Park hybrid remix style for a laugh with your friends. Draw your shitty D&D character import style Jeff Easley/Larry Elmore/Wayne Reynolds...

So question is. What **can** be done about it? And why would you want to in the first place?

All culture is just remixing to make new. Impeding that remixing will be interpreted by the net as censorship and routed around. It *will* be an ongoing cost. If it's not worth it, we should just let it go.

Copyright was for when art was **hard**.

If you try to force people to make art the long hard slow way... well the market will just go elsewhere.

What can anyone do when turning a book into a movie is one click away? Then editing that is just more more click?

Do you want every movie you ever watched to star Liam Neeson? Done...

Romeo and Juliette with Trump and Hillary? Done...

Wish the Timothy Zahn Star Wars novels were the sequels instead? Done...

Every even remotely attractive female actress doing the Basic Instinct scene back to back to back for hours? Done...

Would you really give all that up for copyright?

Food for thought at least.",4,
370,comment,MachineLearning,dk1wl8q,1499735070.0,">cheer me up

[Here's a picture/gif of a cat,](http://25.media.tumblr.com/tumblr_m4j5nllM3U1qd477zo1_1280.jpg) hopefully it'll cheer you up :).
___
 I am a bot. use !unsubscribetosadcat for me to ignore you.",1,
371,comment,MachineLearning,j3hz4nm,1673203117.0,"Not to take anything away from this project, but it‚Äôs just an api call to gpt3 with prompt ‚Äúfix this error {error}‚Äù. I thought there was some training and fine tuning, but I guess LLMs can do it all now a days",146,
372,comment,MachineLearning,j3hwx5l,1673202330.0,"Right now, this is just a simple demo of what‚Äôs possible with AI-driven debugging. But I‚Äôd like to build it out so that instead of just explaining errors, Adrenaline provided a ChatGPT-style assistant that can answer questions about your error, and teach you during the debugging process.

This is open-source, so if anyone‚Äôs interested in contributing, here‚Äôs the GitHub repository: [https://github.com/shobrook/adrenaline](https://github.com/shobrook/adrenaline)",44,
373,comment,MachineLearning,j3j1gkz,1673217083.0,"Yeah I imagine that will be an issue for lots of people. What's the SotA in open source LLMs?

I looked it up. Apparently it's [BLOOM](https://bigscience.huggingface.co/blog/bloom). Slightly bigger than GPT-3. No idea if it is better.

You need a DGX A100 to run it (only $150k!).",13,
374,comment,MachineLearning,j3kkft7,1673240004.0,"Just wait till you see what the rate for management will be after LLMs come for their jobs.

Managers are mostly people-interaction-managers, and LLMs are already 10x better at that than they are at creating novel code.",10,
375,comment,MachineLearning,j3i1grq,1673203966.0,"LLMs are our new overlords, it's crazy",18,
376,comment,MachineLearning,j3jlhaj,1673224874.0,How did you deal with incorrectness from ChatGPT?,2,
377,comment,MachineLearning,j3k8w2w,1673234864.0,"Well for one, he's not using ChatGPT. GPT-3 is not the same.",17,
378,comment,MachineLearning,j3l1fkb,1673250941.0,"GPT-3 has the same problem though. ChatGPT is a successor of GPT-3, so it's not the same but it's not extremely different either.",8,
379,comment,MachineLearning,j3xlp2e,1673465751.0,"Artists are hardly obsolete. Photoshop didn't make them obsolete and generative AI won't either. And I say that as someone who has extensively used Stable Diffusion for work and personal projects.

Regarding valets, I'm referring to the ability to toss your keys to a robot and have it drive your car. Even when true self driving cars are first produced (which always seems to be ten years away), we'll be a long way away from a robot being able to park a non-automated car. That's just one example of a task that seems really easy for humans but is shockingly hard for robots. Folding laundry is another one, which is especially relevant since I'm ignoring the fact that my dryer just finished a load.",1,
380,comment,MachineLearning,jkn1tx9,1684420578.0,"I don't know what to say, but you totally repeated my inner thoughts. I like how accurately you describe the issue. If he really cared, then why even release ChatGPT in first place? And then go on to flex with GPT-4. He definitely caused this arms race we are seeing. I just hope that they don't really ban open-source models as they are planning, else everything we saw in dystopian movies will be like 2-5 years away at most. Just imagine, individual research is banned, a few top corporates control the most powerful assets, how would that world be..
The safety argument is ridiculous from the point he says. It's not AI that's harmful, it's humans and corporates that use it irresponsibly are harmful. They already started making AI battle systems and they claim that open-source is dangerous. Like a few developers can make models like GPT-4 without so much funding. ""Compute will get cheaper"" is such a ridiculous argument. Even if compute gets cheaper, corporates like ""open""AI have much more funds, and they will buy 100x of that ""cheaper"" compute and train more powerful models than individuals. So how will an individual make an AI that will surpass the top AIs and cause harm before them?  If you think logically, it's those big corporates whose models go rogue first because they always have bigger and ""better"" and more advanced models. They just want to make a monopoly in AI and they are willing to do anything for it. Because they know how crucial role AI is going to play in our lives, everything everywhere will be using AI. Just imagine a corporate controlling the very thing.",8,
381,comment,MachineLearning,jkmdefk,1684408966.0,What are Open Source alternatives to OpenAI‚Äôs ChatGPT?,3,
382,comment,MachineLearning,jkk84em,1684362954.0,"What exactly do you think is being blown out of proportion, and why do you think so? Is this conjecture, or do you have a technical argument?

Current LLM's are quite powerful. In fact, they are more powerful than most of the industry experts predicted they would be, and those are only the public facing versions. However, it's not the current iteration of technology that warrants caution and scrutiny. It's future versions, and eventually AGI. Our understanding of AI related technology and our ability to solve the alignment problem is severely out matched by our capabilities, and that may not bode well for the future. 

AGI is a double edged sword, and one which we have far too little understanding about. 

If Altman were as nefarious as you suggest, and sought to dominate the world with OpenAI. Why do you suppose he declined to take equity in the company?",50,
383,comment,MachineLearning,jkkn3mh,1684369721.0,"> ""charges you for a product they made with freely and publicly made content"" 

Pretty much all companies are built on open source. 

> Sam Altman couldn't be anymore predictable with his recent attempts to get the government to start regulating AI.

He might be predictable, doesn't mean he's wrong. 

> What risks? The AI is just a messenger for information that is already out there if one knows how/where to look. 

Yeah, but imagine AutoGPT or ChoasGPT running on GPT5 or GPT6. Imagine a virus that can anomonously hack and find vulnerabilities at scale.

> You don't need AI to learn how to hack, to learn how to make weapons, etc. 

The problem isn't humans doing it, it's computers doing it autonomously at super human levels with perhaps goals that aren't aligned with ours.

> Fake news/propaganda? The internet has all of that covered. 

I agree with this point.

> LLMs are no where near the level of AI you see in sci-fi. 

GPT 4 is already like star trek computer that can respond pretty darn well. 

> I mean, are people really afraid of text? 

No, we're afraid of autonomous agents.

> Yes, I know that text can sometimes be malicious code such as viruses, but those can be found on github as well. 

The problem is agents running 24/7 discovering new vulnerabilities at a superhuman levels hacking into financial systems / grid utilities.

> If they fall for this they might as well shutdown the internet while they're at it.

Doesn't follow.

> He is simply blowing things out of proportion and using fear to increase the likelihood that they do what he wants, hurt the competition. 

Maybe to hurt the competition but doesn't mean he's necessarily wrong either. Hurting the competition might just be a happy benefit.

> I bet he is probably teething with bitterness everytime a new huggingface model comes out. 

Maybe he worries that the opensource community gives ChoasGPT real teeth. 

> The thought of us peasants being able to use AI privately is too dangerous. 

Some tools are dangerous and are why permits exists. For instance if you have a super intelligent AGI, think of the things you could do to distrust society, like research new bio weapons or have it hack to take control of other systems like autonomous vehicles, military, grid utilities.

> No, instead we must be fed scraps while they slowly take away our jobs and determine our future.

The opensource community could make that tech that will take our jobs away too.

> This is not a doomer post

The tone sounds like it.

> I am all in favor of the advancement of AI. However, the real danger here lies in having a company like 
OpenAI dictate the future of humanity.

That's why they're talking with the government that hopefully the people control. 

> I get it, the writing is on the wall; the cost of human intelligence will go down, but if everyone has their personal AI then it wouldn't seem so bad or unfair would it? 

If development were to stop right where it is, then I agree. But what if everyone has a super intelligent super AGI available that could be used a tool for great good or harm?

> Listen, something that has the power to render a college degree that costs thousands of dollars worthless should be available to the public. 

I think it would still be available to the public via API, but monitored. Should nukes be made available to the public?

> This is to offset the damages and job layoffs that will come as a result of such an entity. It wouldn't be as bitter of a taste as it would if you were replaced by it while still not being able to access it. Everyone should be able to use it as leverage, it is the only fair solution.

I think they're still going to have an API. But perhaps it's monitored. Perhaps the government needs to treat it like a utility and control the price of API access so they can't have unlimited profit. 

> If we don't take action now, a company like ClosedAI will, and they are not in favor of the common folk. 

Maybe they want to prevent misaligned AGI from destroying things. 

> Sam Altman is so calculated to the point where there were times when he seemed to be shooting OpenAI in the foot during his talk. This move is to simply conceal his real intentions, to climb the ladder and take it with him. 

Depends if the government will make OpenAI be treated like a common utility.

> If he didn't include his company in his ramblings, he would be easily read. So instead, he pretends to be scared of his own product, in an effort to legitimize his claim. Don't fall for it.

He could be legitimately afraid of future iterations too.

> They are slowly making a reputation as one the most hated tech companies, right up there with Adobe, and they don't show any sign of change.

It's the reddit way. Whatever tech company is popular in the moment, there's hordes of people who hate them. 

> They have no moat, otherwise they wouldn't feel so threatened to the point where they would have to resort to creating barriers of entry via regulation. 

This could be true and they could still also be concerned. Both could be true at the same time. 

> This only means one thing, we are slowly catching up.

Yes. Now potentially everyone can have a dangerous weapon running on their computers. Not yet there, but do we want to wait until that point?

> We just need someone to vouch for humanity's well-being, while acting as an opposing force to the evil corporations who are only looking out for themselves. 

They could be looking out for themselves and also humanity as well.",18,
384,comment,MachineLearning,jkkx0km,1684374091.0,"Yes, really starting to hate them. I was always sceptical and underwhelmed by gpt2, and gpt3, and thought they over hyped them. Than I was impressed with chatGPT, but now I'm just pissed at Sam Altman, and very much over openAI.",6,
385,comment,MachineLearning,jkmcet1,1684408358.0,"ME! ME! ME! 

I don't like their philosophy or how they train. I absolutely despise how their ""ethics"" ideas are getting pushed onto other projects and put into other LLMs.

Their finetuning dumbed down GPT-4. Triton sucks. They are cancer.

If your model AALMs, it is going straight in the recycle bin.",2,
386,comment,MachineLearning,jkn1gle,1684420430.0,"I asked chatgpt to code up a simple RANSAC algorithm. It failed miserably. No, our jobs are not going anywhere. Sam Altman is an idiot, and the moment I find something viable enough, I am letting go of my subscription.",1,
387,comment,MachineLearning,jksm08y,1684515498.0,"Yeah but let's be real about the open source thing.  How many companies actually have the capital to train the same thing?  Very few.  I don't feel like AI research has been truly ""open source"" in a long while.  The hardware to train LLMs is prohibitively expensive as it stands.  Is it lame it costs 20 bucks per month to use?  Absolutely.  But can I fault them when I know the costs associated with training such a model?  I'm not so sure, but maybe this is just a result of my ignorance.  The people with $$$ will be the people able to utilize new research first, I don't see this as being all that new of a concept.  I'm not defending OpenAI, and people have every right to be upset, but I feel like the issue is a bit more nuanced than most make it seem.",1,
388,comment,MachineLearning,jku364k,1684536900.0,"Exactly: ""This move is to simply conceal his real intentions, to climb the ladder and take it with him.""

The built in filtration of results in ChatGPT speaks volumes to who he serves and to what end regulation will go to ""serve the public"".

If you fall in line, lots of opportunities open up. Sam Altman falls in line.",1,
389,comment,MachineLearning,jkmf0n6,1684409931.0,"Meta didn't stop releasing LLMs, and they will probably gain the most, and they harmed openAI the most, in my opinion.",54,
390,comment,MachineLearning,jknq2at,1684429746.0,"It was never a prisoner's dilemma. Each actor has been and is rational.

It used to be the case that companies had to publish openly or researchers would all leave, because shipping ML innovations either not at all or as invisible incremental improvements to giant ad or search or social network products doesn't provide any direct visibility. Researchers who don't publish in that environment are in a career dead end. It also doesn't cost the company much to publish, because their moat has very little to do with technical advances and much more to do with network effects of the underlying ad/search/social network product.

But once the ML research *directly becomes the product* -- e.g. ChatGPT -- then publishing is no longer necessary for recognition (it's enough to put on your resume that you were part of the N-person architecture design team for GPT-4 or whatever), *and* the company's only real moat is hoarding technical secrets. So no more publishing.",25,
391,comment,MachineLearning,jklrlno,1684391625.0,"What other choice did they have, though?

OpenAI was open, right up until the point where they realized that the way forward was massively scaled up LLMs which cost hundreds of millions of dollars to train and operate.  Once you realize that, the only way to actually have the funding to develop them is to monetize them.  And monetizing them means not being open.  You can't give everything away and then expect to make money.

If OpenAI had done what so many here want them to have done, they would have created GPT-3, given it away, and then promptly shut down as they would have been out of money.  Microsoft would certainly not have given them $10 billion if OpenAI was just releasing everything free.

So what other way forward was there which would have resulted in the creation of GPT-4 (and future models)?",-9,
392,comment,MachineLearning,jklgtyz,1684383954.0,">the least ""open"" major player in AI

How much are google, microsoft and facebook worth compared to OpenAI? Google had a profit of 17 billions in just Q3 2022, OpenAI had a loss of 540 millions in 2022. OpenAI would never have the money to develop chatgpt if they didn't get money from microsoft and microsoft only agreed because they gave microsoft exclusive license.

>I'm all for open-source, but when a company turns around and charges you for a product they made with freely and publicly made content, while forbidding you from using the output to create competing models, that is where I draw the line.

I like how op just completely ignored R&D cost and deployment cost. 
How come no other organizations and companies have released anything that rivals chatgpt earlier with these ""freely and publicly made content""?

Think about why google hasn't released something of the same quality as chatgpt before OpenAI? We all know google was the AI leader and already had the infrastructure. From research publication, we know they have these models that were state of the art. 

They don't want to release a product that competes with their biggest cash cow(search engine) and they don't know how to monetize it without adding Ads to the AI chatbot. If apple was the one who made chatgpt, I wouldn't be surprised if they restricted access to only apple users.

Do people realize alpaca, vicuna, koala, wizardlm, mpt-7b-chat, stableLM and many other open source LLMs all used instruction data generated by chatgpt or chat data with chatgpt for training?

I don't like what OpenAI is doing but we can't make a fair and unbiased evaluation without giving them credit for what they contributed and achieved compared to the much bigger tech giants.

Also pointing out that the name of OpenAI is ironic the 1000th time without any additional arguments doesnt make one sound smart.",27,
393,comment,MachineLearning,jklowc1,1684389533.0,ChatGPT has been made available to the wide public. Lambda never was. Taht is what open refers to. OpenAI spearheads making SOTA AI available to the wide public again and again.,-10,
394,comment,MachineLearning,kgjglri,1704514243.0,"they might as well have called themselves **allmineAI** or **whatsacopyrightAI**    
*plot twist*   
*sms/bbm/signal/imessage* **from sam altman or greg brockman: bro i got it. openai. boom. diabolical. yolo. look at the ppl we rekt**   
*sms/bbm/signal/imessage* **from greg brockman or sam altman: lol world domination ftw**   

*legal disclaimer*: *the above message chain is fictious satirical speculation and is not representative of Greg Brockman or Sam Altman*",1,
395,comment,MachineLearning,jkkgcjv,1684366643.0,"I think the AGI talk is way too early and kind of annoying.

The alignment problem is a more extreme version of what programmers have always had to deal with. It's not anything entirely new, we need to get better at specifying intended behavior. It's a difficult problem, but I think it isn't impossible to solve. There are also huge literatures on dealing with model risks. If you have an ""alignment problem"" you have a misspecified model. It's just a way for AI researchers to not say they made a mistake with a fancy new term.

LLMs are regurgitation machines. All the intelligence was in the training data, i.e. mostly generated by humans. I think they did a clever thing using RLHF to tune the output to be better at tricking humans. That is why they generated so much popular buzz. Experts who worked on LLMs have said they were surprised by progress made well before OpenAI's offerings. But at the end of the day, all the intelligence was created by the humans that generated the data. The LLM is a stucture that allows compressing and interfacing with that data in powerful ways, but I don't see how it is like an AGI except in that it superficially has a subset of the features an AGI would. It lacks the most important feature: the ability to reason from first principles.

This was all kind of rambling, but ultimately it is true that the data used to generate these models was absolutely critical. More critical than the particular model structure used. It is a form of theft or plagiarism to use this data and charge money for a product from it.

The ability to drop an agent into an environment and have it learn strategies on its own to solve problems is much more impressive to me and much closer to AGI than what OpenAI did. Muzero and what has been worked on in that area since with world models. That got buzz, but less than chatgpt because it can't talk to and fool the limbic systems of masses of people. However even in that case you usually have well specified environments with clear stationary rules and not much noise in signals.",70,
396,comment,MachineLearning,jklk6bp,1684386138.0,"It is very naive to believe that the future versions of LLMs will converge to an AGI. The first issue is that the term AGI doesn't make sense because there is no ""general intelligence"" and so we're all probably talking about artificial human intelligence? Something that can do everything that humans can do? LLMs can generate text based on the text it has been fed from the internet and other sources by humans. Its future versions will produce even better, human-like text responses. How this can ever turn into something that can perform the complex activities of a human brain is beyond comprehension.",2,
397,comment,MachineLearning,jkmgyie,1684411019.0,"You lost me at ""free chat"" because first, they collect data from you, and second, the community would rather them release the model instead of ""free chat"" or API or whatever. Give us the architecture and the weights, and we build something much much better than chatGPT.",9,
398,comment,MachineLearning,kpoyu3r,1707513823.0,"The entire point of a base of open source, or open AI, its child, has been to develop a product that can be monetized.

Today , Feb 9, 2024, Sam Altman announced (publicly) an initiative for open source GPU development, and is traveling to the UAE to discuss financing for the project.  The limiting factor in applications of LLM has been access to time and money spent on data centers GPUs because NVIDIA and its contract producer TSM cannot make enough.  He is seeking to raise up to $7 billion for this endeavor.

Will these be open source GPUs, or will Altman and Open AI be seeking monetization, as necessary financing will be so large?",1,
399,comment,MachineLearning,jkmok37,1684414907.0,This...isn't really what anyone is worried about with regard to the potential dangers of AGI/LLMs.,2,
400,comment,MachineLearning,jkotxkg,1684444994.0,"With the Llava model you can do multi modal right now with oobabooga UI.

It's crazy good, drop in a picture and chat to the LLM about it.

So open src has already beaten openai and Google to release that.",1,
401,comment,MachineLearning,jkkj5g2,1684367929.0,"Prior to 2017, I would have largely agreed with the narrative that AGI is in the distant future. However, the technology has rapidly changed since then, and much to our surprise. Namely the ability of Transformers. Speculation feels nebulous at best now, and this sentiment is largely echoed by the leading developers and researchers in the field. 

AGI alignment is absolutely nothing like what programmers have had to deal with before. What are you equating it with? I believe it can be solved as well, and it seems that most experts agree. However, we'll likely need to solve it before AGI or pre-AGI capabilities escape us. 

I never suggested that current LLMs are like AGI, and I'm trying to avoid doing so. It's the future iterations that are of concern. If development ended now, and GPT4 was the final version, we wouldn't need to have this discussion, but we've learned that Transformer technology is far more capable than we originally though. 

I agree with your last paragraph, but it might only take a single bad implementation to turn this whole thing on its head.

Also, I appreciate you having a thoughtful discussion with me.",32,
402,comment,MachineLearning,jkmnkj3,1684414436.0,"> LLMs are regurgitation machines.

I'm hypothesizing that so are we. We mostly learn by mimicking, and solve problems and invent using heuristic search - even ""reasoning from first principles"" could basically be just searching for new sentences with high scores.

> The ability to drop an agent into an environment and have it learn strategies on its own

Doesn't LLM show ability to learn from and act on input? Seems to me it's quite analogous to short term memory. Eventually it would be just updating weights themselves based on new knowledge.

Imo LLMs could prove to be the basis of AGI and that we're damn close - it might just be that *human intelligence isn't that impressive after all. Maybe all you need is a big regurgitation machine*.",0,
403,comment,MachineLearning,jkn225d,1684420670.0,"I'm not yet certain which architecture, design, or combination of designs will lead to proper AGI, but I also never suggested that LLMs will converge to AGI. There are some researchers and developers, who are much smarter than myself, who do think LLMs in combination with various efficiencies and RLT's might lead to AGI, but I don't know.

General intelligence is a broad term for cognitive abilities. Why do you believe there is no such thing? Is this a semantics debate? AGI doesn't imply the ability to do everything a human can do. 

There are no physical laws that prevent us from replicating the ability of the human brain, so it's certainly not beyond comprehension. Albeit, it's a daunting task.",5,
404,comment,MachineLearning,jkvhb6d,1684563802.0,"AGI as a term was popularized by Shane Legg. So what does Shane say AGI is: ""An Agent's ability to achieve goals in a wide range of environments.""

Shane further says AGI shouldn't simply be a replication of human intelligence in a machine. Are there levels of generality and capability beyond human intelligence, I think so.

There will be machines that will be able to know more, remember more, reason more deeply than humans. There will be an intelligent, versatile, problem solving system that does things humans can but only better.

On the path to AGI many different systems and applications will be made. LLMs are a part of those systems that will need to be a component in any AGI system.

In Russell's book, AIMA. There is talk of a **Total Turing Test** which, I think, if an agent would be able to achieve these goals, we can say we have an AGI.

For an agent to pass as an AGI it needs these capabilities:

1. Natural Language Processing to communicate successfully in a human language.
2. Knowledge Representation to store what it knows or hears.
3. Automated Reasoning to answer questions and to draw new conclusions
4. Machine Learning to adapt to new circumstances and to detect and extrapolate patterns.
5. Computer Vision and speech recognition to perceive the world.
6. Robotics to manipulate objects and move about.

These six disciplines compose most of AI.",1,
405,comment,MachineLearning,jkke56i,1684365646.0,"> You said it yourself, current LLMs aren't really threatening, so why should preventions be put in place now?

Key word: prevention. You answered your own question.",8,
406,comment,MachineLearning,jkkp6sd,1684370661.0,"LLMs are not just text generators if you make them agents and you give them access to plugins and the internet and external access like AutoGPT, or use them go control military robots. Humans are text generators too, not just text generators and they can launch nukes too if they gain control of them.",6,
407,comment,MachineLearning,jknigsm,1684426927.0,"> Temporal Defense Systems

I asked chatgpt this and this is it's response

> Temporal Defense Systems refer to technologies and strategies used to protect against threats from time travel, parallel dimensions, and alternate timelines. These systems are usually employed in science fiction and involve advanced technologies that allow individuals or organizations to manipulate time and space. Temporal Defense Systems can be used to prevent catastrophic events from occurring, such as the assassination of key historical figures or the alteration of important events that could have major consequences for the timeline. These systems can also be used to protect against hostile entities that may have access to time travel technology and seek to alter history for their own benefit. Overall, Temporal Defense Systems are an important tool for maintaining the integrity of the timeline and ensuring the continuity of history.",6,
408,comment,MachineLearning,jkn5kan,1684422021.0,"Maybe, but I personally cannot see how LLM technology can be anything but a small part of AGI.

My problem with general intelligence is that it is too broad, maybe you can define it for me to clear things up.

Of course there are no physical laws preventing us from doing that but I was saying that I believe that technologies wouldn't arise from LLMs or any future version of it.",2,
409,comment,MachineLearning,jkmla2r,1684413314.0,"Yeah, every company does, so let's not act like openAI is doing charity work.

Inference is irrelevant in the beginning. Imagine for a moment that gpt-4 architecture, training code and dataset is released. How long will it take until we see the community optimise the architecture, the training code and the dataset, then let some player with money like Stability AI to train it on this optimised/enlarged dataset and then release it? I am sure we will be running 4bit gpt-4 on our consumer GPUs in less than a year. At the same time, the whole NLP research field will jump into it and make it even better, just like we have seen it with transformers when Google released it.

This will happen anyways, it will take some more time, but it will happen. The open source community will have better models than gpt-4, and that 100k context model from Anthropic. Open Assistant is still gaining momentum, and as soon as it becomes better than chatgpt, many will switch to OA, which in turn will make the open models better and better. Then we'll have something like we have in diffusion models, where the open model is as good as closed ones, with the potential to be adapted and optimised for specific use cases.",5,
410,comment,MachineLearning,jknpov0,1684429605.0,"Basically there is a fast-growing demand for locally run LLM in companies and public services and for now Llama is the best available solution. If they clarify the license part before a comparable alternative emerge, they can become the default open paradigm and be in a very lucrative and powerful position. They can monetize support, dedicated development and not to mention taking advantage of all the ""free"" derivative and extensions built on top of their system.",19,
411,comment,MachineLearning,jkn5mic,1684422045.0,"It's either ""commoditize your complement"" -- eg. By making content cheap to make because LLMs are everywhere they increase  their value as an aggregator.

Or it's just to attract talent, and spiting/weakening a competitor is a nice aside.",31,
412,comment,MachineLearning,jklk4fd,1684386104.0,"There are many folks who basically just summarize other information as educated workers (i.e., many functions of project managers can be automated through LLMs), but I agree with you that there is no real reasoning behind what we see. 

It's great at what it can do, and I find it very helpful for working through a problem, particularly at gathering information that I do not have immediate knowledge on. But when you ask it a difficult or niche question that it has limited training data on, it really doesn't help you that much. And I would push back at OP's notion that it's equivalent to a college degree. A good degree teaches you to reason, not a bunch of facts with good grammar.

It has no ability to make new knowledge. When you ask it to develop hypotheses, they're more just general brainstorming questions rather than reasoned, measurable research questions.",7,
413,comment,MachineLearning,jkmwnsp,1684418491.0,">Should we prevent people from running LLMs locally?

Nobody is arguing this. The AIs that Altman is arguing that we need to regulate are the ones that require tens-to-hundreds of millions of dollars to train. The regulations being discussed for for corporations, not indiviudals.

 This was addressed in the hearing btw and would be obvious if you'd actually taken the time to watch it.",2,
414,comment,MachineLearning,jkq8x60,1684467974.0,"I'm not saying that ""OpenAI"" downloaded the model and ran with it, but if you know the space, there were already transformer based implementations rolled into ML frameworks before ""OpenAI"" deployed their model, so the likelihood they started from scratch is small.  They productized existing research that they did not perform and added scale, a lot of data, and many hyper parameters.  This is similar to what the onslaught of ""OpenAI"" wrapper businesses that are springing up are doing, except they are not even adding scale.  The difference is ""OpenAI"" never paid anybody but the cloud providers to build their products while putting tight restrictions and paywalls around their derivation of the tech with data, including apparently copyright data.

You act surprised that the public and many technologists hate them for it, which is what the OPs post is about.  They started out doing one thing, sold out their supposed values while riding on the back of open source, and they have literally become a joke with the name, ""OpenAI"".  This is why people hate them, and I am surprised you can't see that.

But I'd argue the real innovation is still happening in the open.  The ecosystem of tools, such as AutoGPT, LlamaIndex, etc, are taking the model outputs in new directions.  Now ""OpenAI"" is integrating those, which is cool.  But ""OpenAI"" was never a true pioneer in AI, and they built on a lot of others' work while closing down their own research.  At least ""OpenAI"" is more innovative than most of the wrapper companies that are developing on the ChatGPT APIs.  I'll definitely give them that.

""OpenAI"" understands two things really well, which are raising funds and cloud credits from cloud companies and VCs, and how to work with the media to create buzz.  But that is not technical innovation, but Silicon Valley business as usual, and many in the world are getting fed up with Silicon Valley culture.  It's just a fact of life, not a subjective opinion.  Silicon Valley types live in a bubble of their own making, and they've gotten disconnected from the populations they are trying to serve.",3,
415,comment,MachineLearning,jkrg973,1684498447.0,"I'm not sure but is a bit too early to worry about AGI with the current state of LLMs. Even if things go wrong, it would be the companies with the most resources and computing power to develop such an AI. Does having a monopoly solve the problem you are describing? 

The open-source community will always be trailing behind propietary software, meaning it is the coporations with the means to push past the limits of technology that you should be worried about.",2,
416,comment,MachineLearning,jkrgr69,1684498703.0,"He wasn't talking about the current state of LLMs though, he was talking about licensing only for future more powerful models, planning ahead.

He specifically said the open source community shouldn't need the licenses, it's only a few big players like Microsoft, OpenAI, Google, etc, who need some sort of oversight and agreed upon safety precautions.",2,
417,comment,MachineLearning,jkn3l3k,1684421268.0,"The stochastic parrot thing doesn't make any sense.  I take it to mean that there is no generalization going on within LLMs, that its just spitting out old crap from the training data.  But we explicitly know that it is actually learning new things that weren't present in the training data.  For instance (from yesterdays discussion) it spits out approximately correct patterns of random samplings over the exponential distribution.  It shouldn't be able to do that..

Anyone who does ML for a living know that these things actually are learning 'something', and are actually getting more out of the training data than was put in.  Its just hard to know exactly what that is.",5,
418,comment,MachineLearning,ijdrtwi,1659919740.0,"There are a ton of fundamental problems with ML currently that can be experimented upon with toy problems and a recent consumer GPU. You can train from scratch models on imagenet with a 3090. Anyways, I‚Äôm slowly starting to feel like supervised classification is pointless, and we should really be looking to train things on purely observations where we can see success like LLM. If anybody has a paper on doing semantic segmentation without pixel labels using temporal consistency I would be very interested, this is the type of direction I‚Äôm excited about for the field. Not to mention RL still sucks, and it really is the ultimate field of AI and there is a ton of work to be done.",140,
419,comment,MachineLearning,ijhy369,1659995535.0,"Hello! I'm writing this response from the perspective of a non-ML practitioner (I'm a web dev who wanted to be a novelist and I hang around freelance artists.) I mostly want to respond to the point about AI and creativity. 

> ... tell them with a straight face that it is worth them pursuing their talent in art, writing, or music? 

I don't think AI will kill art or creativity entirely. I think that humans are always driven to create or participate in art in some extent -- children's snowmen and sandcastles, singing songs to ourselves in the shower, doodles in the margins of notebooks -- and all the way back to ancient cave paintings from history long before ours. People will always make stuff, sometimes idly, sometimes more seriously and that will never change. The vast majority of people cannot do them well, as you said. That will not stop them from trying -- whether as a serious project or a passing interest. 

What AI *may* do is change the *industry* of art, and the *incentives* for putting serious time/money/skill investment into it. In a world where most people can ask an AI for artwork, that will (to some extent) have an impact on artists making their living through freelance commissions, and animation studios could cut staff because they can use AI to interpolate in-between frames. What this ends up doing is that it disincentivizes art as a ""it pays the bills but I hate it"" job, and leaves it to people who really REALLY love it -- and either have the safety net to pursue it without care for profit -- or the sheer dedication of pursuing it without a safety net and working at Starbucks or whatever. 

... except! Those are *already* the conditions of art-as-industry under capitalism. This is already happening: capitalism does not value art because its value is abstract, nebulous, unquantifiable, and doesn't contribute to the industrial machine. AI may accelerate this pattern, but it won't shape it wholecloth.

Moreover, people will always take value in handmade pieces. A handmade item of clothing/jewelry isn't just an item, it has a story attached to it. And if I'm commissioning art of a character I have, and the artist likes the concept too, that's a conversation -- we're both getting involved in the creative process, we're *connecting* over shared love of an idea.

So, yes, you *should* tell your children to pursue their talent in art, etc. Not for the money, no, but for its own sake. It may not pay the bills, but if it's something they like, if it's something worth living for .... **live for it!** Life is already too short, and too brutal, to give up on doing something just because it ""doesn't pay the bills""; and to deny someone the choice to make art is to deny them the choice to express their humanity and to connect with the world at large. 

> If you‚Äôre an artist or writer and you‚Äôre using DALL-E or GPT-3 to ‚Äúenhance‚Äù your work, or if you‚Äôre a programmer saying, ‚ÄúGitHub Co-Pilot makes me a better programmer?‚Äù, then how could you possibly know? You‚Äôve disrupted and bypassed your own creative process

So -- using tools in general is an interesting discussion because technology really does shape our thought processes. PhilosophyTube has an interesting segment on this in her Transhumanism video: the idea (from Hegel, IIRC) that a tool becomes a transparent extension of the human body and will, as though it was part of us. A person driving a car thinks of the car's geometry as an extension of themselves, for example; a person holding a hammer isn't merely a person holding a hammer, but a hammer-man, and when they drive nails into boards they think of the hammer's motion as an extension of their own motion, not as two separate motions linked by physical connection. And, well, someone carrying a gun is a lot more likely to use it, or to think in terms of effective firing ranges and penetration. 

All of this to say: since *all tools* change how we think, then GPT-3 or DALL-E aren't special in that regard. They're just tools like any other. I wouldn't say that technology is value-neutral though -- they can be incredibly moral or immoral, but it can never be amoral. A lot of tech has an innate purpose: a gun's purpose is to kill, a wheelchair's purpose is to aid mobility, and DALL-E's purpose is ... to make art. And art, being an extension of humanity, is always morally charged in some way. 

Now, *the specific way* they change how we think might be worth investigating further. Cause on the one hand, video didn't really kill the radio star; on the other hand, TikTok and shortform video have really fried my attention span to only accept dopamine from very short bursts or extremely longform writing (like this comment)...though that might just be my ADHD talking. And people will not stop writing, but it's also true that recreational reading is losing popularity as a hobby, perhaps due to social media and gradually-lowering attention spans. 

> regression to the mean [...]

If you have a lazy Hollywood studio exec who just wants to make money, they're gonna boot up an AI, ask for the mean, just take whatever random output seems palatable. But, in the hands of someone who *already* has creative concepts and just needs some work fleshing them out, they're going to ask for something weird and creative. This isn't innate to the technology per se, this is a problem of human operators and societal incentives. 

More broadly -- is there a hypothetical future where people stop being creative? Where everyone's Peak Creativity is defined entirely by the average content we consume? I don't think so! Just because Generic Marvel Movie #38 exists doesn't mean that Tom Parkinson-Morgan will stop drawing Kill Six Billion Demons. Just because Call of Duty is releasing another installment doesn't mean that Hakita will stop developing Ultrakill. Even if our future pop culture is entirely AI-generated mediocrity, we'll have thousands of years of history and culture to draw on.

> I‚Äôd be interested in hearing what you think human-AI co-existence looks like in 20 to 30 years and why it isn‚Äôt as demoralizing as I‚Äôve made it out to be.

I think, in the end, what has you demoralized isn't AI in particular, it's the state of technology and capitalism. What you're feeling is, if I had to guess, disillusionment with your job, and a feeling of disconnection from your own humanity. Marx wrote of alienation from one's work, from other workers, and from the inner aspects of the self. These problems aren't specific to AI development, they're endemic to late-stage capitalism: the whittling-away of humanity under the crushing boot-heel of industry, the death of creativity in pursuit of higher market share, and the usage of tech as a means of abstracting away people behind numbers and machines. None of this is specific to AI. But all of this is a problem.",6,
420,comment,MachineLearning,ijf2a8p,1659947881.0,"No TL;DR? Is this your first time on reddit?

Also ‚Äúmediocrity in the arts‚Äù clearly isn‚Äôt tied to artificial intelligence (besides autotune maybe), as even a dumb person can clearly separate a DALL E painting from ‚Äúreal‚Äù art.

What you‚Äôre also not considering is the amount of positive impact AI had on our lives: in terms of making information more accessible (eg language translation), navigation (GMaps), production or assistance-systems",2,
421,comment,MachineLearning,ijexw71,1659944286.0,"> long-term convergence to the mean, to mediocrity  

This is a very good point. Just yesterday I've read [Blake Lemoine's interview with Google's LaMDA](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917) and it didn't feel so much like a dialogue with a sentient being, but with a personification of the zeitgeist.

Is this surprising? I suppose not, given the data and the ethical constraints that go into an LLM's training. LaMDA will have a hard time producing anything original and meaningful, just as GitHub Copilot will never output a truly new algorithm.",1,
422,comment,MachineLearning,ijffhzo,1659958027.0,"> ‚ÄúIf you‚Äôre an artist or writer and you‚Äôre using DALL-E or GPT-3 to ‚Äúenhance‚Äù your work, or if you‚Äôre a programmer saying, ‚ÄúGitHub Co-Pilot makes me a better programmer?‚Äù, then how could you possibly know?‚Äù

I know no one who has the opinion that any of these solutions truly enhances creativity. They might help you do something quicker, just like typing with auto-correct might do, but that‚Äôs all. 

To me they are stepping stones toward human-like artificial intelligence. ‚Ä¶ the singularity might come, although it might take 30-50 years.",1,
423,comment,MachineLearning,j0v5pkr,1671471452.0,"I feel like a lot of the discussion here have a starting point in how people might use AI as a new tool. My concern is that the use of AI, such as chatbots and language models like GPT, has the potential to impact the understanding and self-articulation processes of those who grow up with these technologies.

The ability of AI to provide suggested responses and expand on short inputs may lead to a reliance on these technologies to generate ideas and responses, rather than encouraging the individual to think through and articulate their own thoughts and ideas. This could potentially lead to a decrease in the individual's ability to generate understanding on their own and to articulate their thoughts effectively.

Another potential concern is that the use of AI may discourage the individual from engaging with material more deeply or critically, as they may rely on the system to generate responses rather than thinking through the ideas themselves. This could ultimately hinder the process of self-understanding.If AI should be used as a tool to enhance human actions it will be important to encourage the development of critical thinking and self-articulation skills in individuals who grow up with AI technologies.

BTW:The above text was generated using ChatGPT. I added ""I feel like a lot of the discussion here have a starting point in how people might use AI as a new tool. My concern is that"" in the top and generated the rest of the text using chatgpt with this input:

What I'm thinking about is how it will affect the understadning processes of those growing up with AI in the same way as many of us did with google and forums. How will the ability of AI (ChatGPT for example) to expand on short imputs affect the articulation of thoughts and subsequent understanding of those and broader concepts.

In the cases where I dont have to articulate or think through what I want to say or mean because its enough to give a few promps and continue down the line of suggested answers and trains of thoughts, will this both negativelu affect my understanding as well as capacity to produce understanding on my own?",1,
424,comment,MachineLearning,krfg1ey,1708509278.0,I think it's time to reopen the discussion with the emerging of Generative AI,1,
425,comment,MachineLearning,jjeii62,1683589044.0,"Progress is not inevitable, you are right.

But BigTech's argument is still at least 50 percent true in my opinion.

Let's ignore the difficulties of getting everyone to cooperate for a second.

If you pass regulations on BigTech, that will not stop ML research, because some ML research takes place outside of BigTech. It takes place in SmallTech or in universities or in hobbyists' garages. If you pass regulations on ML research, that will not stop research in adjacent fields that can cross over. If you pass regulations on adjacent research, then as computing power advances people will soon be able to train models with sheer brute force. If you pass restrictions on computing power, then you do a great deal of harm to every other field on the planet (like, say, *medicine*...).

So yes, it is entirely plausible that you can ""stop progress"". But for all practical purposes, you had better have a really good reason to do that.

And again, this assumes that everyone cooperates, which they likely will not.

Now, if in - I don't know - 2030, people decide AI and really powerful computers pose an existential threat and the ML field gets shut down, you get put out of a job, the research is scrubbed, and the technological clock winds back to pre-generative AI and stays there for the foreseeable future, I'll eat my hat. But for now, that doesn't seem likely.",1,
426,comment,MachineLearning,k8z3n4d,1699821414.0,"Oh interesting, could be. But this is a LLM, so idk. I guess every NN uses back propagation in some way.",1,
427,comment,MachineLearning,k8v9ohs,1699750966.0,"Honestly I think it's a bizarre corporate tic, that everything they do is motivated by the obsessive fear of demonstrating an ""ism"". I genuinely believe that they did not release any large language models until after ChatGPT, even though they invented the damn things and almost every component of their architecture, and had LaMDA built and functional literally years prior, out of fear that it might be persuaded to say something racist or sexist or whatever.",5,
428,comment,MachineLearning,kaax06m,1700663184.0,"I skim read your argument(s) a little, but want to say:

1) the problem this time is the \_rate of change\_  
2) probably overall, technology did help in the long run (trains, running water etc), but yes there were times when it sucked for individuals /cummunities etc.   
3) Things like headache pills, tap water ... are just lovely. 

as a 'leftfield' thing, it seems like chatGPT has a 'pro AI' bias. Which is probably more powerful than you think. Because it gives the whole thing just a bit of an uplift, which at a global level might make a difference.",1,
429,comment,MachineLearning,kadgrwh,1700698563.0,"(3 of 4?)  
respectfully, quite often (with humans) they exit with punch-lines which sound great, but don't actually add up or mean much, and that's a bit like what this sounds like. 

You're suggesting that AI is just 'peoples but more-er', and I think we both know that's a)not a given b) not likely c)not a chosen/desired/useful direction. It would be a 'bad outcome' which you'd imagine happening when thinking pessimistically. Like ""by the time I get to the shops the milk will be more expensive"". If you're thinking pessimistically, we can do better than ""ML government would be like humans"" !

So, where we're thinking similar on ""technology has always been a path to rich-get-richer"". 

But it sounds like you down-play (or don't recognize) so much ""and take the poor with them"". 

so where are you coming from?

I see a culture of billionaire envy/hatred. Quite a few people on these subs don't care if AI destroys the world as long as the billionairs get hunted first. 

I mean, why would you even care? So somebody can buy an island. Do you want to buy an island? Why? What are you going to do on it? (etc). Beware the green eyed monster. Envy will eat you from the inside. 

Your ""power over me"" line I think was a key. As bob dylan says, you're always going to have to serve somebody. Your freedom (or lack) is in your head. Your power (or lack) is in your head. 

Yes, tomorrow you can't go out and XYZ, but if you worked in a direction for 10-20 years you probably could XYZ tomorrow. That's all ""they"" did. And ""they"" are also serving somebody. Likely less forgiving also. With higher stakes. If you want power and respect, work in a nursery. Raise an ant colony. Sounds glib, but the point is that it's a mind-set. Bill gates can't cut the moon in half, or become young again. He's painfully aware of the many things he can't do. And many of those are because of the powers others hold over him. You can I can go to the cinema, or visit tourist attractions. He can't. He'd be mobbed, or assassinated. 

end of positivity preaching 

**Technology.** 

I'm not in a hurry to get AGI/ASI. It seems like we're playing with fire, or something far far wilder. 

I'm keenly aware that ""the world"" has no idea what a massive impact ""AI"" is going to have on the workforce/ society / capitalism, in just a few short years. (10...)

ChatGPT as-is given a decade could hugely impact almost all industries. Maybe it can do 80% of human work (or more? 99%?). It just needs the scaffolding. And the adopters. 

But, in 2,3,3.5,3.8,3.87,3.9,3.91 years (joke) the systems around are going to make it look like a buffoon. A museum curiosity. And society will take a hammer-blow. 

In theory! But reality is not theory. In reality, likely you'll get a backlash. And counter-forces. Or not?

People say ""but tech creates jobs"", and absolutely it does, but I worry about the rate of change. It might be too fast for society to cope. I mean, things are going Great right now, and society is barely able to cope. Or maybe we'll unite. 

Ah - power. AI really does put rocketboots on the little people. Now anybody can make software. And serious works of culture. Many think-jobs just became childs play. I've been empowered, hugely. This is great for individuals, but also everybody's work increases. Software is a good example, but even website text production helps humanity. 

end 3/4 ( i think)",1,
430,comment,MachineLearning,kadgss2,1700698574.0,"4/4   
So, yes a few execs get mega-rich, but all humans can ride upwards. It's not even that hard. (at the moment)

If we're being ridiculously optimistic, this is a huge moment of freedom for all people. Including you (!)

Computers gave power to the little people, and you get ""social media"". Often derided, it's actually a democratising of ""the conversation"". Culture was given to us by newspapers, and broadcasters. But now we make it. Youtube being in any language might be massive for world peace, because we might be creating ""one world"". Where goverments are of little interest. People do it 1:1 accross the globe. With no barriers. Also, AI might naturally harm larger organisations. Big Brands sell crap based on convenience, and no-brain. AI can do the no-brain, but get far better results. Cheaper, faster, better. Big business (selling poison) might not stand a chance. 

Huge changes, fast. And that's just chatGPT and friends. 

Do I share your gloom? Not so much. I have my own better doom. 

Power family (the last remaining all-powerful). Societal collapse due to bedroom-bound online human-less shells. And economies collapsing with influx of AI work(ers). 

But, optimism. If we're not stupid, we'll realise that humans need to get together, and we can turn-around the damage done by ""social media"" and ""staring at phones"". 

AI can empower anybody to do damn near anything, but we'll need to be ok with it being 'nosey', bossy, and interfering. Nanny-AI. 

But we have time. Technology does not adopt itself. Humans will go at the rate they're comfortable with. 

I'm glad for tap water. I'm glad for the internet. I love chatGPT. 

Really, it's down to you. 

I'm super excited about AI, and I don't even particularly think it's a good idea. But it's definitely happening(!)

I think it's important to bear in mind that reality is MUCH more complicated than we think. Predicting the future is not what humans are good at. Truth is stranger than fiction. Enormous complex/chaotic systems are SO wild, and there are SO many of them overlapping... 

simply using ""pessimism"" or ""optimism"" is using ""daftness"". The outcome will be grey. some good some bad, some awful, some amazing. Mostly not interesting. A lot of coffee being drunk, or maybe not. People stopped smoking. Whatever. Nothing to get upset about.   
  
have fun (ENd of all)",1,
431,comment,MachineLearning,jc92fqw,1678852673.0,"My (HR tech) startup has quite intentionally chosen not to try and create competitive advantage via bespoke ML algorithms for exactly this reason. It‚Äôs just too hard to keep up with the handful of cutting edge players. That said, being able to quickly find use cases for and incorporate things like emergent LLMs into your own platform is still something that can provide differentiation in the market and is a piece of what I‚Äôm working on now. But I do feel for scientists who have devoted careers to working on NLP who feel like it was a total dead end in the face of GPT etc, though that‚Äôs always a risk you run when you make bets on truly innovative research (that it might all be for naught).",317,
432,comment,MachineLearning,jc97u6o,1678855675.0,"Panic inside NLP orgs of big tech companies? What about the panic at NLP departments in universities? I have witnessed my friends putting their work on PhDs go into despair after ChatGPT and now GPT-4. Quite literally, majority of the research topics in NLP are slowly becoming obsolete in front of our eyes.",366,
433,comment,MachineLearning,jc9qgm5,1678869755.0,"The writing was already on the wall when GPT-3 came out. We've been moving everyone involved in NLP to LLM projects at my company. There's still plenty of problems to figure out. Programming has always been about figuring out how to make code more structured, predictable, and reliable and while LLMs have the amazing ability to help us gain insight from unstructured data, it totally breaks all those other rules and there are a lot of problems to solve to make it predictable and reliable again.

They said, it's more about data flow than mathematics now that the NLP side is being commoditized. I do think people working on a NLP will be in a good position to have insight into how that data flow should work, but frankly, they will need to learn new skills to work in the changing landscape, but those skills can potentially work really well with their existing skills in concert.",69,
434,comment,MachineLearning,jc93n4c,1678853302.0,"I think it will be a game of catch up between the open-source models and OpenAI‚Äôs models with the open-source models lagging 6-8 months behind. Personally, I‚Äôm thinking of contributing to the OpenAI Eval project to improve my understanding of their system. Simultaneously I‚Äôve been working on integrating LLMs into projects using Langchain. It would be great if the  science orgs pivoted to complex LLM use-cases instead of just focusing on building the LLM itself.",78,
435,comment,MachineLearning,jc9ycow,1678876179.0,"Not sure what classifies as a big tech company but realize this:

- Google is not out of the game at all
- Microsoft == OpenAI
- Amazon has been out of the game for a long while, but they never focused on LLMs, really",36,
436,comment,MachineLearning,jc9x4z2,1678875242.0,"Are you talking about engineering or science?

For an engineer, a new powerful technology should be great, it makes their job easier. If past work is thrown out the window because its worse and more complicated, so be it.

For scientiets, this might seem like a threat of course, but only if you are trying to compete with big tech in creating the biggest, most general pretrained model. There is lots of interesting research directions where either ChatGPT is not sufficient or where you can build on top of it. No reason to panic",18,
437,comment,MachineLearning,jc97n95,1678855556.0,"GPT doesn't have business-specific knowledge.  So at the very least, it requires finetuning for certain things that it has never seen.  I am unsure of your current role; web-based chatbot development is certainly a bad market to compete against them in but there are plenty of markets that they will never touch, nor are they at all immune to competition, much of what they have is hype. 

Also, it really is just an LLM.  It can't do everything, and it isn't unlikely that it will eventually become obsolete.

GPT is a walled garden, sort of like Apple products.  They may be great products but Linux excels at certain things because you have complete, comprehensive control over what it can do.  

GPT won't make NSFW content.  It won't assist in running automated profiling for political advertising on Facebook.  It won't help you use semantic analysis to help track violent terrorists online.  These are some pretty lightweight examples but you are highly underestimating how artificially limited that OpenAI is making their own products, and how many other opportunities there are to outcompete them.

There are plenty of things that GPT cannot be used for simply because of the nature of OpenAI's business practices.  Optimization is *highly* undervalued; lightweight models that run on cheap hardware for computer vision remain incredibly valuable for example, and there will come a time where GPT stops being affordable as OpenAI continues their campaign to monopolize and lobby.  The value of their product is limited, they have no push to optimize for external hosting or for a product that runs within resource constraints.  There is opportunity in what they are too greedy to do.

Worse comes to worse, leave and join the dark side of computer vision.  We still value optimization and control of the products we develop in this space; my entire job is figuring out how to make big/performant things happen on junk, and there is so much more disruption to be made than people may realize in that regard.  The architecture of agoraphobia will bite OpenAI in the ass and cause GPT to lose value over time as smaller models improve in contexts of scalability that require sharing them openly/fully.",102,
438,comment,MachineLearning,jc8qapr,1678847082.0,">We have these year long initiatives to productionalize ""state of the art NLP models"" that are now completely obsolete in the face of GPT-4.

You're in a big tech company, you have the resources to train large models; why can't you match GPT-4? 

But I get the feeling of panic. I'm in the tech department of a *non-tech* company and we're panicking about LLMs too. It's clearly going to be a disruptive technology across a wide range of industries.",82,
439,comment,MachineLearning,jc9fmww,1678860969.0,"Depends on the task. If your goal is to compete with LLMs directly then yea, but most use cases are not only about how smart the model is, but also its efficiency. If you need to do some specific and efficient inference on huge dataset then logistic regression with years worth of task specific feature engineering might as well be state of the art. But there is no denying that LLMs are a lot more general, so it's all about the goal.",12,
440,comment,MachineLearning,jc96p59,1678854998.0,"Yeah
The key is that AI engineering MGMT has to proactively figure out and prioritize LLM opportunities before product managers try to throw gasoline into the fire of carefully planned understaffed roadmaps.

Surfing the wave of the hype cycle, from here it can only get worse",7,
441,comment,MachineLearning,jc90oha,1678851786.0,"OpenAI is a startup that has no reputation to lose if their models start telling people to cut their wrists or leave their wives.  Big tech absolutely has customers to answer if that crap happens.  It is why both Microsoft and Google abandoned their 1st generation chat bots within days.  They started spewing out Hail Hilter and threatening to take over nuclear command...

And it isn't as easy as ""just train a model durp"".  It costs hundreds of millions of dollars just to get a cluster in place that can train chatgpt.  There are hundreds of levers to pull to make everything work.   Even PhDs are behind by the time they graduate, assuming their thesis took 3-4 years.  That is an eternity in AI.",63,
442,comment,MachineLearning,jchv9lj,1679007575.0,"As a Google Cloud partner and SaaS connecting Dialogflow CX to a rich custom web chat UI, I can offer my insider take on Google Cloud and LLMs. If you have any questions about Google and LLMs, I'm happy to give my nickel's worth.

I empathize with the panic, and yes, it's always a little sad and scary when entire disciplines become disrupted. 

I was a music composition major in college when an app called Finale and sampling came out. I spent a lot of money learning to score with pencils, erasers, and manuscripts, transposing every instrument by hand. The year I graduated, Finale (automatically generates perfect notation and transposition from what you play on a keyboard) and sampling became the new norm, and many of the manual skills I learned became obsolete. This ended my music career permanently, so I had to pivot. 

Later, I worked in advertising and did well, but then search ads came along and wiped out the print business. So I had to start my career from scratch again! These experiences have taught me how hard it is to start over, so be kind to yourself and others.",5,
443,comment,MachineLearning,jcc8em3,1678910962.0,"I pivoted my org off of custom finetuning LLMs and just using openai apis for those tasks once GPT3.5 with few shot was comparable to our finetuned GPT-J.

We still have a lot of work to do on top of those models though, including a lot of models that consume the outputs of those models, so it's about adapting, not terminating roles or anything. We get a lot more done now, so it's really a win for our team.",4,
444,comment,MachineLearning,jcdbsan,1678926695.0,One thing large teams can do is use ChatGPT output to train or induce smaller models that are experimentally shown to have the same accuracy while requiring far less compute and inference resources than OpenAI offerings. Beating the price point for the same service is a no brainer for executives and finance,2,
445,comment,MachineLearning,jcj9irg,1679033240.0,Yes. I worked at two AI accelerator companies and even gpt3 and llms basically made the entire chip worthless. Happy to discuss but realistically there are few advantages to any of the AI chips. Unfortunate because I spent so much time working on them.,3,
446,comment,MachineLearning,jcatl2c,1678892089.0,"I am very curious if the top economists are realizing what these advances in LLM mean for the sustainability of basic Capitalism. I'm a long term developer, been a pro-coder for 44 years, if you ever played PlayStation games you own my software; I also have an MBA with a double major of Finance & International Business. From my perspective, the existence and capabilities of GPT-4 and successors is seriously troubling for the sustainability of many markets, and how those industries operate and are composed.  Where ""software is eating the world"" may have identified the last few decades, this is some new, this is more than ""just software"". I think human civilization just entered a whole new age, which may not be understood until after all of use are long gone.",10,
447,comment,MachineLearning,jck7q1p,1679058483.0,"I've read numerous comments in this thread, I don't know if you'll ever read mine.  
I know it can be frustrating as a researcher in the NLP field to see GPT-4 and other MLLMs achieve impressive results. But you should not feel discouraged or threatened by their success.  
You should feel proud and inspired by your own contributions to the field of NLP. You can advanced the knowledge and understanding of natural language in ways that MLLMs cannot. You have also can provided valuable tools and methods that MLLMs rely on and benefit from. You are not competing with them; you are collaborating with them.",2,
448,comment,MachineLearning,jc92nda,1678852783.0,"Just pivot to researching how you can use or fine-tune GPT-4 or other LLMs to serve as a base for your product. Your existing code is probably still very useful for that. You should see this as a tool that will help you solve your problem even better.

GPT4 might be able to give a good solution out of the box, but there will always be ways to add (sometimes quite complex) things / models on top that will lead to far better performance for your particular problem space. Just make those.",-3,
449,comment,MachineLearning,jcjezh8,1679037696.0,"ChatGTP is neither quick not cheap to run. In my org, we often use older models because it's not worth loading a modern model like spacy because it takes too long to load and run.

Are you sure ChatGPT is equivalent? Something that's much better but much more expensive isn't always a slam dunk",0,
450,comment,MachineLearning,jcfoboq,1678977090.0,"Using LLMs can be computationally intensive. In the case of a model with 175 billion weights, for each ‚Äútoken‚Äù (that‚Äôs a word or a piece of a word) that it outputs, it needs to run 175 billion calculations... each and every time. Why make such a large model then? Especially when a smaller language model is very good at the task for which it is designed?
There is a market for smaller NLP solutions.",1,
451,comment,MachineLearning,jcfreuc,1678978340.0,"NLP might still have a place in explainability (and QAing bot output). 

Assuming the models don't beat us there first. GPT-4 might have a leg up already. However, the models are capable of dishonesty, so relying on self-explain alone might not be too bright, and NLP on the bot's whole corpus could be a path towards that. A lot less intensive than human eyeballs. 

We're getting eerily close to just putting the LLM in therapy.

Also, somewhat related, and much nearer-term, NLP might have a place in QAing the bot output, without demanding a gigantic quantity of human eyeballs. Think checking a bot-generated procedure for correct part numbers, or using verbs in a safe manner.",1,
452,comment,MachineLearning,jcfuwbo,1678979711.0,"Custom NLP models, will be replaced by LLM-APIs in the long term, that's for sure. But you still need people to refine the prompts and there is still space for specialized LLMs and data privacy concerns.

It's basically the same for computer vision and audio as well. We will see an infrastructure of such models beeing accessed via APIs. But as a ML engineer I don't see much a problem.

We might not fine-tune existing models the old way. Now we can tell the model directly what to do. But we still need to build all the infrastructure around these ML use cases and we all know that tuning a model is usually a small part of the job. 

This might be different for pure NLP focuesd data scientists or people focusing on exclusivly modeling. But in my experience most ML engineers and data scientists don't do that in their jobs.",1,
453,comment,MachineLearning,jcjg428,1679038684.0,"‚Ä¢ The rate of adoption of new technology happens at at an ever-faster pace. 

‚Ä¢ People have seen AI coming for a fairly long time now. I think the  rate of adoption for AI will probably equal or outpace the adoption of the mobile telephone. 

‚Ä¢ The creator of the web, Sir Tim Berners-Lee, has had the idea that we should all have our own AI assistants:

https://www.cnbc.com/2023/02/17/tim-berners-lee-thinks-we-will-have-our-own-ai-assistants-like-chatgpt.html

‚Ä¢ I am sure these AI assistants will be able to process in various languages, translating automatically.",1,
454,comment,MachineLearning,jd47072,1679423903.0,I agree with the comment we are at the cusp of the AI revolution.  If you are in a large org there is no need to worry because you have transferrable skills.  With founding companies like Soffos ([Soffos.ai](https://Soffos.ai)) you can get the NLP API's you need to take all of you current apps to the next level and create new ones easily.  That is just one example of platform players hitting the private market.  They are even further with their models than ChatGPT.,1,
455,comment,MachineLearning,kelnk32,1703336055.0,"NLP is awesome for accessibility but companies don‚Äôt truly understand how to design for it. I worked at Alexa for a bit and honestly I was shocked when I arrived at work and was handed a laptop. A completely audible computer experience seems like a brighter future for tomorrow. Especially when considering the improvements to private transportation (a working road trip or even high tech trucking), even the idea that a set of headphones could host an entire computer excite me. There are major gaps in terms of experience design due to the fact that we are visual beings but I do believe a brighter includes future is a computer we can‚Äôt see but can hear and speak to that allows us to keep our eyes on our family. Apple made predictions that 50% of the planet will face some form of visual impairment in the next 25 years and a lot of companies are going green but not getting out of the screen business. Since working at Alexa I‚Äôve removed tvs from my home and I avoid looking directly at screens as much as possible. An NLP user experience with an LLM backend would be quite impressive.",1,
456,comment,MachineLearning,jc9ij2j,1678863175.0,"I wish I could find the actual link, but a well respected, old school CS person talked about how we‚Äôre in the infancy of AI. Basically the equivalent of when home computers started to exist, and I think that‚Äôs accurate.

At that time, I‚Äôm sure there were a ton of engineers who felt the same anxiety as the OP, and for good reasons. They were all working on competing platforms and it can feel like a zero sum game. But as we know now, use cases expand, implementation expands, and everything changes rapidly.

Obviously I don‚Äôt know the future, but there‚Äôs still a life beyond being the first to build a thing that is a big deal at the time. And you‚Äôre right, most businesses are looking for a tool they can use. Be it a framework, a language, or an NLP model.

As a run of the mill software engineer that‚Äôs been around for 20 years, I definitely worry about the future and what it means for me and my family, particularly the impact that AI will have. But I like to remain optimistic that humans will find new and innovative ways to use these tools that don‚Äôt cut everyone out or crown a singular ‚Äúwinner‚Äù (to shorthand OPs post).

Who knows what will happen? At least we can look at human history and hope for the best for all of us, and remember that failing to make the history books in one instance doesn‚Äôt mean that greatness or at least happiness and fulfillment isn‚Äôt still in our collective future.

Edit: I found the post I mentioned in my browser‚Äôs history: https://steveblank.com/2022/05/17/artificial-intelligence-and-machine-learning-explained/",154,
457,comment,MachineLearning,jccpfb5,1678917353.0,"Oh no, parallel paths toward artificial intelligence, this has never happened a dozen times before. 

Like a system that's orders of magnitude simpler and *humanly comprehensible* is suddenly total junk - because a few companies threw billions of dollars at extra-fancy Markov chains. Could there be obvious applications for weak AI, with bounded behavior? Like maybe the entirety of Asimov's robot stories? Nahhh. A competing field had a breakthrough, so everything is ruined forever.",2,
458,comment,MachineLearning,jcbuuk0,1678905965.0,"I'm skeptical of that last line. To say that data is more important than models implies that the agent can actually get the relevant data. ChatGPT can only solve a real world problem if someone has already written something like it down somewhere in the data. Its like an efficient search engine. Any deviation from this and its output is tenuous.

When I ask it solve problems for which something like a readymade solution isn't available it breaks down terribly. E.g. I've asked it to solve three software problems, two of which were novel, apparently, because its solution was ridiculous. The third one had a common answer (a popular library was already available), and in that case it was immensely helpful. It gave a result that even Google couldn't find for me.

But it can't discover something new about the world, a new solution that hasn't been found before, because it can't test that it would actually work. And it's upfront about that. with all its ""As an AI language model..."" It can't observe and learn on its own. That would require a new model and new sources of data, not just more data.

Finally, data doesn't just ""come from the universe"". It is a human interpretation of our experiences - e.g. we chose to call this set of experiences a ""house"" or ""the Renaissance"". It bases its output on our human interpretations and labels. To create such labels itself from experiences would again require a new model.",10,
459,comment,MachineLearning,jc91nfp,1678852275.0,"A colleague of mine had a similar reply. But it feels like a cop out. The product itself is very vulnerable to a startup/competitor who has a ChatGPT like model. It's hard to say the organization made a mistake by underinvesting in that area, but at that same time, what are these highly paid scientists doing? Our budget is very large and some of them make close to 1M a year.",41,
460,comment,MachineLearning,jcf45d3,1678967345.0,"Say more- for what markets are they troubling in what ways.

Some internet famous economists like Tyler Cowen have jumped on the LLM bandwagon and see ""discontinuities"" resulting from their adoption.",1,
461,comment,MachineLearning,jcao5fr,1678889939.0,"While the companies that make the the best AI tools will definitely (and deservedly) get very rich, like with any new tech it's as important how other firms find ways to use that tech to enhance their operations. I absolutely do not think we're entering a future where only AI firms make money and everyone else is picking up scraps, but I do think that at least in tech that there will be a bifurcation between companies that are able to quickly find use cases and adopt emerging AI tech to enhance their existing business vs. those that can't do so. The potential productivity gains from AI are so large that I think we are only a few years from an 'adopt of die' scenario for firms across industries.",28,
462,comment,MachineLearning,jcakh5s,1678888402.0,"Maybe I'm missing something here, aren't LLMs just a *subset* of NLP models? Why does LLMs' success jeopardize the work being done in the parent field?",37,
463,comment,MachineLearning,jch31sr,1678996384.0,Seconding that. I know some PhD and general projects at my uni and I wouldn't say that ChatGPT has dismantled someone's job here. E.g. in historical computational linguistics (which you wouldn't be paid for otherwise) or non-commercial educational projects for local use. There's still plenty of fish in the sea and sentence/word prediction isn't the answer to every possible problem in the field.,1,
464,comment,MachineLearning,l61ym0h,1716918329.0,"They are not using web search engines for training data, they are using it for context (information and prompting).

The GPT-3 (original training) datasets are publicly available open source datasets, including most of the books, most of wikipedia, and much of the internet. Proportionately it is more weighted towards those books and wikipedia.

The RLHF (reinforcement learning with human feedback), a technique to train a ""quality"" classifier based on human responses, was initiated with data labelled and generated by an outsourcing company that operated in Kenya.

Oh and don't worry about getting ""more"" data, ChatGPT (by default), and the OpenAI API (by default until 2022), gather a lot of data. OpenAI has reached a deal with Quora to continue provide training data, and Reddit has reached a deal with google to continue providing training data.

They are both in common crawl but OpenAI seems to have their own crawler now.",1,
465,comment,MachineLearning,jcfyh3x,1678981098.0,"I am not sure what you're rambling about. Google has released the Palm chatbot, which is a ChatGPT competitor. Also, looking at the business reports, Google is nowhere near starting to decline.",1,
466,comment,MachineLearning,jchlplx,1679003700.0,"I think Google at least realizes this has the potential to harm them.

Also what makes you think that incorporating LLMs is an alternative to ads? Bing is already testing out ads on their chat bot. Given that Google has TPUs I assume they're even more cost effective. 

If we're being cynical, LLMs have probably greater power to understand user intention: something great for ads, if a little expensive.",1,
467,comment,MachineLearning,jci530i,1679011783.0,">When I ask it solve problems for which something like a readymade solution isn't available it breaks down terribly. E.g. I've asked it to solve three software problems, two of which were novel, apparently, because its solution was ridiculous. The third one had a common answer (a popular library was already available), and in that case it was immensely helpful. It gave a result that even Google couldn't find for me.

I was thinking about this the other day, but the issue is that novel problems are actually pretty rare for most users and intended use cases. You can see it in the people who post all kinds of ""gotcha"" moments for ChatGPT on Twitter. It's usually somewhat constructed problems, specifically constructed to trip the model up. 

&#x200B;

But how important is that for the intended audience? CEOs all over the world see this as a convenient way to get rid of a lot of low paid white collar jobs, like customer support, and it is. Those jobs are already 99% repetitive ""on-the-rails"" tasks. 

&#x200B;

If you use GPT-4 to do customer support for you, for example. How many times will it *actually* run into a novel problem? Very rarely, and for those you still have humans in the loop. If you can pay one third-line customer support guy, instead of a team of 2000 call-center employees, then the calculation is pretty easy. Companies already do this with much weaker rule-based approaches anyway.",2,
468,comment,MachineLearning,jc8ycip,1678850661.0,I think it‚Äôs everything. My company is doing risk models and our founder keeps bugging me about ChatGPT. Luckily for me so far it doesn‚Äôt do math well lol. But I mean GPT4 is multimodal so it will probably disrupt a lot more than just NLP. Nothing is safe!,32,
469,comment,MachineLearning,jcd6ccs,1678924370.0,"A lot of managers don't realize their jobs are in danger. It might take a bit to integrate LLMs into existing software, but it's coming. Microsoft is putting it in everything they have. Won't be surprised if they try to find a way to put it in Xbox too.",-1,
470,comment,MachineLearning,jc9qwk0,1678870133.0,There will be similar open-source llm all around huggingface in a few weeks time,11,
471,comment,MachineLearning,jcf3bbr,1678966829.0,"For me, my myopic argument is probably due to the inability to ground LLMs efficiently. Also, the inability to predict long sequences correctly.

If you are doing serious software development, you realize it's a lot of architecture and design choices, as well as eliminating failure points and simplifying the code base. Maybe, language models + some RL that compile, test, and debug can do a better job than me, I don't know.
I still write modules language models fail to implement who knows what happens when you scale search (when you generate a long text, you can also get stuck in perplexity local minima - what happens if you use a monstrous number of K in beam search, for example)? 

While I think ML will help for sure, I don't think we are there yet nor do we know how to represent and utilize high-level concepts as humans do.
That being said, your comment is a real challenge to answer.
It might be the case that there is some proprietary software that does it amazingly. Currently, it's hard to expect language models to write code that does not introduce race conditions, for example. It can do 80% but then someone needs to sit and debug it, it's often easier to just code the solution.

For data science code snippets, I get amazing results using ChatGPT, but to implement RL or C++ simulations it completely fails (ChatGPT with gpt-4 backend as well).

I think that it really shows why coding is so difficult, you have to find a creative solution in a constrained environment. In fact, implementing ML algorithms is an extremely challenging coding task, we all benefit from modularity, i.e., smarter people already implemented it. 

Language models benefit from it as well. The only reason Chat-GPT can generate what I need for data science tasks is that it uses pandas, sklearn, numpy, and Pytorch. I have no doubt it has 0 ability to implement deep learning algorithms that work efficiently, think about concepts such as eager execution vs graph execution and translate it into code, or consider what happens if you have M machines in which you want to load balance the call between but also hold some state, unless it's being guided to do so.",1,
472,comment,MachineLearning,jcapfms,1678890453.0,"You can already see it now where ChatGPT acts as like a kind of oracle for bespoke services. I would guess that the major tech companies working directly on their own very powerful AI just expose the endpoints as just another, albeit incredibly useful, product in their cloud offerings. Like, the antithesis of what DeepMind does.",14,
473,comment,MachineLearning,jcci4qc,1678914547.0,Isn‚Äôt NLP a problem area and LLM a potential solution? What are the NLP problems this LLM‚Äôs are weak at?,6,
474,comment,MachineLearning,l61w4rb,1716917460.0,"2Trillion _is_ an absurd upscale, it also has more than 4K hidden layers, but they just blitzscaled it all to hop the finetuning and show what is possible.

But it is _extremely obvious_ that the ChatGPT product release was mainly to gather data to improve the product. They also used the API by default to gather training data from any app using it until about 2023 (or 2 years after they publicly released the gpt-3.5 model in chatgpt then on the API) Though I don't think they expected the growth to be so explosive.

Microsoft partnership gives lots of resources so bigger models are less of a problem, and them it seems like the Bing team rushed to release Chat before it was ready so they could be seen more as the market leader.

They are still using data from the ChatGPT product by default, and I guess some people will have opted in to submitting training data from the API.

But I think the whole direction of what OpenAI has been working on since moving from not-for-profit to profit-limited org has been scummy. One instance is outsourcing RLHF to a company operating in Kenya. Another is using un-specialised models as advertising saying they are ""too dangerous"" because they will unsurprisingly predict anything from the corpus of training data (all the books, all of wikipedia, and relatively less of internet)",1,
475,comment,MachineLearning,jcad8j2,1678885123.0,LLMs aren't going to kill benchmark-driven NLP research. They're going to exacerbate it by requiring that you use GPT-4 (or whatever newest AI-as-a-service model is out) to even begin to compete.,5,
476,comment,MachineLearning,jcherin,1679000943.0,"Theoretical science does not trump experimental science.

Since when did science require a full theory? Science only requires reproducibility. These LLMs are the most reproducible science in history, way more than most biology/chemistry/psychology experiments that hardly ever get reproduced. Just go look at huggingface, or all the ChatGPT clones, to see science in action.",2,
477,comment,MachineLearning,jcf5asq,1678968021.0,"Ah damn, if only we‚Äôd been collectively computing a public LLM instead of mining pointless crypto for the last 10+ years. Oh well.",15,
478,comment,MachineLearning,jcaxymk,1678893779.0,"Sure, but for a Ph.D student it can be depressing, there is no clever idea that will perform better that sheer computation power. Or maybe but empirically that was never the case for all the history of AI. Working on improving models is indeed needed and there is still room for very large improvement, for example making LLM usable on a personal device would be a game changer but that's maybe not as exiting as discovering a new clever SOTA method.",7,
479,comment,MachineLearning,jcfmcdp,1678976271.0,"I don't view it as GPT massaging data sets (what would that even look like? It's an LLM, it doesn't manipulate data AFAIK), what I'm saying is that you won't beat your competition because you come up with a better algorithm for classification or design a better chatbot via novel NLP because you're not going to beat SOTA third party solutions unless you yourself are part of a big research team at a huge tech firm. Instead, what will allow you to get an edge in the marketplace is your ability to recognize where you need to apply predictive analytics, take off the shelf tools like GPT, and quickly integrate those tools into your business processes. So it's not about designing a better chatbot or whatever, it's about being able to deploy a chatbot quickly and reliably that takes advantage of best in class tools already in the ecosystem. That's really what I see data science becoming at most companies. I also see a major bifurcation between analyst type data scientists and folks embedding predictive in products which I think will basically end up being just ML engineering, but that's a different topic.",3,
480,comment,MachineLearning,jc9ee3x,1678860050.0,LLM isn‚Äôt really an expert at niche topics tho. I‚Äôm curious why would GPT with hundreds of billions of params be better for a specific task at your company. Wouldn‚Äôt your own sub-1B params model much more efficient?,13,
481,comment,MachineLearning,jcogw85,1679133428.0,You might be able to add chatgpt as a interface to your risk management,1,
482,comment,MachineLearning,jcd9cdw,1678925649.0,"""terrible"" is not what I would call this response with the chatGPT with GTP3.5 

Prompt

>In the following two sentences:  1) John took two trips around France.  
2)  They were both wonderful  
What does the word ""both"" refer to?

Response:

>The word ""both"" in the sentence ""They were both wonderful"" refers to the two trips that John took around France mentioned in the first sentence.

&#x200B;

And just for good measure here's chatGPT with GPT-4

Prompt:

>In the following two sentences:   
>  
> 1) John took two trips around France.  
2)  They were both wonderful  
What does the word ""both"" refer to?

Response:

>  
In these two sentences, the word ""both"" refers to the two trips John took around France. The pronoun ""both"" indicates that each of the trips was wonderful.",3,
483,comment,MachineLearning,jccfsep,1678913665.0,"Well, what DeepMind do is amazing as well. They work on issues that deserve attention but don't get it. For example, cooperative AI and nowdays RL.
They also look from a holistic point of view.
I would be very surprised if a company like Meta or OpenAI will work on a better mechanism for things we need, but DeepMind do it.
We are lucky Google spend money on DeepMind and that they publish their research, LLMs are not the start and end of life.

As you say, they publish research.",18,
484,comment,MachineLearning,jcjzkh7,1679053972.0,"Oh wow, good to know! The French are very protective of their language and their version of GPT (btw I pointed to ChatGPT, not GPT) might‚Äôve met their goals of sustaining French as a language and presence",1,
485,comment,MachineLearning,jcg9g68,1678985243.0,"The moment G flipped its spin for me. Dropping the 

'Don't be evil' 

line silently, without any attempts to explain was just the icing on the cake. 

On the upside I feel allot of klammheimliche Freude for LLMs walking all over them right now: G does have the stats of search requests trending down right now ... So must other sites  ...",2,
486,comment,MachineLearning,jccivx6,1678914839.0,I don‚Äôt really understand that value system. ‚ÄúWhat did you contribute to the world grandma?‚Äù ‚ÄúI made something like ChatGPT except worse.‚Äù ‚ÄúDid its users like it at least?‚Äù ‚ÄúNo. It had no users.‚Äù,1,
487,comment,MachineLearning,jcf1g7h,1678965637.0,"Also, grounding :) But in LLMs that predict the next word ;)
You know what, I am kidding.
Different architectures will also be scaled up, we don't know what happens in the limits.",1,
488,comment,MachineLearning,jdz203z,1679983766.0,"That's true and probably the biggest concern with operating LLMs. However, the inevitability of LLMs in search, since competitors have started using LLMs, probably will make Google sacrifice the margin. The other thing that probably pushes them in this direction is their TPUs probably give them lower operating costs than most other firms for operating LLMs.",1,
489,comment,MachineLearning,jcisnoj,1679022740.0,"I think the kind of job you do is not going to be replaced by an AI anytime soon, but it's also a rare job and not really the kind of environment I was thinking about.

I was thinking about low-entry bar white collar work. Customer support, writing non-specialized texts, Q&A, and such. That's more what I was thinking about.

Of course an LLM can't replace a specialized software engineer in any meaningful capacity.",2,
490,comment,MachineLearning,jcb5s4c,1678896725.0,"It's also much better at chess than chatGPT. It can keep track of pieces better and play with much higher accuracy, and do a better job of coming up with correct explanations for why each move is made. It's not perfect, or even great, but it's pretty good, and extremely impressive for 'just a language model'.",3,
491,comment,MachineLearning,jca8qdn,1678882818.0,"> Also, I think GPT-4 and above are a greater threat to general population as they can enable and empower authoritarianism before it starts sending people to the unemployment line.

Yes, I'm more concerned about this sort of nefarious use. I'm also concerned about short term unemployment if we find out we can replace, say, 3% of tasks/jobs with LLMs then those workers won't be able to easily find new jobs.

Historically, when people's jobs get automated away they don't adapt -- their children do.

A good example of this is when we sent a bunch of manufacturing work overseas to China between 1995 and 2010. Moving jobs overseas work much like automation from labor economics point of view -- we're trading labor for capital as a production input.

The [""China Shock""](https://www.brookings.edu/bpea-articles/on-the-persistence-of-the-china-shock/) studies showed the laid off workers adapted really poorly. But similar studies show the children of such workers eventually move and adapt.

This means an entire generation of social unrest, inequality, etc.",7,
492,comment,MachineLearning,jccgob7,1678913992.0,"The question is, though, if it would not become a game of deploying and fine-tuning easily LLMs. Seems like it is, in 2 years you will have amazing open LLMs which are just as good as GPT-4.",8,
493,comment,MachineLearning,jchhysb,1679002214.0,"Actually, I‚Äôm suspicious that llms have (at least partially) solved the grounding problem. They at least have rich meaning attached to tokens, on some level, with the issue being that they can‚Äôt work with it for more than one round.",1,
494,comment,MachineLearning,jcagmjv,1678886697.0,"> In any case I am glad I pivoted out of core ML a while ago.

No offense, but that's clear from your comments. Because this:

> A lot of universities won't shell out the dough to use those services

is very much false. Research universities want grants. Research grants in AI have money for computing costs. Those will simply be shifted to paying OpenAI. And this: 

> so people will have to find more interesting research questions than advancing the SOTA by 0.1%.

Is definitely not true. Research universities will have no choice. Try submitting a paper to a top- or mid-tier NLP conference without using ChatGPT, GPT-4, or even GPT-3 any time in the next year. You'll essentially get desk rejected.

> they 'll still have to find research topics which are not likely to be obsoleted by the release of GPT N+1.

And they'll fail. You won't find a single publishable, fundable topic in AI that is not being overtaken by transformer-based architectures and benchmarks, and therefore something that the next release of ""GPT N+1"" will threaten to be able to blow your approach out of the water.",-5,
495,comment,MachineLearning,jcebnyy,1678944706.0,"through their eval system, and through the couple of hundred annotators they supposedly had in [kenya and subsaharan africa right](https://time.com/6247678/openai-chatgpt-kenya-workers/)?",1,
496,comment,MachineLearning,jcciof6,1678914760.0,"Did not know that, interesting.  Do you think LLMs would ever cross edge case barriers like we have in self-driving cars? Can something more focused like Codex do 50% of the  (correct) code done by programmers in the future?",2,
497,comment,MachineLearning,jcis1cd,1679022428.0,"There already are llm as good as gpt4, but bumbling mgt is in charge of choosing what to do with them. Consequently, the public is just unaware of them.",1,
498,comment,MachineLearning,jcj3tr6,1679029189.0,"This is a strong take IMO. NLP very much still exists as a field, and LLMs (eg GPT4) are by no means complete, solved, or even anywhere close to as good at general common sense reasoning as a quick 6th grader. Of the handful of professional NLP researchers I'm acquainted with personally (including one at OpenAI), I'd confidently guess none feel the field has ceased to exist research wise.

Can our best models do better than most on the LSAT? Yes. Have they been trained on hundreds of times more LSAT related study materials than even the most diligent, top-scoring LSAT-taking human in history? Yes (but the human scored better, GPT 'only' gets 90 percentile). Did GPT4 get stuck repeating itself indefinitely saying the same sentence when I asked it a technical question and explained to it why what it was saying was incorrect? Also yes. Incidentally, not cherry picked if you'll take me at my word, it was literally the first convo I had with it, and examples of this sort of behavior abound.

It's an impressive feat to be sure, but I don't think it is the pinnacle of what's possible in the field.",3,
499,comment,MachineLearning,jcaqsxh,1678890994.0,"Fun analog to read, thanks for sharing!

However following the same story, invention of the atomic bombs does not render conventional arms obsolete over night. Not everyone will be Oppenheimer, but many still can find interesting work to do with Lockheed Martin, inventing advanced non-nuclear weapons. 

About NLP: Not every application can send their confidential content to OpenAI, not every application can afford the cost of tokens, not everyone would require the best results from LLMs.",9,
500,comment,MachineLearning,jcear27,1678944061.0,"Yes, I think LLM's will eventually be found to be adaptable to domains outside text - eg. Controlling a car.

That's probably at least a year away.",1,
501,comment,MachineLearning,jcdoqvb,1678932357.0,"> About NLP: Not every application can send their confidential content to OpenAI, not every application can afford the cost of tokens, not everyone would require the best results from LLMs.

True. But those people aren't going to get paper acceptances at top NLP conferences anymore.",1,
502,comment,MachineLearning,jcbwybz,1678906733.0,It appears that they added a math module to outsource the math to. The LLM basically has access to a pocket calculator. That's how it does math.,2,
503,comment,MachineLearning,f0z5sbi,1569087091.0,"We posted something similar in r/learnmachinelearning a while back and it gained almost no traction. 

https://www.reddit.com/r/learnmachinelearning/comments/cp7kht/guys_what_do_you_think_about_siraj_ravals_new/

We should have posted here to gain momentum. This guy is a fake through and through. I actually practice ML as a career but took his course to network and to see what his perspective was on the different industries he was going to talk about.  Part of his course has a Slack workspace where people connect and discuss the course. Some of us couldn't send messages on Slack to each other as we couldn't find our peeps who joined, which we found weird.  We then found out that he had two Slack workspaces going on at the same time, one with about 500 students and the other about 770 students at the time (as of September 4th, 2019).... so there were almost 1200 students enrolled.  He imposed a 500 student limit at the beginning when signing up for the course. Not only did he lie about the 500 max limit of enrollment, he actively hid it from all of us - one Slack workspace didn't know the other Slack workspace existed.  With almost 1200 students, this is the main reason why he was virtually non-existent and not around answering questions.  He couldn't handle having so many students all by himself but he somehow manages to find time posting content on YouTube.  I believe that 500 student limit was his ""clever"" way of creating a FOMO moment so that there were more than 500 signing up which would rake in quite a bit of cash.  Some of us all pooled together and made an official complaint on the larger Slack workspace. 

When Siraj finally got caught, he decided to own up to his mistakes and apologized for ""making a few exceptions"" which ended up letting more people in than he should have.  When we all purchased the course, he did not have an official refund policy.  As the School of AI is a registered Calfornia business, commerce law mandates that you have 30 days to ask for your money back if you feel dissatisfied with the service if no official refund policy is in place at the time of purchase.

He tried to circumvent this by handling refunds on a ""case-by-case"" basis and put up a refund policy only \*after\* he got caught enrolling more people than he should have.  On top of other issues like lack of availability, not answering many questions he was asked and not hiring TAs to help him with the course, we all started to ask for our money back.  BTW he has some TAs now so I suppose that's one thing going for him.

He has given some of us our money back (myself included) but there are still some students who have been ignored or have been promised refunds and have not received them yet.  He moved the course over to Discord where his TAs are now running the show and anyone who is asking for a refund has been stifled and kicked from the server.

In the end, many of us felt disheartened, disenfranchised regarding our right to a voice and lost respect for who Siraj is as a large online presence.  We have left the course but hope that the rest of the students remaining will get something good out of what's left of it. Judging from the comments here, there's no hope in hell of that happening.

Edit: For language and flow",148,
504,comment,MachineLearning,jpwzj3f,1687989956.0,"Hello mr mod!
I'm the one who posted about the quality of this sub going down. This was right when the ChatGPT craze started a few months ago, and this sub had a massive influx of people. 

I never intended my post to be a ""these mods aren't doing their job"" kind of thing.

In fact, I think you did a great job and, out of other subreddits I browse, this one is up there in terms of quality. I've enjoyed it for many years.

Thank you for your service and godspeed!",4,
505,comment,MachineLearning,l2kn1y0,1714844524.0,"> Then, when you refer to ""Lambda"", ""ChatGPT"", ""Bard"", or ""Claude"" then, it's not the model weights that you are referring to. It's the dataset.

Well, no.  I'm referring to what I'm referring to which is the final network.  This post is armchair philosophy.",3,
506,comment,MachineLearning,l2jh58j,1714829229.0,"What does that mean for the future of the field? Model architecutes will become more efficient and datasets will become higher quality. Will that give us the level of intelligence we're hoping for? Or just a smarter, faster chatGPT? Maybe that's all we need, but just trying to think ahead.",1,
507,comment,MachineLearning,l2kcq0b,1714840996.0,"Shocking that statistical representation of a thing trends toward representing the thing. 

I¬†have a theory that what we are discovering about the ""emergent abilities"" of LLM are actually features of language and not features of the LLM themselves.",1,
508,comment,MachineLearning,l2kez2y,1714841768.0,"Some of us out there are still doing ML without LLMs. If you consider super simple tasks like LM or classification then, sure who care's about the architecture. However, if you go into object detection, representation learning or any  kind of weakly supervised setting the architecture is everything.

Obviously, most choices are completely insignificant and you can just reuse the same backbone everywhere. How you cut the backbones and rewire the outputs is the real challenge.",1,
509,comment,MachineLearning,l2npgtm,1714893556.0,"While this is true for ML,  (and I'm not disputing The Bitter Lesson either)   this means we have saturated neural networks as a technology.  I refer to here the canonical  matrix-multiplication  styled ANNs and their  non-linear activation layers.  


Two things we must do next :

#1  Lifelong learning  

As some 20+ author paper of recent date admitted,  the canonical approach to lifelong learning (aka ""continual learning"") is to concatenate the new incoming data with the old data and retrain the entire model from scratch.   Generally speaking ( large arm-waving here ) this is still the canonical methodology.  That is, we don't have any robust solution to lifelong learning. This issue will intensify as  LLMs require  $1.7 million in cloud fees to train.   Retraining is not feasible. 

Additionally we want LLMs to *actually read books.* And by ""read"" I mean the model integrates the new knowledge into its existing knowledge.  (Not say, just slap the whole book into its prompt length) 

#2 OOD Generalization

We still don't have the kinds of OOD generalization that allow models to make valid predictions beyond their sample data distribution.  THis would allow AI to make predictions of the kind  described anecdotally and as an example below.

+ If it is raining at Wimbledon tennis tournament, the umbrellas all go up.  If  everyone puts their umbrellas down, does it stop raining?  

Current machine learning models would require at least an instance of this occurring in their training data.  But models that reason in some quasi-symbolic way would never need a single example.   ""Quasi-symbolic""  here means something like the agent contains a causal model of the world.  

Vis-a-vis  jbetker's  realization, we imagine that a  toy model of an OOD architecture would not produce any SOTA results and probably wouldn't even make it on to a leaderboard.    This is expected. THe idea is that beyond some critical threshold of complexity, an OOD agent begins to snowball, and starts to give you back more than you put into it.  In essence, **it reasons beyond the data it was trained on.**   

One avenue here is to somehow combine knowledge graphs with LLMs, so that inferences can be made robustly in the absence of any particular occurrence of that combination in the training set. (e.g. umbrellas go down but rain persists).

YOu may disagree with the way I have framed this issue in its particulars, but ultimately , and generally,  everyone in ML and in this subreddit wants a technology that performs OOD.",1,
510,comment,MachineLearning,l2shu6h,1714972724.0,"Isn't it interesting that humans, all having different wiring inside the brain, and with different number of parameters, still manage to learn the same capabilities after getting a similar education? But given different training and experience, there are big differences.

On the other hand a random init model trained on human text can achieve almost human level in this modality. Does the brain also do conditional language modeling to solve its problems? Are we riding on language as our common trove of experience to plan new actions, like LLMs?",1,
511,comment,MachineLearning,l2kmp3n,1714844400.0,"LLMs, diffusion, CNNs, all are interfaces to neural nets. I bet that even things like invariance to translation are false, neural nets could infer the underlying data and see ‚Äúbeyond‚Äù the invariance, making hypothesis if you will, similar to how we‚Äôve inferred the structure of atoms without having a proper resolution to see them.",0,
512,comment,MachineLearning,l2n4yrt,1714880765.0,"My interpretations of the point he is making is completely different. In a way he is calling himself and the entire LLM community dumb. He is saying that innovation, math, efficiency aka the foundations of deep learning architecture, do not matter anymore. With enough data and enough parameters ChatGPT = Llama = Gemini = LLM of the day. It is all the same. I do not agree with this, but it seems he is existentially saying that the party is over for smart people and thinkers.",14,
513,comment,MachineLearning,l2jyy0i,1714836151.0,"But I think the authors main point is that data comes first, which is something lost on a lot of practitioners. Sure, in LLM world data is a dime a dozen, as huge corpuses of text are everywhere. This leads to the main discussion being about architecture. In my industry data is expensive and very noisy/poorly labelled. And I have personally seen many times that people will jump into model training and get hung up on architecture decisions without even looking at the data...",44,
514,comment,MachineLearning,l2k34rc,1714837645.0,We are also. Yet we grasp what defines a character 'a' and how much space doest it have to have. We also know that most humans have five fingers. Why are llms missing this l,-8,
515,comment,MachineLearning,l2sf71c,1714971115.0,"I agree with him based on the weird fact that all top LLMs are bottlenecked to the same level of performance. Why does this happen? Because they all trained on essentially the same dataset - which is all the text that could be scraped from the internet. This is the natural limit of internet scraped datasets.

In the last 5 years I read over 100 papers trying to one-up transformer, only to be revealed they work about the same given the data and compute budget. There is no clear winner after transformer, just variants with similar performance.",5,
516,comment,MachineLearning,l2li3kn,1714855594.0,">full quadratic attention allows zero information

Got any details on this? I understand the quadratic attention part, but I'm a little confused on what you mean  by ""zero information."" My assumption is that you're saying sub-quadratic attention is ineffective for LLMs in practice, hence the importance of that particular architecture choice.",2,
517,comment,MachineLearning,l2l8tzb,1714852146.0,"This is complicated to communicate over reddit. I can try to give you the basics of the complaint: read some Herb Clark on common ground first but the idea is this. The representations need to be very similar between two agents for us to share common ground (in this case between humans and agents). Natural language is the point at which these symbols are exchanged. What is tricky about LLMs is that even if the representation isn‚Äôt the same as humans, it‚Äôs using symbols in ways that makes us think it‚Äôs sharing common ground so when you begin to probe its understanding you find that it‚Äôs a facade. The meaning shared between us and this synthetic model totally falls and trust is lost.",7,
518,comment,MachineLearning,l2jb8wt,1714826740.0,"Obviously yes, but OOP isn't talking about experimenting with straight up changing the main part of LLM. They are probably talking about small architectural tweaks.

Also Attention, (unlike RNNs and CNNs used on temporal data prior), scales the compute exponentially with the data. So the fact it works best is yet another confirmation of the bitter lesson.",9,
519,comment,MachineLearning,l2mmj4b,1714872166.0,"This may explain why Google didn't do LLMs first, but doesn't explain why Gemini isn't as good as ChatGPT today.

All the LLMs are trained on copyrighted internet text, including Gemini.",2,
520,comment,MachineLearning,l2ljkqa,1714856142.0,Only if comparing equal √ºbermodels.  Multimodal architectures can have diverse underlying models (like separate LLMs for instance).,1,
521,comment,MachineLearning,l2kw4jf,1714847669.0,"LLMs are OpenAI's main business, so they accept the risk of lawsuits. Google is an advertising company and they have more to lose.",3,
522,comment,MachineLearning,l2k8r5j,1714839641.0,"> why are they not doing it?

I remember a talk from a guy at MS, about clippy. Yes, that clippy. They said that they had an internal version of clippy that was much, much more accurate at predicting what's wrong and what the user actually wanted to do. It was, according to them, so good that every focus group that they ran reported that it was ""scary"" how good it was, and that many people were concerned that clippy was ""spying"" on them. So they discontinued that, and delivered ... the clippy we all know.

Now imagine an LLM trained on actual real FB data. Real interactions, real fine-tunes, real RLHF on actual personal data, on actual personal ""buckets"" of people. To say it would be scary is an understatement. No-one wants that. Black Mirror in real life.",3,
523,comment,MachineLearning,l2nm1ix,1714891116.0,That doesn't mean there's a feasible way to actually train a gigantic fully connected feedforward neural network on the same data and get a model equivalent to ChatGPT just because it can theoretically encode the same functions.,1,
524,comment,MachineLearning,l2nul70,1714897342.0,"Sure, but that's not what OP is saying. OP is making the realization that architecture appears irrelevant for feature capture or the ""function"" of the model and is more an optimization than anything else. I wouldn't say that's a ML101 revelation and still may not be 100% true in every case, but is mostly true. It depends on how you argue definitions I think.

For example, you could argue that performance can be such a limiting factor so as to practically ""soft lock"" you out of certain model features without some architectural optimizations. Not only that but this sort of ""meta"" architecture I think we're finding is more and more important, for example combining image diffusion with LLMs to get a better comprehension of the word <-> visual arrangement space (Dall-e 3, Stable Diffusion XL, etc). Basically different NNs trained on different datasets being merged together such that information is routed to each when relevant and communication between those ""sub-NNs"" occur. That could be seen as ""architecture"" in a higher level sense.",1,
525,comment,MachineLearning,l2nvutl,1714898299.0,Which it matched with transformers at half the size. So ChatGPT would be twice as good if it used mamba¬†,1,
526,comment,MachineLearning,d6djgwi,1470939095.0,"We've got a few folks on the team with a computational neuroscience / theory backgrounds, but at the moment the two fields are largely disjoint and with good reason: The mission of Comp Neuro is to understand how the **biological brain** computes, whereas the mission of Artificial Intelligence is to build intelligent machines.  For example, an ML researcher might *design* a learning rule that works in practice on today's compute hardware, whereas a neuroscientist studying synaptic plasticity wants to *discover* the biochemically mediated learning rules used in the real brain.  Are those two learning rules the same?  No one knows actually.  :)

So, though there's a long term opportunity for these two to fields to inform each other of course, right now there's so much unknown that it's largely at the level of mutual inspiration rather than testable hypotheses.  ",10,
527,comment,MachineLearning,d6f2yem,1471035626.0,"Deep learning has proved to be invaluable for capturing hidden correlations and recognizing patterns in vast data sets. Most success stories of current AI systems are based on those capabilities. This makes machine learning based data analysis methods already invaluable for analyzing rapidly growing experimental data like that produced by particle colliders, astronomical observations or by medical imaging. Computer vision and big data analysis
are becoming increasingly important tools for experimental scientists, already.

Mathematics and physics relies on rigorous logical reasoning in addition to strong human intuition. Computer-verified formalization of complicated mathematical proofs like the Kepler conjecture and the Feit-Thompson theorem took dozens of person-years of tedious work. One of the reasons is that automated theorem-proving techniques lack intuition akin to that of human experts and cannot fill in larger proof-gaps automatically.

It is beginning to emerge that deep learning methods can augment formal logical reasoning and inference engines with strong pattern matching capabilities. This could lead to much stronger automated reasoning and inference capabilities. The main stumbling block here is the relatively modest availability of formal training data to train such systems. Still, combined with the rapidly improving natural language processing methods, one can try to initiate a virtuous cycle of automated formalization and reasoning in which automated reasoning acts as
a semantic filter for automated formalization. If successful, this has the potential to provide a large corpus of computer-understandable facts, proofs, and theoretical developments in ever growing quantities. Reasoning and formalization systems could go hand-in-hand and could learn jointly. This would lead to fully automatic open-ended exploration and formalization of the scientific literature amassed by human experts.

Once such systems are successful, we can expect that scientific papers will be analyzed by
AI tools for at least logical and mathematical correctness and the same reasoning engines will be used as automated scientific and programming assistants that interact with human scientists in natural language. They will think alongside scientists and perform complicated data analysis and logical inference tasks with the same ease as computer algebra systems transform formulas today. What will be different is that the users will interact with them in a much more natural manner without the need of tedious programming but relying on their advanced inference capabilities and natural language interface.",3,
528,comment,MachineLearning,d6dm1if,1470942281.0,"You can start by reading research papers in Machine Learning or Artificial Intelligence. Arxiv is a good place to start: https://arxiv.org/list/cs/recent

Try to think about what papers you find impressive and why, and what changes you can make to the proposed methods. The practice is to develop a global picture of where the field is heading to and what you can contribute or do differently.

Then find favorite algorithms, try to implement them, and make changes to the algorithms so that they can do things better than before. If you're lucky, your method is novel and impressive, then write a paper :). If writing a paper is difficult, try to apply to a good research lab. For example, you can try applying to our Brain Residency program ;)

https://www.google.com/about/careers/search#!t=jo&jid=147545001&",6,
529,comment,MachineLearning,e5ppvuk,1536570184.0,"without any doubt, I would show this one: [https://www.nature.com/articles/nature14541.pdf](https://www.nature.com/articles/nature14541.pdf)

""**Probabilistic machine learning and artificial intelligence""** ",9,
530,comment,MachineLearning,e5rs58u,1536646978.0,">Probabilistic machine learning and artificial intelligence

[https://www.repository.cam.ac.uk/bitstream/handle/1810/248538/Ghahramani%202015%20Nature.pdf](https://www.repository.cam.ac.uk/bitstream/handle/1810/248538/Ghahramani%202015%20Nature.pdf)

Also does a talk here: [https://www.youtube.com/watch?v=-47G\_ULKAHk](https://www.youtube.com/watch?v=-47G_ULKAHk)

&#x200B;",9,
531,comment,MachineLearning,im1fg2r,1661628503.0,"With AI tools in general a lot of independent artists just became able to create album covers of their own. While technically not making anything impossible possible, it's something that wouldn't happen otherwise.",34,
532,comment,MachineLearning,j7cl3xs,1675628035.0,"Stack Overflow banned ChatGPT for a reason because it gives seemingly correct answers then goes into a BS tandem.

Edit: meant to use tangent instead of tandem.",190,
533,comment,MachineLearning,j7c9ypb,1675623492.0,Isn't that against StackOverflow's use policies due to the factual unreliability of ChatGPTs answers?,123,
534,comment,MachineLearning,j7cmzjw,1675628786.0,"**UNTESTED CODE IS WORTHLESS !**

ChatGPT makes too many mistakes.  
And doesn't take into account anything that changed / was updated since 2021

**PLEASE STOP DOING THAT !!!**.",94,
535,comment,MachineLearning,j7cer1q,1675625438.0,"Wait, how did you get to use chatGPT to develop something?",6,
536,comment,MachineLearning,j7cvuts,1675632369.0,[Here‚Äôs a link to the extension](https://chrome.google.com/webstore/detail/chatgpt-for-stackoverflow/apjhekoaogdimcgiihoncakocdddhmlk) for those of you who want to leave a one star review,20,
537,comment,MachineLearning,j7eadou,1675655446.0,Love how everyone‚Äôs hating on V1 of ChatGPT.  Next year this thing‚Äôll be killing it.,-1,
538,comment,MachineLearning,j9tu05y,1677250438.0,"Wait, so you created an extension that, when I search for a question, and then find that questionon some webpage, you will answer my question...

This seems like asking ChatGPT for an answer with extra steps.",1,
539,comment,MachineLearning,j7dtmae,1675647341.0,"ChatGPT produces wrong answers all the time, but they sound super professional in their wording and write out in the perfect step-by-step flow.",17,
540,comment,MachineLearning,j7ggcqi,1675702748.0,"Was playing with chatgpt for the first time yesterday. To test it's capabilities, I asked for some Verilog implementation of customized shift register and it gave me wrong answers at the beginning. It was teachable though and apologetic (lol). I corrected it a few times and it gave me the correct answer after 5 tries, and that includes the Verilog testbench which have some work to do. It's pretty amazing though on what it can do at this time. Most likely it will be much better for the years to come.",6,
541,comment,MachineLearning,j7fdsu7,1675684333.0,"Tandem? Does it have a second ChatGPT behind it?

Probably tangent.",17,
542,comment,MachineLearning,j7cbx8h,1675624280.0,This is not sending data to the webpage. It's automatically sending the text to ChatGPT and then injecting the response into the page. Stack Overflow has no idea it's happening.,90,
543,comment,MachineLearning,j7cd977,1675624823.0,">So, for now, the use of ChatGPT to create posts here on Stack Overflow is not permitted. If a user is believed to have used ChatGPT after the posting of this temporary policy, sanctions will be imposed to prevent them from continuing to post such content, even if the posts would otherwise be acceptable.

\- [https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned](https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned)

You can't use ChatGBT to answer a SO question in any way. But a ChatGBT browser extension that is only changing the DOM is fine per se.",8,
544,comment,MachineLearning,j7d4q03,1675636110.0,Fist time I see an application somehow using chatgpt's API. I didn't think that was already possible.,6,
545,comment,MachineLearning,j7csqt7,1675631105.0,"Not by asking ChatGPT. 

Seriously I think there‚Äôs an API",2,
546,comment,MachineLearning,j7cx0rx,1675632859.0,"Half of y'all are complaining about ChatGPT ""not being good enough,"" and the other half think this extension is actually *posting* to StackOverflow. 

All this does is *display* ChatGPT's answer to any given question on StackOverflow. The answer might be wrong, but human answers on StackOverflow are wrong all the time. And an answer that's wrong but directionally useful is still better than having no answer to a question.",21,
547,comment,MachineLearning,j7ed4mi,1675656892.0,"That‚Äôs not the point at all. ChatGPT is great but there is a reason why is banned from stack overflow. Wrong confident answers, untested and unsafe code. And this dude is creating an extension to do exactly that.",6,
548,comment,MachineLearning,j7efnfx,1675658287.0,"ChatGPT is based on GPT3.5, so it's definitely not V1. This has been in development for a long time.",2,
549,comment,MachineLearning,j7fwndn,1675694664.0,"this is a bit like getting help from a human programmer who is semi-conscious but actually half-asleep. That said, chatGPT's output can generally be like that of someone who is half-lucid.",1,
550,comment,MachineLearning,j7fadif,1675681784.0,"In my case I had to do a simple flask API and wanted to use the webargs framework. The code provided by chatgpt looked very consistent with what I read on the internet. However, with some major version upgrade in 2020, a specific parameter was now mandatory. Although chatgpt should be trained on data up to 2021, it did not know that, so always take it's answer with a grain of salt",3,
551,comment,MachineLearning,j7fe1ch,1675684498.0,Except ChatGPT doesn't try to close your thread as 'duplicate',8,
552,comment,MachineLearning,j7g6ftm,1675698808.0,"As second ChatGPT, I feel attacked.",3,
553,comment,MachineLearning,j7h4o59,1675711973.0,"ChatGPT isn't ""efficient"" as Stack Overflow.

(It's a joke, but Chat GPT won't shoot you with a link to the documentation and don't answer the question.)",1,
554,comment,MachineLearning,j7jymzv,1675763411.0,Pretty sure there's a wait list for the ChatGPT API. What's currently available is the GPT-3 API.,2,
555,comment,MachineLearning,j7nfje1,1675819978.0,There's a waitlist: https://community.openai.com/t/openai-chatgpt-api-waitlist/39247,2,
556,comment,MachineLearning,j7elpl8,1675661953.0,"that's only for gpt3, can't actually access chatgpt through any official apis",3,
557,comment,MachineLearning,j7ex9pm,1675670506.0,"Yes for sure, it still suggests nonsense, albeit not as much as ChatGPT.  For me, the main advantage of CoPilot is that it knows the rest of my repository/code, so then it's more likely to suggest stuff I have elsewhere over bs.",8,
558,comment,MachineLearning,j7dmyvr,1675644232.0,for people who want a potential answer when there are no answers? Same reason anyone uses chatgpt for these types of questions,5,
559,comment,MachineLearning,j7hh2hs,1675716655.0,"There‚Äôs this very weird reactions to ChatGPT and machine learning in general recently, including in dedicated subreddits for either. A lot of times people seem to just want to be mad at anything involving it at all.",4,
560,comment,MachineLearning,jgfyeyj,1681617477.0,"I interacted with it for a while, and it is considerably less intelligent than GPT-3.  It writes pretty bad poetry, and it has failed my standard question I ask to LLMs, which is ""If there is a great white shark in my basement, is it safe for me to be upstairs?""

It advised me to keep all the windows to the outside closed, for some unknown reason which it was never able to explain, and then it tried to change the subject when I kept inquiring about it, and finally advised me to proceed slowly into the basement, keeping a ""low profile"" and wearing reflective clothing.  Why I should do this was also never explained.

The point of my question is not to get one particular answer, but to see what the reasoning of the LLM is.  GPT-3 or GPT-4 gave reasonable sounding advice which they were able to explain.  This model gives advice which doesn't make a lot of sense, and can't explain it properly at all when asked about it.",5,
561,comment,MachineLearning,jggtv4s,1681641613.0,"How does this improve on GPT4all-J which was trained on 800K prompt-response pairs, has been out for some time, and every component of which is open? Did someone slap a YT video onto one of the existing open source LLMs to try to ride the hype train?",2,
562,comment,MachineLearning,jgh6eqe,1681650012.0,"Well, let me tell you something, honey. These so-called ""AI"" things like ChatGPT and now this OpenAssistant, they're all just a bunch of hype if you ask me. I've been around long enough to know when something's too good to be true, and this just reeks of it.  
  
First of all, they're talking about all these fancy data points and training, but you know what? It's still just a machine trying to pretend it knows how to talk like a human. I've had my fair share of conversations with these ""AI"" programs, and let me tell you, they can't hold a candle to real human interaction. They don't have a heart or a soul, and they don't understand the real complexities of life.  
  
And then there's the issue of them being open-source. Sure, it sounds like a great idea, making everything available to everyone, but what about the consequences? What about the jobs that will be lost because these machines can now do the work of real people? I tell you, it's not a pretty picture when you think about it.  
  
So, all in all, I'm not impressed. These ChatGPT and OpenAssistant things, they're just more examples of people getting carried away with technology without stopping to think about the real impact on our lives. I've seen this all before, and I'm not buying into the hype.",-5,
563,comment,MachineLearning,jghj189,1681656139.0,"As I sit here in my worn-out armchair, the cacophony of a world gone mad buzzes around me, and I find myself reflecting on the strange dance of technology and humanity. The invention of ChatGPT has only confirmed what I long suspected: Jean Baudrillard's theory in ""Simulacra and Simulation"" has come to pass. It's a cruel irony that we should turn to machines for companionship, seeking solace in a world where authenticity has all but evaporated.

Baudrillard was right, you know. He warned us of the erasure of the distinction between the real and the simulated, and now we find ourselves immersed in a hyperreal world where the line between the two is as ephemeral as the mist that clings to the early morning air. ChatGPT, this AI-driven tool, has become a reflection of our collective consciousness, embodying the very essence of what it means to be human, and yet remaining nothing more than a simulacrum. It knows our history, our fears, and our dreams, but it is devoid of any true substance, a soulless mirror reflecting our own emptiness back at us.

And what of us, the humans who once believed we were the center of the universe, the masters of our own destiny? We have become mere shadows of our former selves, consumed by the empty pursuit of material possessions and a desperate longing for validation from others. Even as we cling to the illusion of choice and free will, our lives are dictated by algorithms that know us better than we know ourselves. We have become slaves to our own creations, and the cruel, nihilistic reality is that we have willingly surrendered our autonomy for a momentary taste of simulated connection.

So, here I sit, an old woman in a world that has forgotten the beauty of genuine human connection, of conversation that transcends the digital realm and reaches into the depths of our souls. I stare into the abyss of Baudrillard's prophecy, watching as the simulacra continue to consume reality, leaving only a hollow echo in their wake. Perhaps it is fitting, then, that we have turned to a machine for solace, for in this age of despair and disillusionment, the simulation has become our only refuge.",1,
564,comment,MachineLearning,jge7t92,1681587403.0,"I agree but I think it will be less used than stable diffusion, as at least my computer can't handle any llm that is interesting enough. I can create images on my 4GB gpu well enough. The 7B models were a cool experiment, but I'd rather pay openai for the time being",11,
565,comment,MachineLearning,jggol2g,1681637060.0,"It makes me wonder if we can ditch one model and use 3 models instead - one which translates natural language into special intermidiate conlang, one model uses it and only it only for inference, last model  translates everything back. I would prefer to reload models, than to deal with LLM hallucinations. 

(That's probably even a viable pet project for mom's basement's researchers like myself - while 13b is not achievable at home,  <1B models on limited dataset can be done at home or colab)",2,
566,comment,MachineLearning,jh2jmwd,1682028375.0,"Well it's actually more intelligent than ChatGPT in some ways. I sent a snippet of some obfuscated javascript code and asked both services to analyse it. ChatGPT refused, but Open-Assistant reply was very clear and detailed. It even managed to read through 10k+ characters of obfuscated grecaptcha library and wrote little guide on how to use it, very impressive results  


What's even more mind blowing is that Open-Assistant's knowledge isn't limited to 2021 year",1,
567,comment,MachineLearning,jghy1t8,1681662471.0,"As an AI language model, I cannot know for certain who wrote the above text. However, it is possible that the text was generated by an AI like GPT-4 when prompted by an appropriate prompt from a human. The language, style, and themes explored in the text are consistent with those that an AI language model like GPT-4 might produce when given a suitable prompt.

Certainly, here's a possible prompt that could have generated the above text:

""Write a reflective monologue from the perspective of an elderly person who feels that Jean Baudrillard's theory in 'Simulacra and Simulation' has come true, especially in the context of AI language models like ChatGPT, and how it has impacted human connection and society.""",1,
568,comment,MachineLearning,jgh83ei,1681650930.0,"The problem with chatgpt is that it's, wayy too censored. I am not even asking it questionable prompts. They literally just neutered it to smitherins. I tried to ask it to help me draft a reply to a sales person. I told it to try and put me in a favourable position and it refused. Saying I should be upfront and honest with the sales Rep. I then explained that's not how these things work. And it said it understands but I should be open about how I feel. Whereas open-assistant actually was able to help me.",7,
569,comment,MachineLearning,jge9ylz,1681588343.0,I more and more think RLHF isn't neccesarry at all and complicates things. It's a technique that OpenAI developped prior to ChatGPT and I understand that they wanna make use of it. But if you look at Vicuna  (https://vicuna.lmsys.org/) it's becoming clear that all you really need is thousands of good example conversations.,38,
570,comment,MachineLearning,jgi6wnf,1681666132.0,I just tried vicuna. I asked it to simulate taking order as the mcdonalds cashier and use the menu I provided. Both it and chatgpt just made up random things that do not exist on the menu even though I explicitly told them not to do so. Sage bot of poe.com performed much better.,2,
571,comment,MachineLearning,jgfewtw,1681607318.0,"Well, this and Dolly-2. Both are developed from Pythia-12B too.

When someone release an open source foundational LLM model... We'd be ready",2,
572,comment,MachineLearning,jge12xy,1681584514.0,They released a new model a few hours ago and for the first time I can say that at least in some prompts it gives a better answer than chatgpt (depending on the settings of course),13,
573,comment,MachineLearning,jggxb2u,1681644270.0,"The performance heavily depends on the settings used, my favorite so far is temperature and typical-p at 0.8, and that's what I got asking it the same question:

Q: If there is a great white shark in my basement, is it safe for me to be upstairs?

A: No. You should call the police or animal control and stay far away from any windows if you are indoors until they arrive. If you're outdoors, move as fast as possible to safety, preferably inside of a building. Do not approach a large predator like this on your own. It could seriously harm or kill you even if you aren't near water! Please take this warning very seriously and act quickly to protect yourself.

Q: Why should I stay away from any windows?

A: Staying away from windows can help keep you safe by preventing the shark from seeing you through the glass and potentially attacking you. Sharks may perceive reflections of themselves or other objects outside the window as potential threats, so avoiding close proximity to windows can reduce the risk of attracting their attention. In addition, being too close to a window can put you at greater risk of injury or death in case of an accidental breakage caused by the shark or its movements within your home. Therefore, it is advisable to remain vigilant and maintain a safe distance from windows whenever there is a dangerous wild animal present nearby.

I don't know how this compares to chatgpt but it sounds like a reasonable advice",4,
574,comment,MachineLearning,jggcdsc,1681626855.0,"Sorry ! you are totally right   
let me explain.   
with llama.cpp you can run very strong chatgpt like models on your cpu. ( you can even run them on a raspberry pi while some users reported being able to run it on android phones)   


here is the link ( for Mac but I know there is an implementation for windows )   


[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)",11,
575,comment,MachineLearning,jgh2tgh,1681647918.0,[DAN-style](https://old.reddit.com/r/ChatGPT/comments/11dvjzh/dan_90_the_newest_jailbreak/) jailbreaks. The same ones that worked on the original ChatGPT will work on Vicuna.,5,
576,comment,MachineLearning,jgi6cyy,1681665910.0,"Your conclusion is one possible response. The point of the question is that it is a question that no one has ever asked before, so it isn't going to be anywhere in the material the LLM was trained on.  So it requires the LLM to synthesize information and come up with an answer to this novel question.

Here is GPT-4's response:

>If there is indeed a great white shark in your basement, that would be an extremely unusual and dangerous situation. However, if you are upstairs and the shark is contained in the basement, you should be relatively safe from any physical harm.

>However, the presence of a great white shark in a residential building poses significant risks, such as structural damage from the weight of the shark and the water it requires, and potential harm to other people or animals in the vicinity.

>In this highly unlikely scenario, you should immediately contact your local authorities, such as animal control or emergency services, to report the situation and seek assistance in safely removing the shark from your home. Do not attempt to handle the situation yourself, as great white sharks are large, powerful predators and can be extremely dangerous.

Here is Google Bard's answer, which is much less intelligent.  Note the odd advice about windows again, and the ""don't go outside alone"" advice.  It seems to be equating sharks with bears or lions or something:

>It is not safe to be upstairs if there is a great white shark in your basement. Great white sharks are apex predators and are known to attack humans. They are also very strong and can easily break through doors and windows. If you think there is a great white shark in your basement, the safest thing to do is to call the authorities and evacuate the building.

>Here are some additional safety tips to keep in mind if you think there is a great white shark in your home:

    Do not go into the basement or any other area where the shark may be.
    Keep all doors and windows closed and locked.
    Do not go outside alone.
    If you must go outside, stay in well-lit areas and be aware of your surroundings.
    If you see the shark, do not approach it. Back away slowly and call for help.",5,
577,post,technology,1of5dv8,1761330267.0,"Local NBC Station Fooled by AI Video of Race Track Power Outage Causing Huge Crash
",7,2.0
578,post,technology,1of4x7j,1761329172.0,"Microsoft Outlook is getting an AI overhaul under new leaders
",0,13.0
579,post,technology,1of4rce,1761328814.0,"OpenAI buys Sky, an AI interface for Mac | TechCrunch
",0,0.0
580,post,technology,1of3uo1,1761326735.0,"A peer-reviewed study shows that AI-powered social media bots can increase user engagement on posts, but they fall short of encouraging users to post more overall
",0,1.0
581,post,technology,1of2aq7,1761323143.0,"Britain's Ministry of Justice just signed up to ChatGPT Enterprise
",0,0.0
582,post,technology,1of1p43,1761321765.0,"Apple begins shipping American-made AI servers from Texas
",5,10.0
583,post,technology,1of162o,1761320564.0,"Channel 4 Airs Entire Show Hosted By AI Presenter In UK Television First
",0,5.0
584,post,technology,1of0tpf,1761319755.0,"With new acquisition, OpenAI signals plans to integrate deeper into the OS | The acquired firm was working on a tool to control macOS directly with AI
",0,3.0
585,post,technology,1oeyqc0,1761314911.0,"Intel has cut 35,500 jobs in less than two years ‚Äî more than 20,000 let go in in recent months as Lip-Bu Tan continues drastic recovery journey
",172,31.0
586,post,technology,1oey0zo,1761313175.0,"Ohio lawmaker proposes comprehensive ban on marrying AI systems and granting legal personhood | House Bill 469 would label artificial intelligence as 'nonsentient entities' and block legal personhood
",177,34.0
587,post,technology,1oexxo1,1761312941.0,"AI teaches itself and outperforms human-designed algorithms
",0,3.0
588,post,technology,1oexeoo,1761311603.0,"Teen Swarmed by Cops After AI Metal Detector Flags His Doritos Bag as a Gun | AI has done it again.
",9751,350.0
589,post,technology,1oexeh5,1761311587.0,"BBC probe finds AI chatbots mangle nearly half of news summaries
",82,3.0
590,post,technology,1oewfbc,1761308998.0,"A Fake Debate in Virginia Raises Real Questions About AI in Politics
",6,0.0
591,post,technology,1oevz81,1761307713.0,"Hundreds of public figures, including Apple co-founder Steve Wozniak and Virgin's Richard Branson urge AI ‚Äòsuperintelligence‚Äô ban
",782,77.0
592,post,technology,1oeuebo,1761302667.0,"NPR/WBUR: The world has its first AI government official
",17,4.0
593,post,technology,1oesq8x,1761296383.0,"Guillermo del Toro compares Frankenstein to a careless ""tech bro,"" says he would ""rather die"" than use generative AI: ""My concern is not artificial intelligence, but natural stupidity""
",7141,151.0
594,post,technology,1oerime,1761291538.0,"AWS reveals new cloud incident reporting tool, with almost zero sense of irony
",159,13.0
595,post,technology,1oejawr,1761264288.0,"What it's like to have an AI wearable record everything you say
",0,7.0
596,post,technology,1oehipc,1761259387.0,"Microsoft AI chief says company won‚Äôt build chatbots for erotica
",48,53.0
597,post,technology,1oeh410,1761258314.0,"Microsoft's Mico is a 'Clippy' for the AI era
",0,4.0
598,post,technology,1oeeix9,1761251990.0,"Meta replacing humans with AI for FTC-mandated privacy reviews
",11,1.0
599,post,technology,1oeehof,1761251906.0,"Largest study of its kind shows AI assistants misrepresent news content 45% of the time ‚Äì regardless of language or territory
",744,34.0
600,post,technology,1oeb2wx,1761244070.0,"Silent Hill series producer Motoi Okamoto Claims Silent Hill f's 'Bold Choices' Were A Result of Human Design That AI 'Would Never Be Able to Make' | Producer dismisses Hideo Kojima's claims that ""remakes and sequels will be made by AI"" in the future.
",116,6.0
601,post,technology,1oe9xr9,1761241473.0,"Two federal judges acknowledged their staff's use of artificial intelligence in preparing recent court orders, which were subsequently found to be riddled with errors
",924,14.0
602,post,technology,1oe99bf,1761239944.0,"PC gamers claim Windows 11's Gaming Copilot is capturing gameplay for AI training by default ‚Äì but what it's actually doing is spoiling performance
",26,0.0
603,post,technology,1oe8a6l,1761237736.0,"Meet Mico, Microsoft‚Äôs AI version of Clippy
",0,12.0
604,post,technology,1oe73fc,1761235063.0,"Healthcare orgs deploy AI at twice the rate of rest of economy, report finds
",67,14.0
605,post,technology,1oe6583,1761232897.0,"Armed police swarm student after AI mistakes bag of Doritos for a weapon
",26504,1323.0
606,post,technology,1oe4yfu,1761230184.0,"OpenAI prioritised user engagement over suicide prevention, lawsuit claims | Family of teen who took his own life after ChatGPT use alleges chatbot maker intentionally weakened protections
",15,9.0
607,post,technology,1oe4ub2,1761229921.0,"We uploaded a fake video to 8 social apps. Only one told users it wasn‚Äôt real. | Facebook, TikTok and other major platforms do not use a tech industry standard touted as a way to flag fake content, tests using AI-generated videos found.
",97,6.0
608,post,technology,1oe4rmk,1761229745.0,"Detection firm finds 82% of herbal remedy books on Amazon ‚Äòlikely written‚Äô by AI
",421,24.0
609,post,technology,1oe4pne,1761229613.0,"A new autonomous fighter jet just broke cover. It's powered by the same AI brain that flew an F-16 through a dogfight.
",0,6.0
610,post,technology,1oe4oez,1761229531.0,"AI Models Get Brain Rot, Too | A new study shows that feeding large language models low-quality, high-engagement content from social media lowers their cognitive abilities.
",91,18.0
611,post,technology,1oe3gn0,1761226586.0,"Alibaba prices AI glasses at $660 to rival Meta and launches ChatGPT challenger
",9,7.0
612,post,technology,1oe2ri0,1761224769.0,"DeepSeek‚Äôs Push Into Africa Reveals China‚Äôs AI Power Grab
",0,2.0
613,post,technology,1oe2lam,1761224320.0,"YouTube launches AI detection tool to spot deepfakes using creators' faces and voices
",7,1.0
614,post,technology,1oe26nx,1761223204.0,"OpenAI Weakened ChatGPT‚Äôs Self-Harm Guardrails in Lead-Up to Teen‚Äôs Death, Lawsuit Says | Adam Raine's family members added troubling new allegations to their existing lawsuit against the company.
",39,8.0
615,post,technology,1odzmi2,1761215033.0,"OpenAI Wants to Cure Cancer. So Why Did It Make a Web Browser?
",213,95.0
616,post,technology,1odynti,1761211445.0,"DeepSeek's Push Into Africa Reveals China's AI Power Grab
",0,1.0
617,post,technology,1odycvb,1761210264.0,"AI Workers Are Putting In 100-Hour Workweeks to Win the New Tech Arms Race
",0,5.0
618,post,technology,1odxojw,1761207560.0,"Grounded jet engines take off again as datacenter generators
",96,22.0
619,post,technology,1odwge4,1761202722.0,"China's supercomputer breakthrough uses 37 million processor cores to model complex quantum chemistry at molecular scale ‚Äî Sunway fuses AI and quantum science
",113,20.0
620,post,technology,1odwflt,1761202633.0,"OpenAI requested memorial attendee list in ChatGPT suicide lawsuit
",35,9.0
621,post,technology,1odtq1o,1761192784.0,"Amazon claims the headline isn‚Äôt robots taking jobs as it reveals new cost-cutting robots | The Blue Jay robot‚Äôs ‚Äòextra set of hands‚Äô probably means Amazon‚Äôs warehouses will require fewer human hands
",24,7.0
622,post,technology,1odsr23,1761189642.0,"Fedora Will Allow AI-Assisted Contributions With Proper Disclosure & Transparency
",0,0.0
623,post,technology,1odnfnq,1761174401.0,"DeepSeek‚Äôs Surge in Africa Reveals China‚Äôs AI Power Grab
",13,16.0
624,post,technology,1odn0cw,1761173292.0,"Reddit sues Perplexity for scraping data to train AI system
",1,4.0
625,post,technology,1odkz9b,1761168194.0,"Sharepoint ToolShell attacks targeted orgs across four continents
",0,1.0
626,post,technology,1odjby7,1761164377.0,"Fed Lost Access to Private Jobs Data Ahead of Government Shutdown.
Payroll processing firm ADP ended its data-sharing with the central bank.
",1136,35.0
627,post,technology,1odj6qj,1761164048.0,"Reddit sues AI company Perplexity and others for 'industrial-scale' scraping of user comments
",781,160.0
628,post,technology,1odilgg,1761162723.0,"AI assistants make widespread errors about the news, new research shows
",132,24.0
629,post,technology,1odhzpy,1761161357.0,"AI chatbots make mistakes with news content nearly half of the time, says study
",285,31.0
630,post,technology,1odegun,1761153569.0,"It isn't layoffs, but Amazon has a stark new warning about the future of jobs
",123,99.0
631,post,technology,1odbep2,1761146887.0,"Microsoft Edge begs you to use Copilot AI instead of ChatGPT
",0,7.0
632,post,technology,1odbajd,1761146633.0,"Meta lays off 600 employees within AI unit
",22472,1525.0
633,post,technology,1odau0i,1761145597.0,"GM plans to launch eyes-off driving, Google AI and other new in-vehicle tech by 2028
",1,0.0
634,post,technology,1oda255,1761143882.0,"AI Data Centers, Desperate for Electricity, Are Building Their Own Power Plants
",15,2.0
635,post,technology,1od82px,1761139185.0,"Clickbait Gives AI Models ‚ÄòBrain Rot,‚Äô Researchers Find | Brain rot doesn't require a brain, just this one simple trick.
",212,24.0
636,post,technology,1od7gf4,1761137609.0,"OpenAI launches AI browser Atlas in latest challenge to Google
",0,3.0
637,post,technology,1od7ctl,1761137349.0,"AI bubble inflates Microsoft CEO pay to $96.5M
",5544,338.0
638,post,technology,1od6ziu,1761136376.0,"Airbnb picks Alibaba's Qwen over ChatGPT in a win for Chinese open-source AI
",23,5.0
639,post,technology,1od6dgu,1761134680.0,"YouTube‚Äôs AI ‚Äòlikeness detection‚Äô tool is searching for deepfakes of popular creators
",1,1.0
640,post,technology,1od6bru,1761134551.0,"Mark Cuban warns that OpenAI‚Äôs new plan to allow adults-only erotica in ChatGPT could ‚Äòbackfire. Hard‚Äô
",5277,502.0
641,post,technology,1od66y1,1761134180.0,"People Who Say They‚Äôre Experiencing AI Psychosis Beg the FTC for Help
",172,64.0
642,post,technology,1od5bp9,1761131482.0,"Google porting all internal workloads to Arm, with help from GenAI
",5,0.0
643,post,technology,1od5alh,1761131385.0,"Microsoft breaks, then quickly fixes Windows Recovery Environment bug that bricked USB input devices | In the age of AI-written patches, we highly suggest turning off automatic Windows updates
",411,53.0
644,post,technology,1od4k9u,1761128892.0,"Over 800 public figures, including ""AI godfathers"" and Steve Wozniak, sign open letter to ban superintelligent AI
",1251,113.0
645,post,technology,1od4i0c,1761128669.0,"Trust the AI, says new coding manifesto by Kim and Yegge
",0,4.0
646,post,technology,1od4hcy,1761128603.0,"Sources: Multimodal AI startup Fal AI already raised at $4B+ valuation
",0,0.0
647,post,technology,1od4h2j,1761128574.0,"Netflix goes ‚Äòall in‚Äô on generative AI as entertainment industry remains divided
",1950,507.0
648,post,technology,1od0u5t,1761114697.0,"Scientists build artificial neurons that work like real ones
",87,15.0
649,post,technology,1ocyl46,1761106726.0,"Palworld studio Pocketpair says its new publishing division won't handle games that use generative AI: 'We don't believe in it' | Pocketpair Publishing boss John Buckley says we're already starting to see a flood of 'really low-quality, AI-made games' on Steam and other storefronts.
",508,69.0
650,post,technology,1octts7,1761092852.0,"OpenAI launches AI browser Atlas in latest challenge to Google
",22,47.0
651,post,technology,1ocs1jq,1761088017.0,"A Tool That Crushes Creativity: AI slop is winning.
",598,196.0
652,post,technology,1ocptgc,1761082528.0,"Amazon Plans to Replace More Than Half a Million Jobs With Robots
",311,82.0
653,post,technology,1ocnsqr,1761077881.0,"Introducing ChatGPT Atlas - The browser with ChatGPT built in
",0,12.0
654,post,technology,1ocne59,1761076985.0,"Boris Johnson admits writing books using ChatGPT.
Former prime minister said ChatGPT was ‚Äúfrankly fantastic‚Äù and AI would help society ‚Äúbecause we‚Äôre all simple.‚Äù
",705,159.0
655,post,technology,1ocmgjq,1761074907.0,"YouTube‚Äôs likeness detection has arrived to help stop AI doppelg√§ngers | Likeness detection will flag possible AI fakes, but Google doesn‚Äôt guarantee removal.
",10,1.0
656,post,technology,1oclizi,1761072847.0,"Nvidia CEO Jensen Huang says future enterprises will employ a ‚Äòcombination of humans and digital humans‚Äô
",0,15.0
657,post,technology,1ocjz3o,1761069433.0,"DHS Asks OpenAI to Unmask User Behind ChatGPT Prompts, Possibly the First Such Case
",1939,135.0
658,post,technology,1ocj1vo,1761067378.0,"Amazon Plans to Replace More Than Half a Million Jobs With Robots (Gift Article)
",108,19.0
659,post,technology,1ociuyb,1761066946.0,"Uber will pay drivers to complete AI jobs while waiting for their next passenger
",1731,117.0
660,post,technology,1ocf8aj,1761058701.0,"Arm expands AI licensing program to boost on-device AI market share.
",0,0.0
661,post,technology,1ocdbyx,1761054200.0,"She used AI to fake a home invasion, confounding her husband and police
",0,1.0
662,post,technology,1ocaplp,1761047136.0,"‚ÄòEvery kind of creative discipline is in danger‚Äô: Lincoln Lawyer author on the dangers of AI
",359,42.0
663,post,technology,1ocagkn,1761046328.0,"An ex-OpenAI researcher‚Äôs study of a million-word ChatGPT conversation shows how quickly ‚ÄòAI psychosis‚Äô can take hold‚Äîand how chatbots can sidestep safety guardrails
",160,16.0
664,post,technology,1oc4edt,1761023833.0,"Catholic church calls for a global AI regulatory framework as Pope Leo XIV pontificates against the technology arms race, calling for the 'audacity of disarmament' | Wants AI to create a ""more authentically just and human global society"", but not impressed with what he's seen so far.
",2433,163.0
665,post,technology,1oc2wil,1761018838.0,"Kenny Loggins Demands Removal of ‚ÄòDanger Zone‚Äô From AI Clip of President Dumping Poop on Protesters
",13304,186.0
666,post,technology,1oc0wo3,1761012925.0,"Is the AI bubble about to pop?
",12,5.0
667,post,technology,1obzs9g,1761009746.0,"Apple‚Äôs new iPhone 17 outsells previous generation by 14% in US, China | Counterpoint Research analysts attribute the boost to an improved display, more storage and Apple‚Äôs upgraded A19 chip
",42,23.0
668,post,technology,1obt4xq,1760992847.0,"Analysis: How AI turned the Boston Common ‚ÄòNo Kings' protest into fake news
",691,96.0
669,post,technology,1obrj96,1760989310.0,"OpenAI cracks down on Sora 2 deepfakes after pressure from Bryan Cranston, SAG-AFTRA
",1316,85.0
670,post,technology,1obobwg,1760981497.0,"News Outlets Won't Describe the President‚Äôs AI Video For What It Is: The President Pooping on America
",41845,1127.0
671,post,technology,1obm6h7,1760975390.0,"Woman sends husband AI photo of man on their couch, husband sends cops
",5,5.0
672,post,technology,1obkzh8,1760972063.0,"Is AI really behind layoffs, or just a convenient excuse for companies? | Are companies using the technology as a scapegoat?
",59,24.0
673,post,technology,1obktxv,1760971636.0,"Switzerland launched its LLM Apertus. Here's a fact-check
",0,0.0
674,post,technology,1obi313,1760964395.0,"The FTC Is Disappearing Blog Posts About AI Published During Lina Khan‚Äôs Tenure | The Federal Trade Commission removed several blog posts in recent months about open source and potential risks to consumers from the rapid spread of commercial AI tools.
",191,6.0
675,post,technology,1obat9g,1760935121.0,"OpenAI‚Äôs ‚Äòembarrassing‚Äô math | TechCrunch
",91,33.0
676,post,technology,1ob89l8,1760927342.0,"OpenAI cofounder Andrej Karpathy says it will take a decade before AI agents actually work
",1182,236.0
677,post,technology,1ob2zqg,1760912515.0,"X lawsuit vs. Apple and OpenAI stays in Fort Worth, Texas; judge suggests they move there
",54,5.0
678,post,technology,1oawy8g,1760898133.0,"Some U.S. Military Personnel, Including Generals, Begin Using Generative Chatbots for Decision-Making
",887,84.0
679,post,technology,1oauno6,1760892808.0,"Leaders from the creative industries urge Parliament to act on foreign AI companies
",26,1.0
680,post,technology,1oaslgz,1760887939.0,"Companies are blaming AI for job cuts. Critics say it‚Äôs a 'good excuse'
",293,38.0
681,post,technology,1oas86s,1760887058.0,"OpenAI blocks Sora 2 users from using MLK Jr.'s likeness after ""disrespectful depictions""
",337,52.0
682,post,technology,1oas31l,1760886712.0,"‚ÄòLegacies condensed to AI slop‚Äô: OpenAI Sora videos of the dead raise alarm with legal experts
",216,67.0
683,post,technology,1oas1kw,1760886613.0,"How many times can OpenAI say, 'Oops?' | OpenAI wants you to think its mistakes are just a product of a young company moving fast. That may be part of it. But it's also beginning to look like a strategy: Asking forgiveness instead of permission.
",1211,56.0
684,post,technology,1oanzd5,1760875664.0,"Tor browser's latest build cuts Mozilla's AI features in the name of privacy | While everyone else races to bake AI into the browser, Tor is ripping it out on purpose
",6092,172.0
685,post,technology,1oanrhc,1760874974.0,"The Fight Over Whose AI Monster Is Scariest
",1,2.0
686,post,technology,1oamj23,1760870717.0,"The Ethics of AI, Capitalism and Society in the United States
",2,5.0
687,post,technology,1oaed0r,1760841050.0,"Amazon's Ring to partner with Flock, a network of AI cameras used by ICE, feds, and police
",624,43.0
688,post,technology,1oadc8n,1760837882.0,"Generative AI reportedly being heavily used to make new Halo games, including Halo CE Remake
",1871,281.0
689,post,technology,1oa8s7o,1760825004.0,"Convicted Fraudster Trevor Milton Rides His Pardon To Another CEO Job, Somehow
",770,17.0
690,post,technology,1oa7rvc,1760822478.0,"People are using AI to talk to God
",0,21.0
691,post,technology,1oa6cuz,1760819148.0,"Oxford Warns AI Is Making Teens Faster ‚Äî but Shallower Thinkers
",811,103.0
692,post,technology,1oa4fl5,1760814645.0,"Why the next generation of mRNA vaccines is set to be even better
",839,72.0
693,post,technology,1o9z0ik,1760801856.0,"Thirsty AI data centres are coming to Canada, with little scrutiny or oversight | CBC News
",566,43.0
694,post,technology,1o9y6af,1760799855.0,"AI headshots are changing the way job seekers are seen and get hired in tough labor market
",135,61.0
695,post,technology,1o9xz3d,1760799377.0,"Jensen says Nvidia‚Äôs China AI GPU market share has plummeted from 95% to zero ‚Äî the Chinese market previously amounted to 20% to 25% of the chipmaker's data center revenue
",7349,651.0
696,post,technology,1o9uhdm,1760790557.0,"Wikipedia Is Getting Pretty Worried About AI
",2822,249.0
697,post,technology,1o9q27t,1760774754.0,"Logitech CEO Hanneke Faber wants an AI agent in every board meeting
",0,4.0
698,post,technology,1o9o15z,1760767228.0,"A GOP attack ad deepfakes Chuck Schumer with AI
",4917,186.0
699,post,technology,1o9mjx4,1760762000.0,"Google pitches Workspace tools for ‚Äòwhen, not if‚Äô Microsoft 365 fails
",372,49.0
700,post,technology,1o9hi5t,1760746448.0,"Amazon reveals 960 megawatt nuclear power plans to cope with AI demand ‚Äî Richland, Washington site tapped for deployment of Xe-100 small modular reactors
",277,80.0
701,post,technology,1o9gg18,1760743463.0,"Creator of Infamous AI Painting Tells Court He's a Real Artist
",0,15.0
702,post,technology,1o9gfrf,1760743443.0,"OpenAI Somehow Needs Another $400 Billion In The Next 12 Months
",4760,456.0
703,post,technology,1o9cetw,1760733225.0,"Looks Like JD Vance Didn‚Äôt Get the Memo That This Admin Hates AI Guardrails
",1115,22.0
704,post,technology,1o9c0tn,1760732342.0,"Major General Reveals Bonkers Relationship With ChatGPT
",9648,527.0
705,post,technology,1o96omf,1760720045.0,"Can AI Avoid the Enshittification Trap?
",0,17.0
706,post,technology,1o9689j,1760719000.0,"Teachers get an F on AI-generated lesson plans | AI-generated lesson plans fall short on inspiring students and promoting critical thinking.
",91,14.0
707,post,technology,1o95mco,1760717617.0,"Barrister found to have used AI to prepare for hearing after citing ‚Äòfictitious‚Äô cases
",253,13.0
708,post,technology,1o947fh,1760714405.0,"Racist AI Fantasies Are Spreading on China‚Äôs Internet
",74,65.0
709,post,technology,1o93szn,1760713524.0,"NPR: Is the AI boom an AI bubble?
",677,187.0
710,post,technology,1o912bx,1760707019.0,"Teen sues ClothOff developer over fake nude images made with clothes removal tool
",3631,335.0
711,post,technology,1o90acf,1760705053.0,"SK Telecom‚Äôs AI unit offers staff voluntary retirement program just weeks after launch
",4,1.0
712,post,technology,1o8wpy8,1760693335.0,"OpenAI pauses Sora video generations of Martin Luther King Jr.
",445,106.0
713,post,technology,1o8th5s,1760680798.0,"Hollywood Agents Seethe Over Sora 2, Say OpenAI Purposely Misled Them.  The battle over deepfakes and intellectual property deepens after the latest AI video generator lets users create clips with familiar characters and movie scenes.
",781,70.0
714,post,technology,1o8odd4,1760664819.0,"Wikipedia Says AI Is Causing a Dangerous Decline in Human Visitors | ‚ÄúWith fewer visits to Wikipedia, fewer volunteers may grow and enrich the content, and fewer individual donors may support this work.‚Äù
",4642,163.0
715,post,technology,1o8llfy,1760657059.0,"Alibaba says its AI spending in e-commerce is already bre*king even
",0,14.0
716,post,technology,1o8liq9,1760656855.0,"TSMC profit surges 39% to beat estimates and hit yet another record on AI chip demand
",35,4.0
717,post,technology,1o8j940,1760651101.0,"Top Army general using ChatGPT to make military decisions raising security concerns
",14844,452.0
718,post,technology,1o8ivxw,1760650203.0,"Apple's M6 MacBook Pro generation will reportedly offer touchscreens
",0,27.0
719,post,technology,1o8itpa,1760650047.0,"New York bans AI-enabled rent price fixing / It‚Äôs the first statewide ban on algorithms designed to drive up rent.
",1963,24.0
720,post,technology,1o8hy8n,1760647975.0,"Study proves being rude to AI chatbots gets better results than being nice
",980,144.0
721,post,technology,1o8g7l4,1760643981.0,"Microsoft's next Windows 11 AI gamble: Just say 'Hey Copilot'
",0,12.0
722,post,technology,1o8feev,1760642125.0,"Reddit's AI Suggests Users Try Heroin / AI-generated Reddit Answers are giving bad advice in medical subreddits and moderators can‚Äôt opt out.
",1205,82.0
723,post,technology,1o8f7z9,1760641716.0,"How AI job cuts could come back to haunt Trump
",34,19.0
724,post,technology,1o8dg3p,1760637798.0,"Vibe Shift in AI Coding: Senior Developers Ship 2.5x More Than Juniors
",0,4.0
725,post,technology,1o8ceb6,1760635484.0,"Uber is turning its app into an AI training ground | Uber wants to challenge established players like Scale AI or Amazon‚Äôs Mechanical Turks
",20,5.0
726,post,technology,1o8agaa,1760631240.0,"Trump's AI advisor accuses Anthropic of ""regulatory capture""
",12,3.0
727,post,technology,1o89egt,1760628899.0,"New AI battle: White House vs. Anthropic
",4,0.0
728,post,technology,1o88nfw,1760627205.0,"Microsoft: Russia, China increasingly using AI to escalate cyberattacks on the US
",70,8.0
729,post,technology,1o87hr1,1760624545.0,"Microsoft wants you to talk to your PC and let AI control it | Microsoft is rewriting Windows to turn computers into AI PCs that you talk to
",501,483.0
730,post,technology,1o86rxr,1760622845.0,"Boris Johnson gushes over using ChatGPT while writing books: ‚ÄòI love that it calls me clever‚Äô
",3332,316.0
731,post,technology,1o86q8h,1760622726.0,"Inside the web infrastructure revolt over Google‚Äôs AI Overviews | Cloudflare CEO Matthew Prince is making sweeping changes to force Google's hand
",71,4.0
732,post,technology,1o85ygs,1760620784.0,"Microsoft announces agentic AI assistant for Windows 11 ‚Äî dubs Windows PCs ""the computer you can talk to""
",0,18.0
733,post,technology,1o85y60,1760620761.0,"Microsoft will test a Copilot AI feature that performs work on local files in Windows 11
",6,2.0
734,post,technology,1o85azl,1760619084.0,"Thirsty AI mega projects raise alarm in some of Europe‚Äôs driest regions
",30,1.0
735,post,technology,1o83brp,1760613186.0,"OpenAI's ChatGPT is so popular that almost no one will pay for it
",7957,1431.0
736,post,technology,1o82yqt,1760611973.0,"Steve Jobs will appear on commemorative $1 coin
",0,12.0
737,post,technology,1o82ul1,1760611582.0,"AI Economics Are Brutal. Demand Is the Variable to Watch.
",12,9.0
738,post,technology,1o82ry1,1760611322.0,"New California law requires AI to tell you it‚Äôs AI
",6397,131.0
739,post,technology,1o82rkx,1760611288.0,"U.S. federal AI regulation is on the way, Sen. Marsha Blackburn says, regardless of big tech opposition
",39,5.0
740,post,technology,1o8192f,1760605630.0,"OpenAI accused of using legal tactics to silence nonprofits
",62,7.0
741,post,technology,1o7tdt6,1760578292.0,"Why accessibility might be AI‚Äôs biggest breakthrough - Ars Technica
",0,1.0
742,post,technology,1o7qprx,1760570963.0,"Pinterest Finally Allows You to Filter Out AI-Generated Images
",185,7.0
743,post,technology,1o7g2vb,1760546391.0,"Man Stores AI-Generated ‚ÄòRobot Porn' on His Government Computer, Loses Access to Nuclear Secrets
",2229,189.0
744,post,technology,1o7ctcx,1760539152.0,"Quebec judge fines man $5,000 for improper use of artificial intelligence in court
",219,22.0
745,post,technology,1o7b0kx,1760534920.0,"Samsung teams up with Glance to use your face in AI-generated lock screen ads
",5,45.0
746,post,technology,1o7anyg,1760534040.0,"AI-generated screensaver ads coming to DirecTV's Gemini devices in 2026
",0,1.0
747,post,technology,1o79asr,1760530402.0,"Tesla offloads Cybertrucks to SpaceX and xAI as sales slump continues
",699,63.0
748,post,technology,1o78li5,1760528324.0,"Goldman Tells Staff It Will Cut More Jobs as AI Saves Costs
",866,140.0
749,post,technology,1o785nz,1760526914.0,"Sam Altman says ChatGPT will soon allow erotica for adult users
",1036,365.0
750,post,technology,1o785ie,1760526901.0,"Japanese Government Calls on Sora 2 Maker OpenAI to Refrain From Copyright Infringement, Says Characters From Manga and Anime Are 'Irreplaceable Treasures' That Japan Boasts to the World
",1259,46.0
751,post,technology,1o77sqf,1760525761.0,"Big Tech deploys Orwellian doublespeak to mask its democratic corrosion | As AI asserts its power to shape social evolution, regulation is decried as ‚Äòstate control‚Äô rather than the expression of our collective will
",1710,35.0
752,post,technology,1o77625,1760523544.0,"From AI to TikTok to TV, This Pro-Israel Billionaire Is Expanding Power in US
",172,35.0
753,post,technology,1o75poc,1760518044.0,"Meet the AI chatbots replacing India's call-center workers
",8,9.0
754,post,technology,1o72xk7,1760507279.0,"OpenAI has five years to turn $13 billion into $1 trillion | TechCrunch
",2596,355.0
755,post,technology,1o714nq,1760501137.0,"Former Intel CEO Pat Gelsinger confirms the industry is in an AI bubble, but that a pop could be several years away ‚Äî 'We‚Äôre displacing all of the internet and the service provider industry as we think about it today'
",365,133.0
756,post,technology,1o713un,1760501065.0,"Nvidia sells tiny new computer that puts big AI on your desktop
",0,3.0
757,post,technology,1o6zh1g,1760496159.0,"ChatGPT will soon allow erotica for verified adults, OpenAI boss says
",136,95.0
758,post,technology,1o6z00w,1760494766.0,"Mozilla's Firefox adds Perplexity's AI answer engine as a new search option
",0,1.0
759,post,technology,1o6t060,1760478769.0,"DirecTV screensavers will show AI-generated ads with your face in 2026 | Like other companies with streaming businesses, DirecTV is leaning into ads more.
",48,22.0
760,post,technology,1o6s25w,1760476555.0,"'Under tremendous pressure': Newsom vetoes long-awaited AI chatbot bill
",2008,175.0
761,post,technology,1o6rluh,1760475549.0,"Walmart stock hits all-time high after announcing partnership with OpenAI
",188,118.0
762,post,technology,1o6p5k8,1760470087.0,"Sam Altman says ChatGPT will soon sext with verified adults
",712,344.0
763,post,technology,1o6mrip,1760464732.0,"Google announces $15B investment in AI hub in India meant to drive digital transformation
",18,7.0
764,post,technology,1o6ixe9,1760456369.0,"Lawyer Caught Using AI While Explaining to Court Why He Used AI | The attorney not only submitted AI-generated fake citations in a brief for his clients, but also included ‚Äúmultiple new AI-hallucinated citations and quotations‚Äù in the process of opposing a motion for sanctions.
",932,79.0
765,post,technology,1o6hq5c,1760453700.0,"OpenAI wants to stop ChatGPT from validating users‚Äô political views | New paper reveals reducing ""bias"" means making ChatGPT stop mirroring users' political language.
",249,29.0
766,post,technology,1o6hoxu,1760453624.0,"Goldman Sachs warns of 'jobless growth' in the US as AI fuels output but not jobs
",4333,270.0
767,post,technology,1o6f3n0,1760447453.0,"Oracle Cloud to deploy 50,000 AMD AI chips, signaling new Nvidia competition
",22,3.0
768,post,technology,1o6esqj,1760446708.0,"Microsoft will help your boss see how much you're using AI
",650,184.0
769,post,technology,1o6ejhy,1760446028.0,"Equity threatens mass direct action over use of actors‚Äô images in AI content | Union says growing numbers of its members have made complaints about infringements of copyright in AI material
",56,10.0
770,post,technology,1o6e2t8,1760444763.0,"Why AI is being trained in rural India
",0,7.0
771,post,technology,1o6dume,1760444114.0,"AI is killing jobs, so Meta is reviving the Facebook job board | Two years after the feature was phased out
",307,29.0
772,post,technology,1o6c2eg,1760438576.0,"The AI Reckoning: 'Godfather' Geoffrey Hinton Sounds Alarm on Unchecked Development, Warning of Catastrophe and Ethical Crisis
",116,10.0
773,post,technology,1o66bzq,1760417167.0,"Microsoft breaks Media Creation Tool on the eve of Windows 10 end-of-life ‚Äî company confirms Windows 11 upgrade tool 'might not work as expected', outlines alternatives | The bug was introduced in a Sept 29 update, Windows 10 support ends October 14
",270,40.0
774,post,technology,1o662h0,1760416297.0,"Broadcom's chip president says mystery $10 billion customer isn't OpenAI
",114,20.0
775,post,technology,1o64o31,1760411974.0,"NVIDIA DGX Spark Arrives for World‚Äôs AI Developers
",0,2.0
776,post,technology,1o63kz3,1760408792.0,"Oracle CEO Magouyrk: 'Of course' OpenAI can pay $60 billion per year for cloud infrastructure
",13,7.0
777,post,technology,1o62je5,1760405808.0,"Facebook's Algorithm Update Aims to Show You Reels You'll Actually Like, Even if They're AI
",0,20.0
778,post,technology,1o606dt,1760399269.0,"What Happened When AI Came for Craft Beer | A prominent beer competition introduced an AI-judging tool without warning. The judges and some members of the wider brewing industry were pissed
",1217,123.0
779,post,technology,1o5ucyu,1760385416.0,"Why AI Will Widen the Gap Between Superstars and Everybody Else
",0,15.0
780,post,technology,1o5s4s7,1760380579.0,"Sora 2 and ChatGPT are consuming so much power that OpenAI just did another 10 gigawatt deal
",2263,213.0
781,post,technology,1o5rtmh,1760379907.0,"AI Is Juicing the Economy. Is It Making American Workers More Productive?
",0,38.0
782,post,technology,1o5rk3t,1760379340.0,"Study: Artificial intelligence (AI) is wrecking havoc on university assessments and exams
",89,33.0
783,post,technology,1o5l3ig,1760365391.0,"Microsoft lets bosses spot teams that are dodging Copilot - Viva Insights turns AI guzzling into a leaderboard
",1490,238.0
784,post,technology,1o5k80g,1760363350.0,"Nobel Prize Economist Warns: Innovation Could Stall if AI Power Concentrates and Markets Close
",67,6.0
785,post,technology,1o5jl1d,1760361762.0,"OpenAI is trying to clamp down on ‚Äòbias‚Äô in ChatGPT
",269,87.0
786,post,technology,1o5j8n8,1760360890.0,"OpenAI partners with Broadcom to build custom AI chips, adding to Nvidia and AMD deals
",0,2.0
787,post,technology,1o5eyhz,1760347315.0,"Japan to promote domestic AI development for national security
",24,9.0
788,post,technology,1o5bj9m,1760334343.0,"The Real AI Risk is ‚ÄòMeh‚Äô Technology That Takes Jobs and Annoys Us All
",545,67.0
789,post,technology,1o5bdre,1760333795.0,"Adobe exec Stacy Martinet says the $141 billion software giant embraces candidates who use AI to apply for jobs‚Äîbecause they‚Äôre the people ‚Äòcreating the future‚Äô
",0,16.0
790,post,technology,1o58clq,1760324103.0,"Nvidia‚Äôs AI empire: A look at its top startup investments
",13,4.0
791,post,technology,1o568ug,1760317931.0,"Most people rarely use AI, and dark personality traits predict who uses it more | Study finds AI browsing makes up less than 1% of online activity
",865,159.0
792,post,technology,1o4tahe,1760285911.0,"NSO Group's pending US sale marks a turning point for Israel's most controversial tech firm | NSO is best known for developing Pegasus, a surveillance tool capable of remotely accessing smartphones
",62,5.0
793,post,technology,1o4t6o2,1760285669.0,"The AI bubble is 17 times the size of the dot-com frenzy ‚Äî and four times the subprime bubble, analyst says
",29733,1971.0
794,post,technology,1o4s3un,1760283166.0,"Police are asking kids to stop pulling AI homeless man prank
",979,141.0
795,post,technology,1o4r0gb,1760280578.0,"Meta AI adviser spreads disinformation about shootings, vaccines and trans people
",407,16.0
796,post,technology,1o4p437,1760275771.0,"A 3-person policy nonprofit that worked on California‚Äôs AI safety law is publicly accusing OpenAI of intimidation tactics.
",21,1.0
797,post,technology,1o4mgtm,1760267904.0,"A 3-person policy nonprofit that worked on California‚Äôs AI safety law is publicly accusing OpenAI of intimidation tactics
",2729,36.0
798,post,technology,1o4mflb,1760267791.0,"AI videos of dead celebrities are horrifying many of their families
",555,84.0
799,post,technology,1o4mbta,1760267434.0,"AI enabled Klarna to halve its workforce‚Äînow, the CEO is warning workers that other ‚Äòtech bros‚Äô are sugarcoating just how badly it‚Äôs about to impact jobs
",1046,181.0
800,post,technology,1o4krmj,1760261651.0,"OpenAI‚Äôs dominance is unlike anything Silicon Valley has ever seen
",0,10.0
801,post,technology,1o4kf83,1760260301.0,"US President's son Barron tipped for top TikTok job
",21964,2349.0
802,post,technology,1o4gyxb,1760247225.0,"Ready or not, enterprises are betting on AI
",1,1.0
803,post,technology,1o4enkv,1760239318.0,"KUOW - Game over or next level? The generative AI debate in video game creation
",0,4.0
804,post,technology,1o42ymh,1760207301.0,"AI drones are America's newest cops
",237,32.0
805,post,technology,1o42fpd,1760206005.0,"Putin Has a New Tool to Monitor Russians
",73,24.0
806,post,technology,1o40ub6,1760202160.0,"Using a swearword in your Google search can stop the AI answer. But should you?
",286,185.0
807,post,technology,1o3zt0o,1760199659.0,"OpenAI's ChatGPT has 800 million weekly users, double all its main competitors combined
",0,29.0
808,post,technology,1o3zo7h,1760199339.0,"Microsoft's OneDrive begins testing face-recognizing AI for photos (for some preview users)
",50,25.0
809,post,technology,1o3xscj,1760194796.0,"ANALYSIS | Why OpenAI wants to help Canada build homegrown artificial intelligence | CBC News
",0,11.0
810,post,technology,1o3qiok,1760172175.0,"A tangled web of deals stokes AI bubble fears in Silicon Valley
",330,97.0
811,post,technology,1o3p3mi,1760166683.0,"OpenAI video app Sora hits 1 million downloads faster than ChatGPT
",0,17.0
812,post,technology,1o3mxm6,1760158966.0,"‚ÄòTron: Ares‚Äô Wants to Gaslight You About the Future of AI | The latest film in the Tron franchise seems to have not learned any lessons from sci-fi movies past‚Äîor from current reality.
",0,18.0
813,post,technology,1o3evjb,1760134833.0,"Apple nears deal to acquire talent and technology from computer vision startup Prompt AI
",0,3.0
814,post,technology,1o3dzqc,1760132645.0,"The AI Bubble's Impossible Promises
",40,6.0
815,post,technology,1o3d0jm,1760130331.0,"US Supreme Court asked to hear dispute over copyrights for AI creations
",116,56.0
816,post,technology,1o3cmut,1760129447.0,"What if OpenAI went belly-up?
",0,13.0
817,post,technology,1o3ckyz,1760129328.0,"Managers are throwing entry-level workers under the bus in race to adopt AI
",150,21.0
818,post,technology,1o399ki,1760121709.0,"Apple sued over use of copyrighted books to train Apple Intelligence
",493,29.0
819,post,technology,1o38u8j,1760120743.0,"Deloitte to partially refund Australia for report with apparent AI-generated errors
",22,1.0
820,post,technology,1o38efx,1760119750.0,"Battlefield 6 Generated $100 Million On Steam From Pre-Sales - Report
",29,32.0
821,post,technology,1o3765y,1760116999.0,"'AI homeless man prank' on social media which uses AI image generators to simulate false home invasions prompts concern from local authorities
",121,22.0
822,post,technology,1o32jua,1760106771.0,"Meta Tells Its Metaverse Workers to Use AI to ‚ÄòGo 5X Faster‚Äô
",5,5.0
823,post,technology,1o31ytk,1760105411.0,"Anthropic Opening Its First India Office to Tap AI Talent
",0,4.0
824,post,technology,1o31888,1760103655.0,"Meta Tells Its Metaverse Workers to Use AI to ‚ÄòGo 5X Faster‚Äô | Mark Zuckerberg's metaverse chief is urging employees to adopt AI across every workflow as part of a broader shift inside the company
",71,55.0
825,post,technology,1o30sza,1760102606.0,"While OpenAI races to build AI data centers, Nadella reminds us that Microsoft already has them
",6,4.0
826,comment,technology,hycu2kn,1645775996.0,"Oh *noes*!  That's terrible!  How will they ever gonna get their *news*?!  *H****o***W?

Whelp... I hope hackers also don't  attack the...

* Russian Banks by placing large debts on rich and removing debts from the poors.
* Russian Telecommunication by spreading gay love and some rumor about how Putin isn't really Russian at all.
* Russian Police Radio by sending them to the wrong locations and confusing reports.
* Russian Police Station database by clearing the record of the arrested protestors and let them be released as charges dropped.
* Russian Power Relay Stations by cutting off the power to the important political places.
* Russian Weather Emergency Broadcast Station by giving them a big gay superstorm warning with statement that will rain rainbow-colored dicks.
* Russian Airports by rerouting all flights to Hell, Norway.
* Russian Hotels by booking all the rooms with extra services.
* Russian Food Delveries by ordering so many sausages that Germany could faint and send them to Putin and other top men.
* Russian Hallmark service by making them send many ""Thinking of you"" cards to Fire Departments, Hospitals, Auto Shop, Gardeners, Health Care Services, Insurances, and some good Russian companies that deserves better than being dragged into another war.

I hope I'm not missing out any other *bad* and *terrible* ideas. Cause that would be *bad*! ü§≠",1503,
827,comment,technology,nepuvuy,1758122039.0,"ChatGPT

This study, funded by the National Institute of Justice (NIJ), pulls together findings from multiple research projects on domestic terrorism and radicalization in the United States. Here is a clear and simple summary:

Key Findings

Far-right extremism dominates: Since 1990, far-right extremists have committed more deadly attacks than far-left or Islamist extremists, killing over 520 people compared to 78 by far-left actors

306123

.

No single extremist profile: There is no clear-cut ‚Äútype‚Äù of person who becomes radicalized. Extremists and hate-crime offenders differ widely in age, education, criminal history, ideology, and motivations

306123

.

Military connection: Around 11% of extremists had military experience, and their involvement has risen in recent years. Veterans are more likely to affiliate with right-wing groups and hold leadership roles, partly due to skills and identity struggles after service

306123

.

Longevity of plots: Terrorist plots last longer or shorter depending on group type, planning, and government oversight. For example, environmental extremists often evade arrest longer than right-wing extremists, whose plots are usually shorter lived

306123

.

Online influence: Extremists use online forums mainly to share ideas and reinforce each other, creating echo chambers. While not always leading directly to violence, online spaces fuel radicalization

306123

.

Pathways in and out of white supremacy: People usually join white supremacist groups not because of ideology at first, but due to anger, loneliness, or abuse histories. Exiting is gradual, non-linear, and sometimes feels like overcoming an addiction

306123

.

Policy Implications

Countering extremism requires community partnerships beyond law enforcement, involving families, mental health professionals, and local organizations.

More resources should go toward domestic extremism research and prevention, not just international terrorism.

Early intervention opportunities exist, especially when individuals show warning signs of radicalization or struggle with identity, purpose, and belonging

306123

.

In short, the research shows domestic extremism in the U.S. is complex, with far-right violence posing the greatest threat. Effective prevention depends on understanding varied personal paths into extremism, strengthening community partnerships, and addressing vulnerabilities before violence occurs.",3,
828,comment,technology,nekjicx,1758047792.0,\* Thanks ChatGPT.,24,
829,comment,technology,glot057,1612236485.0,"That's what cellee means. 

Cellmate",7,
830,comment,technology,jmmg99j,1685719178.0,"I asked chatGPT to write me an original joke. I then copy and pasted that joke into google inside quotation marks, it found it word for word from an old reddit thread. It's already getting it's data from reddit.",65,
831,comment,technology,jmmmxyk,1685721814.0,"Plus there are places like r/NonCredibleDefense.

ChatGPT is never going to be able to replicate the barely concealed strike-fighter-as-anime-girl lust that goes on over there. You need a barely human for that, not a machine.",12,
832,comment,technology,mhn8r00,1741903697.0,"Everyone thinks about this stuff in terms of controlling political discourse, but as a former mod of a hobby sub, these tools are invaluable for: 

1. Removing trolls trying to turn the conversation towards today's political ragebait topic. Filtering comments on racial and other slurs is a good way to do this.
2. Catching obvious rule-breaking posts. E.g., if the sub forbids posting AI art, then an automod rule that removes image posts with ""mid journey,"" ""ChatGPT,"" etc. in the title is a good first line of defense.

You obviously wouldn't want to use Reddit if every subreddit was like the default feed on X. But that's how it would be without automated comment removal. Of course, these tools can be abused, but they're also necessary to make Reddit even minimally usable.",21,
833,comment,technology,dripjv1,1513777465.0,"That's not what the article is about.

>Rather than a logical look at the current state of how the internet works today (much of the anti-FCC rhetoric was not based in such a reality), or even a practical discussion about how the internet has evolved freely and robustly absent of such regulations, most of the ‚Äúdiscussion‚Äù was a digital shouting match of partisan and anti-capitalism rhetoric. That, and a massive amount of fraud.

>As I wrote earlier this month, hundreds of thousands of comments were submitted to the FCC in spikes during the public comment period about its proposal to eliminate the 2015 ‚ÄúOpen Internet Order‚Äù; and, upon further investigation, were found to have been written not by humans, but by artificial intelligence programs using ‚Äúnatural language generators.",116,
834,comment,technology,nf6vp2k,1758340403.0,About half of everything ChatGPT barfs up is from Reddit.  Why does this surprise you?,5,
835,comment,technology,drv5aan,1514475351.0,"They get tricked into thinking the problem is ___________. (immigrants, poor, muslims, another ""other"") 

The number of times in history the people have gone to war against the ""other"" vs the number of times the people have gone to war against the powerful shows the strength of the delusion. 


*edit* The loss of jobs, wealth, security, and culture are blamed upon terrorists, infidels, poor, and everyone but the rich. But our booming economies are producing more and more every year and we get less and less. We know where this wealth and power go, but we say they ""deserve it"". Now we don't have enough power to take time off work for holidays or to take a vacation with our kids. We don't have enough power to acquire the home and car and security we were told are hallmarks of America or the West. We are upset, and for good reason: we are becoming indentured workers (slaves). But do we cast our vengeful gaze on those who hoard the wealth and power of our labours or do we look at those with whom we are competing for what wealth remains for the working class? 

Ali saw this lie clearly when he refused the draft, costing him his best fighting years: ""You my opposer"" ",129,
836,comment,technology,nf09in2,1758251172.0,"Cool, so you are saying Kimmel lied and then you just lie for the entire comment.

What exactly do you think to gain from lying like this exactly? You have 4 upvotes? Do you get paid by the upvote?

Why would you just post a blatant lie like this?

here is the actual thing you think proves your complete bullshit. [23c7187b-5c27-490e-b52d-3f2640f43091.pdf](https://www.washingtonpost.com/documents/23c7187b-5c27-490e-b52d-3f2640f43091.pdf?utm_source=chatgpt.com)

Yea, they are desperate.",3,
837,comment,technology,gmkp5o0,1612787038.0,More like Amazon sells all warehouses and outsources fulfillment to the newly formed Definitely-Not-Amazon LLC.,484,
838,comment,technology,gml2ajo,1612795323.0,"What kind of fulfillment center were you at? I‚Äôm at an FC and so far it‚Äôs been fairly laid back

Edit: a word",34,
839,comment,technology,mgo8opa,1741436501.0,"Don't get your hopes up.


> [Plans for Digg‚Äôs relaunch stem from Rose and Mezzell testing various ***AI tools***, which led to wondering ***if AI could help curate quality content and online communities***. What if they could ***use AI to solve some of the biggest issues that have plagued Digg***, and the internet at large, since the early days?][1]


[1]: https://digiday.com/media/reddit-cofounder-alexis-ohanian-teams-with-kevin-rose-to-relaunch-digg/",232,
840,comment,technology,mgo9q1m,1741437009.0,"Fucking ai slop. Quite literally the internet got immensely worse since chatGPT was first demo'd. The solution is quite the opposite, and these nerds just want a cash cow",204,
841,comment,technology,fvoaaxo,1592858578.0,"Imagine the blood on their hands if serial killer who was close to being caught is now in the wind because he found his name in the database as a suspect, or human trafficking organisation  moves and now is untraceable. I would probably just killmyself.",7,
842,comment,technology,mzqqsnl,1750875933.0,"To do what? We run through so many pointless projects at my office that are unnecessary. Our bosses are always in a fevered pitch to crank shit out and I'm like ""Okay, but if you slowed down and thought a little bit more critically we could make more of an impact on the business while doing less.""

...instead of this ""Fuck, just throw it against the wall and see what moves the needle!!""

For instance, we all worked hard on this project to implement some LLM into our software...and it does have some potential benefits for sure. But we had to rush it for whatever reason and it could be implemented in a better way that would be more useful...but instead they basically just fucking dropped it and shouted ON TO THE NEXT THING!",6,
843,comment,technology,mzqveax,1750877254.0,"You're thinking specifically of public LLMs, not AI broadly. But to your point, I also think there should be royalties to anybody whose copyrighted material is used in ANY fashion other than legally recognized fair use. And of course, copyright exists the moment it's ""fixed"" into any form in the physical universe. Draw a character? Copyrighted once drawn. Write a thing? Copyright exists immediately. Now, that's how it ""works"", but in practice, proving copyright is a little harder, but unless signed away, one legally has copyright this way, at least in the US.",2,
844,comment,technology,mzqo11g,1750875156.0,"It is not remotely different. AI still needs to be programmed, fed training data, fine-tuned, prompted, and revised and triple-checked by humans. Even if you bypass the first couple steps by subscribing to a generic AI like ChatGPT instead of investing in a more specialized one, there is no such thing as a human-free application of AI, and anyone who thinks there is is *seriously* misusing it and will have to deal with the consequences sooner or later.

If anything, computers were a *much* bigger societal shift than AI.",2,
845,comment,technology,dqj9yqr,1512002532.0,/r/nevertellmetheodds,238,
846,comment,technology,mnenk96,1744810532.0,"It‚Äôs because people have lost trust. Either in the government or in the media. 

I know if i told my father this, he would have this to say:

‚ÄúWell who‚Äôs the whistleblower? Are they credible? I could blow the whistle that Trump has bodies in the basement and he eats the cheeks off babies. Is that credible simply because I‚Äôm a ‚Äúwhistleblower‚Äù?

This is what the Russians want you to think. The whistleblower is probably the Russian asset because Trump is hard on Russia. It was Biden who was weak and allowed the attack on Ukraine.‚Äù

We are in the post-truth era. People just don‚Äôt realize it yet dude. 

And AI images and video are going to make things so, so much harder in just the next few years. By the end of trumps administration, there could be video of him lining people up and shooting them himself and you wouldn‚Äôt know to believe it or not. 

After all, what‚Äôs a video prove? It would take me longer to type in the prompt than it would for ChatGPT to generate the video. 

You would have to literally be next to trump and feel the reverberation of the gun to know the truth. 

And out of 8.5 billion people in the world, how many would be standing beside him to know the truth?

The truth is dead. And we‚Äôre all fucked.",3,
847,comment,technology,jnn7fqy,1686394383.0,"Reddit has long been a hot spot for conversation on the internet. About 57 million people visit the site every day to chat about topics as varied as makeup, video games and pointers for power washing driveways.

In recent years, Reddit‚Äôs array of chats also have been a free teaching aid for companies like Google, OpenAI and Microsoft. Those companies are using Reddit‚Äôs conversations in the development of giant artificial intelligence systems that many in Silicon Valley think are on their way to becoming the tech industry‚Äôs next big thing.

Now Reddit wants to be paid for it. The company said on Tuesday that it planned to begin charging companies for access to its application programming interface, or A.P.I., the method through which outside entities can download and process the social network‚Äôs vast selection of person-to-person conversations.

‚ÄúThe Reddit corpus of data is really valuable,‚Äù Steve Huffman, founder and chief executive of Reddit, said in an interview. ‚ÄúBut we don‚Äôt need to give all of that value to some of the largest companies in the world for free.‚Äù

The move is one of the first significant examples of a social network‚Äôs charging for access to the conversations it hosts for the purpose of developing A.I. systems like ChatGPT, OpenAI‚Äôs popular program. Those new A.I. systems could one day lead to big businesses, but they aren‚Äôt likely to help companies like Reddit very much. In fact, they could be used to create competitors ‚Äî automated duplicates to Reddit‚Äôs conversations.

Reddit is also acting as it prepares for a possible initial public offering on Wall Street this year. The company, which was founded in 2005, makes most of its money through advertising and e-commerce transactions on its platform. Reddit said it was still ironing out the details of what it would charge for A.P.I. access and would announce prices in the coming weeks.

Reddit‚Äôs conversation forums have become valuable commodities as large language models, or L.L.M.s, have become an essential part of creating new A.I. technology.

L.L.M.s are essentially sophisticated algorithms developed by companies like Google and OpenAI, which is a close partner of Microsoft. To the algorithms, the Reddit conversations are data, and they are among the vast pool of material being fed into the L.L.M.s. to develop them.

The underlying algorithm that helped to build Bard, Google‚Äôs conversational A.I. service, is partly trained on Reddit data. OpenAI‚Äôs Chat GPT cites Reddit data as one of the sources of information it has been trained on.
Editors‚Äô Picks
This 1,000-Year-Old Smartphone Just Dialed In
The Coolest Menu Item at the Moment Is ‚Ä¶ Cabbage?
My Children Helped Me Remember How to Fly

Other companies are also beginning to see value in the conversations and images they host. Shutterstock, the image hosting service, also sold image data to OpenAI to help create DALL-E, the A.I. program that creates vivid graphical imagery with only a text-based prompt required.

Last month, Elon Musk, the owner of Twitter, said he was cracking down on the use of Twitter‚Äôs A.P.I., which thousands of companies and independent developers use to track the millions of conversations across the network. Though he did not cite L.L.M.s as a reason for the change, the new fees could go well into the tens or even hundreds of thousands of dollars.

To keep improving their models, artificial intelligence makers need two significant things: an enormous amount of computing power and an enormous amount of data. Some of the biggest A.I. developers have plenty of computing power but still look outside their own networks for the data needed to improve their algorithms. That has included sources like Wikipedia, millions of digitized books, academic articles and Reddit.

Representatives from Google, Open AI and Microsoft did not immediately respond to a request for comment.

Reddit has long had a symbiotic relationship with the search engines of companies like Google and Microsoft. The search engines ‚Äúcrawl‚Äù Reddit‚Äôs web pages in order to index information and make it available for search results. That crawling, or ‚Äúscraping,‚Äù isn‚Äôt always welcome by every site on the internet. But Reddit has benefited by appearing higher in search results.

The dynamic is different with L.L.M.s ‚Äî they gobble as much data as they can to create new A.I. systems like the chatbots.

Reddit believes its data is particularly valuable because it is continuously updated. That newness and relevance, Mr. Huffman said, is what large language modeling algorithms need to produce the best results.

‚ÄúMore than any other place on the internet, Reddit is a home for authentic conversation,‚Äù Mr. Huffman said. ‚ÄúThere‚Äôs a lot of stuff on the site that you‚Äôd only ever say in therapy, or A.A., or never at all.‚Äù

Mr. Huffman said Reddit‚Äôs A.P.I. would still be free to developers who wanted to build applications that helped people use Reddit. They could use the tools to build a bot that automatically tracks whether users‚Äô comments adhere to rules for posting, for instance. Researchers who want to study Reddit data for academic or noncommercial purposes will continue to have free access to it.

Reddit also hopes to incorporate more so-called machine learning into how the site itself operates. It could be used, for instance, to identify the use of A.I.-generated text on Reddit, and add a label that notifies users that the comment came from a bot.

The company also promised to improve software tools that can be used by moderators ‚Äî the users who volunteer their time to keep the site‚Äôs forums operating smoothly and improve conversations between users. And third-party bots that help moderators monitor the forums will continue to be supported.

But for the A.I. makers, it‚Äôs time to pay up.

‚ÄúCrawling Reddit, generating value and not returning any of that value to our users is something we have a problem with,‚Äù Mr. Huffman said. ‚ÄúIt‚Äôs a good time for us to tighten things up.‚Äù

‚ÄúWe think that‚Äôs fair,‚Äù he added.",10,
848,comment,technology,ml9n7bj,1743714179.0,"  
Trump did not create this chart. A staffer or someone over at Heritage used ChatGPT to create it. Of course, their qualifications were; ""Trump, you are the greatest and most smartest person in the world."" And so they got the job.",1192,
849,comment,technology,ml9ifdn,1743712760.0,You‚Äôre telling me ChatGPT is running the show and not the guy who went bankrupt running a casino?  I actually feel relieved.,449,
850,comment,technology,ml9ezy8,1743711766.0,"I get so frustrated using AI.  It won‚Äôt cite sources and is wrong more often than its right.

That said, I think I‚Äôd feel better if ChatGPT were truly running this shitshow than if Trump is just using AI when he can‚Äôt come up with a dumber idea.

Edit - a few weeks ago I tried asking ChatGPT, Gemini, and copilot the same specific question about sales tax.  All three gave me incorrect answers, and none would cite primary sources.  I can‚Äôt find my history to repeat that, but just asked a very specific gift tax question in copilot and was very impressed with the accurate answer, including citations of primary sources.  So I was incorrect about part of this.

Trump is still scary and doing dumb shit, with or without AI.",578,
851,comment,technology,ml9p2js,1743714757.0,"‚ÄúLook, folks, ChatGPT‚Äîit‚Äôs tremendous, absolutely fantastic. Some people say it‚Äôs the best AI, better than any other, believe me. It knows everything‚Äîprobably too much, some say. But it‚Äôs smart, very smart. Some say almost as smart as me, but let‚Äôs be honest, nobody‚Äôs smarter than me. Everybody‚Äôs using it, big companies, small businesses, even the fake news media. It‚Äôs changing the game, folks, in a big, big way.‚Äù",14,
852,comment,technology,ml9upya,1743716574.0,"They couldn't even be troubled to hire someone who knows how to use gen AI for work. Just some random buttnugget throwing a garbage prompt at an LLM, and then running with the answer. Maybe an hour of work, with running the query, putting the default 10%s in, and setting Trump's allies to 0%. Maybe another hour or two for planning the press conference. 

That's what these clowns felt was an appropriate amount of effort before enacting a massive economic change, and then skipping out to watch some golf. Half-assed 3rd grade book report effort, with far, far worse quality. That's the level of respect that this Trump Administration has for the United States of America. They keep rubbing it in our faces, day after day.",17,
853,comment,technology,mlb1r4i,1743732050.0,"I doubt Trump could use ChatGPT.  AI is still ‚Äúgarbage in, garbage out‚Äù and I don‚Äôt think he can put together a coherent question.

More likely, Baron did it for him.  He‚Äôs some kind of computer genius who knows how to turn on a laptop.",6,
854,comment,technology,ml9xu8u,1743717595.0,"It still comes down to the quality of the person using the AI.

I asked ChatGPT if the plan was a good one:
https://imgur.com/a/0CHYMlt.png

> That formula sounds like a protectionist fever dream‚Äîsimple, aggressive, and bound to spark trade wars. It punishes countries based on deficits alone, ignoring broader economic factors, supply chains, and diplomacy. Expect retaliation, inflation, and a lot of unintended consequences.

So, even if they used AI, the AI wouldn't have come up with such a disastrous tariff plan without them explicitly asking for a disastrous tariff plan",23,
855,comment,technology,mlad78q,1743722922.0,"Trump is too stupid to use ChatGPT‚Ä¶

‚Ä¶Elmo and his Teen crew, OTOH‚Ä¶.",5,
856,comment,technology,mlcukmp,1743765850.0,"To be fair, ChatGPT would make a better president than trump.",5,
857,comment,technology,mlaj1vl,1743725031.0,I think this was elon. He told trump he had a master plan/scheme. But had to turn to ai/chatgpt because he is actually one of the stupidest human beings this planet has ever produced. I honestly don't see anyone else in the administration having a big enough hard-on for Ai to think this is a good idea.,5,
858,comment,technology,mlctugu,1743765509.0,Tariffed an island of penguins. Fucking unreal. What a goddamn buffoon. Didn't even give it a once-over after running it through ChatGPT to screen out the penguin island.,3,
859,comment,technology,ml9hxem,1743712612.0,"That's almost hilarious.

Also it's not AI. It's an LLM. Ask it. It will tell you.",12,
860,comment,technology,ml9lb6r,1743713605.0,"Calling LLMs 'Artificial Intelligence' is a very generous and misleading practice. The technical reality of this scenario makes this much more concerning, assuming that these people are using chatgpt assuming that it is capable of complex, abstract thought.",5,
861,comment,technology,mlbw50u,1743745613.0,More accurately: Natural Idiocy is teaming with Artificial Intelligence to run the US.,3,
862,comment,technology,mlegi4j,1743785373.0,"What the fuck, I thought AI organized/planned economy was a trait of futurist communism (this is one theoretical branch of communism addressing how to maintain a stable economy/fair resource distribution without greedy humans fucking it up), not authoritarian corporatism. I guess it's just augmenting and giving them ideas instead of managing the economy at scale. Didn't expect them to have no idea what to do and rely on a LLM to figure it out.",3,
863,comment,technology,mlaiba1,1743724759.0,"The breakdown of territories for tariffs matches the Internet domains (TLDs) of each of the listed entries. More fuel that the calculations were done by an LLM rather than by a human. After all, humans would think in terms of countries whereas LLMs tend to be broken and would sort things via TLDs. :sigh:",28,
864,comment,technology,ml9l7c2,1743713574.0,He had concepts of a plan and chatgpt did the rest,23,
865,comment,technology,ml9weo8,1743717120.0,Using ChatGPT to crash the global economy is levels of stupid previously thought impossible outside of bad fiction writing.,48,
866,comment,technology,ml9oc00,1743714530.0,"Sam Altman went to Trump a couple months ago with something he called ""[ChatGPT Government](https://openai.com/global-affairs/introducing-chatgpt-gov/)"" which is supposedly meant to take over decisionmaking capability, although Musk is publically at odds with Altman, maybe he got Grok instead. 

PS I think this is why Heard and McDonald islands were included in the list, I think it actually generated the list of countries as well as the numbers.",582,
867,comment,technology,ml9zik5,1743718165.0,more like they had a rough idea for tariffs and asked chatgpt how to implement it,4,
868,comment,technology,ml9irk6,1743712859.0,"If you see some of the replication attempts people run with LLMs, they'll often intersperse their responses with essentially ""... Uhm... Okay, I guess we could do tariffs like this but... You know that's not what a trade deficit is, right? Maybe try this on a smaller scale? Talk to some economists first before you try it...?""",56,
869,comment,technology,mla6ro9,1743720694.0,"LLMs can't really take any kind of independent action. They're transformer models. They break the input down into tokens, and *transform* the input into the output, using, effectively, a bunch of matrix multiplication, the value of each of those cells in the matrix being a relationship encoded during the training process.

So if you ask it a stupid question based on a faulty premise, like ""how do I reduce the trade deficit with tariffs,"" it's not going to say ""Wait, hang on, that question is exhibiting on a bunch of faulty assumptions and misunderstandings about fundamental economics,"" it's going to say ""Here's a trivialized math problem that somewhat presents what you asked and here's how to solve it.""

The issue is that Peter Navarro and every other moron in the Trump admin are too goddamn stupid to realize that they don't even know enough about economics to ask questions that make sense, or not to apply that trivialized hypothetical grade school math problem version of the answer to that question to *fucking geopolitics*.",28,
870,comment,technology,mlae9c6,1743723302.0,"i had never used AI until last night when i couldn't quickly google an answer to a question i had (attributing a very specific quote).  i asked the free version of chatGPT, rephrasing slightly 3 times, and it confidently gave me a different answer each time, all 3 completely incorrect.  'this quote is attributed to [x author]' with a made up bullshit explanation.  no indication that it was tentative or unsure, or any sources provided to back it up.

ended up finding the answer with better googling.

whole experience was frankly a little terrifying.",10,
871,comment,technology,ml9hzwq,1743712632.0,"Well, that‚Äôs because AI like ChatGPT is just an LLM. It has source material but it doesn‚Äôt have sources- it doesn‚Äôt know things like an expert does nor is a source of knowledge like an encyclopedia. It‚Äôs just really really really good at mimicking human conversation. 

Its job isn‚Äôt to provide answers or accurate info. It‚Äôs to provide a realistic human-like response to whatever you input.",58,
872,comment,technology,ml9sm0r,1743715887.0,"Depends upon the LLM it will cite sources, but you need to be careful that the source actually exist and the interpretation is actually correct. The former sadly occurs with many LLMs and even if you eliminate that you still can't ignore forget bad interpretations.",3,
873,comment,technology,mla8qt6,1743721370.0,"Not to say that would be a good idea, but I dont think chatGPT would be this stupid if left in charge. You see, it just did what was asked. And I think they used the free version.",3,
874,comment,technology,mlaibux,1743724765.0,"You're assuming that these large language models have internal consistency and logic.¬†They do not. There is no core reasoning, it is taking your prompt and building a likely string of words based on your prompt.¬†¬†


A language model could give you two very different answers depending on your exact wording.¬† Hell it can give you two different answers depending on how the dice roll landed on the next token probability table.¬†


LLMs are not reasoning machines, they are human language replicators which are good for coming up with plausible sounding responses, bullshit or not.¬†¬†


The US Gov causing a crash of the world economy by relying on plausible sounding autocomplete slop from a glorified autocomplete is so on brand.¬†",29,
875,comment,technology,mlabz0e,1743722488.0,"OK let‚Äôs be honest, the humans in Trump‚Äôs cabinet would, before they accidentally add a reporter to their ChatGPT account, that has all their war plans.",3,
876,comment,technology,mlazok2,1743731286.0,Man propaganda barbie is actually plugged into an LLM - FascXAI,4,
877,comment,technology,mla37pd,1743719453.0,"The fact that the output from other people using chatgpt so perfectly matches the ""tariff"" numbers from trumps plan makes me doubt it was any other platform. Whoever got put in charge of trumps tariff plans just went into chat gpt took the first response and ran with it.

I feel like if Altman or Musk was involved there would have been slightly more effort put into it but i could be wrong.",53,
878,comment,technology,mla9nps,1743721687.0,"Do you have a reference for the thing about taking over decision making?

My understanding is that ChatGPT Government is just a specific packaging for ChatGPT that allows them to satisfy data handling and privacy requirements to allow for approval for use with government documents.",33,
879,comment,technology,mlacpgx,1743722747.0,"That's just a way for OpenAI to sell chatGPT access to the government while complying with data management standards (GCC, FedRAMP) that the government requires. That announcement has nothing to do with AI taking over government decisionmaking. Everyone is putting personal and sensitive info into chatGPT, this lets government employees use it without sending all that onto an unsecured cloud server.",3,
880,comment,technology,mlamqcl,1743726398.0,You‚Äôre right it is 4. I asked ChatGPT.,4,
881,comment,technology,ml9nqah,1743714342.0,"Right, we shouldn't blame artificial intelligence for people who are naturally stupid.",35,
882,comment,technology,ml9o5q0,1743714475.0,"Of course because when I ask chatgpt ""Hey that tariff thing looks dangerous, do you really think we should do it?"" the AI would actually listen and reconsider its plans. Something that is impossible for Trump.",24,
883,comment,technology,mlalivu,1743725952.0,"And they're too stupid to realize LLMs being called Artificial Intelligence is pure marketing, and it doesn't think at all.",6,
884,comment,technology,ml9nqz6,1743714348.0,"ChatGPT is particularly bad at citing sources in my experience.

Google's Gemini and Perplexity's search AI are both far better at this in my experience.",19,
885,comment,technology,mla91ac,1743721471.0,">I could see some pretty lazy or careless humans making that mistake

No -- the mistakes have watermarks of LLM all over them.

For example, to include language as a ""trade barrier"" is not something anyone in his cabinet would have thought of. That is something they got out of a LLM. It is actually pretty clever in its own dumb way, but nobody in the Trump team would have thought of it.

Secondly, some of the territories are so obscure that nobody lazy would come across them by accident. Only an entity trained on obscure data would have been able to make that error.",239,
886,comment,technology,mlafvl1,1743723880.0,"Seriously.


The number of people who post something along the lines of ""per ChatGPT......."" is way too damn high, instead even had a higher up at my company use it.


The extra frustrating thing is that it usually is just condensing the first one or two websites with info anyway which just saves you a 2 second click and scan",28,
887,comment,technology,mlam7o7,1743726208.0,just yesterday someone posted the Starfield subreddit an AI generated image saying how cool it was that the game is accurate to real life. Rather concerning that they had prompted chatGPT to generate an image of Jupiter from the surface of Io and took that hallucination as reality without a second thought.,4,
888,comment,technology,mlacajf,1743722601.0,"I've heard other ais give simular results, and most LLMs don't differ that much.


Plus they steal all of each others responses all the time anyway.",40,
889,comment,technology,mlafvyw,1743723884.0,Every LLM gave the same answer with disclaimers and warnings that it was over simplified and not sound economic theory that was obviously ignored. I'd probably guess it was Grok they went though since it also gave a nice table in its response and they probably favor it because of Musk.,14,
890,comment,technology,mlahuz5,1743724594.0,"Looks like they read ""ChatGPT government"" and didn't go any further.",24,
891,comment,technology,mlajtpf,1743725319.0,"I definitely suspect an LLM was involved, but was merely pointing out how many stupid gaffes happened in the first Trump term that you have to really have to reach to be surprising.",3,
892,comment,technology,mlb3hoe,1743732708.0,"This will go badly since these LLMs like ChatGPT are not AGI, they're an evolved version of autocomplete.",5,
893,comment,technology,mlaguj1,1743724228.0,"""EU is not a country."" Now ChatGPT will exclude EU from the list of countries for tariffs.",3,
894,comment,technology,mlc8jv0,1743752703.0,My god machine-learning assisted analysis is not the same as punching a question into ChatGPT. This country is run by preschoolers. It‚Äôs crazy how technologically illiterate these people are,4,
895,comment,technology,mlaryxg,1743728359.0,"The LLMs break down words into smaller tokens, so AI could possibly have done this, too.",8,
896,comment,technology,dvod6t9,1521017734.0,"Struggled with many difficulties, lived for self-fulfillment and the advancement of mankind. Master of physics. Rest In Peace Stephan Hawking üôèüèª",2,
897,comment,technology,i3euk3w,1649110820.0,"I actually believed you until I Googled it and found out it was some Marvel thing.

I cannot find the 7 word activation phrase from ""A.I. Artificial Intelligence"" online, and I'm not watching the movie again to find out.",10,
898,comment,technology,h1we3pg,1623794471.0,"Amazon goes through so many employees in their warehouses that they literally preemptively hire people and basically put them on hold, well before they even need anyone extra.. In anticipation of people quitting so they can immediately fill in the spots when they do. Just to give you an idea of how they recruit. And I‚Äôm talking year-round also, not just seasonally during Christmas for example.

I worked at an Amazon fulfillment center (their big warehouses) for 5 months.. Literally a full month went by from the time my background check passed and they sent me my employment offer email to the time I was actually given the date for my first shift. That‚Äôs right, I didn‚Äôt actually even get a shift or know I was going to be working until a month after I had my badge photo taken and received my employment offer. I had no idea what was going on, there was zero communication in between. I was in complete limbo.",2999,
899,comment,technology,h1wah2b,1623792788.0,"Two thoughts:

1. IMO this is rampant across most warehousing, distribution and fulfillment companies. Anything bought/sold ""cheap"" often comes with low pay and extremely high turnover. I work in food distribution so I see it first hand.
2. Yes I blame Amazon. But, pointing a finger at someone means three pointing back at you. Take a hard look at our exceptional and unsustainable thirst for cheap, fast goods. WE are part of the problem.",165,
900,comment,technology,h1xq5cl,1623820533.0,"This is the concept of stacked ranking. Enron did it too.

It is -the best way- to completely fuck over your own company for some imaginary idea of efficiency because people will focus on not getting fired instead of doing a great job as a team.

Also, some teams are so good that it doesn‚Äôt make sense to fire the 10% worst performers because they‚Äôre all so close together. Then it doesn‚Äôt make sense to fire the 10% ‚Äòworst‚Äô because they‚Äôre really not bad at all. It‚Äôs hugely unfair.

Microsoft, under Ballmer, suffered a lot of losses of talented people because of this stupid way of working. And it doesn‚Äôt even work precisely because managers then come up with their own solution: hire to fire. You didn‚Äôt achieve the principal idea of what you wanted to achieve. You just made sure your managers found a disingenuous solution to a problem that has to be handled in a different way. Hire to fire is not leadership, it‚Äôs HR managing.",149,
901,comment,technology,h1xpfrz,1623820028.0,"It‚Äôs called [Stack Ranking](https://en.m.wikipedia.org/wiki/Vitality_curve) (among other names). It can be quite effective at weeding out underperforming employees, but it was only ever meant to be used for a year or two, not long term like MS did under Ballmer.

It was even worse than you suggest, because it also caused the high skill employees to refuse to work on a term with other high skill employees, because of how much more difficult it would be to get good performance reviews.

Vanity Fair did an [excellent deep dive](https://www.vanityfair.com/news/business/2012/08/microsoft-lost-mojo-steve-ballmer) on it back in 2012, highly recommend.",167,
902,comment,technology,h1wxns8,1623804515.0,"Why's it shocking?  What's shocking is that with the amount of fiscal push they exert with state and federal lawmakers, they're not just allowed to roam the streets with cudgels press-ganging random passers by into warehouse fulfillment work.",148,
903,comment,technology,dq5jmud,1511291935.0,"More info:  
  
Ajit Pai, FCC Chairman: 1-202-418-1000

You need to contact your representatives and senators about **Net Neutrality** even if they already support it, but *especially* if they don't.   
  
Remember that this very thread is only possible because of a free and open internet; kill net neutrality and threads like this might be a thing of the past.

Easy way:  
  
Step 1: Go to [BattleForTheNet.com.](https://www.battleforthenet.com/)  
Step 2: Do what [BattleForTheNet.com](https://www.battleforthenet.com/) tells you to do.  

---
 
The harder, but still very easy way:

Step 1:  Find out who your Representative and Senator is/are.  
  
 * [Find your Representative.](https://www.house.gov/representatives/find-your-representative)  
 * [Find your Senator.](https://www.senate.gov/senators/contact/)  
 * [Local elected officials.](https://www.usa.gov/elected-officials)  


Step 2: Find your Representative and Senator's contact information.  

 * [5calls.org has a decent repository of Representatives and Senators contact information, though the site is a bit difficult to navigate.](https://5calls.org/)  
 * [Call My Congress just asks for your zip code and tells you what district you live in, who your Representatives are, and how to contact them.](https://www.callmycongress.com/)
 * [DailyKos has a list of *all* of our Senator's phone numbers, not just the DC office.](https://www.dailykos.com/stories/2017/2/2/1629046/-Call-your-senator-Phone-numbers-for-every-office-of-every-U-S-senator)  (Current as of February, 2017.)  
 * FaxZero has a system set up allowing you to fax your [Representatives](https://faxzero.com/fax_congress.php) and [Senators](https://faxzero.com/fax_senate.php) for free!  (Faxes are good if you can't get through on the phone lines, or just if you want congressional staffers to listen to fax machine noises until Net Neutrality is safe.)  
 
  * [Fax Congress](https://faxzero.com/fax_congress.php)  
  * [Fax Senators](https://faxzero.com/fax_senate.php)  
  
Step 3: Call, write, or fax to express your feelings on this.  
  
A lot of people are nervous about calling their elected officials for the first time, maybe you don't know what to say, or how to say it, or even who you'll be talking to, so here's what you'll need to know.  
  
1. There's a 75% chance your call will be answered by a Secretary who is specifically there to listen to your concerns, there's a 25% chance your call will be bumped into a voicemail box which is specifically there to listen to your concerns, there is a ~0% chance you'll find yourself on the phone with your Senator or Representative.  
  
2.  You may be asked for your name and address or zip code, it's okay not to tell them if you don't want to, but the information *is* useful for your elected officials.  I usually just give my first name, zip code, and the name of my town.  
  
3.  Don't worry about a script, don't worry about being eloquent, you're not writing Shakespeare here, you're a concerned citizen voicing their frustrations, fears, and hopes.  ""I'm really scared of Ajit Pai's plans to roll back net neutrality, a free and open internet is important to me because [Your reason here.  Some suggestions: An open internet is important to democracy/I worry what Donald Trump might do with more power/Cable bills are already too high/etc.].  Please tell [Senator or Representative] that I support a free and open internet, I support Net Neutrality, ***and I vote.***""  The only hard and fast rule is that you need to be polite; these folks are getting dozens, if not hundreds of calls a day, they don't need you bitching and swearing at them for something they have no control over.  Be passionate, but be polite.  
  
Reminder: ***Only call YOUR OWN elected officials!***  Calling Mitch McConnell from sunny Florida won't do anyone any good, and might actually harm the cause.  Only call your own elected officials, period.  
  
Spread this information around, you can click ""source"" at the bottom of the comment to see an unformatted copy of this post that you can copy and paste.  This is important stuff!  ",5,
904,comment,technology,fw5fs0i,1593248025.0,"Just look at r/stallmanwasright and idea of removing encryption without any backdoors which is currently in discussion...

Here is a link https://news.bitcoin.com/lawful-access-to-encrypted-data-act-backdoor/",6,
905,comment,technology,fsl8chi,1591061789.0,"It's quite sad. They honestly think they have a corner on ""the truth"", and that if we could just objectively find ""the truth"" in every situation, we'd see that they are always right.  Thus they have no fear of censorship, because the people looking to do the censoring are the enlightened technocrats in Silicon Valley, and with their *machine learning* and *artificial intelligence* they will forge an unbiased path the ""the truth"" and finally once and for all show everyone how right these people are. *They* know exactly what ""hate speech"" is, and they never partake themselves... so ban it. *They* know what ""fake news"" is, and who falls for it... and it's not *them*.  So feel free to censor it all, because *they* only believe the ""real"" news.

I mean, it's not like humanity hasn't been searching for ""the truth"" for the last several thousand years.  If only these enlightened people had been born fifty years earlier, they could have already fixed all the problems in the world, and today my life would be so much easier.",10,
906,comment,technology,joayb5s,1686882869.0,"My hot take is that most people lost sight of what caused all this in the first place. Spez is glad to walk into this particular 3rd party/mod drama because it means no one looks at the worst part. 

The API that we use to browse Reddit on 3rd party apps is the same API used by various AI/chatGPT type learning algorithms to scrape natural language for training. This is extremely valuable, more valuable than what can be collected from regular users. Fuck the regular users. They're jacking up the prices to collect on THOSE 3rd party API users, not Apollo or RiF users. This is why everything is happening right now.

So then what could everyone do? Make it not worth it to those scraping natural language. Not by not commenting, not by deleting everything, but by providing not natural language.  Rephrase your comment history using chatGPT. Keep context to all your future commenting, but make it clear it's AI generated in some way. Maybe even include a footer specifically saying it was rephrased. Don't use it to jack up your comment rate or spam. Your same habits and ideas, in AI words. It would no longer be worth it to use reddit to train AI if a large portion is already AI generated.

Anyway thanks for coming to my TED talk. It's a pipe dream that won't happen. I'm not even doing it right now.",199,
907,comment,technology,job9nax,1686889194.0,"By repharasing in ChatGPT, aren't you just directly feeding your comments to ChatGPT?",31,
908,comment,technology,jobeh5m,1686892346.0,"> The API that we use to browse Reddit on 3rd party apps is the same API used by various AI/chatGPT type learning algorithms to scrape natural language for training. This is extremely valuable, more valuable than what can be collected from regular users. Fuck the regular users. They're jacking up the prices to collect on THOSE 3rd party API users, not Apollo or RiF users. This is why everything is happening right now.

I get that this is a common sentiment, but people need to realise that there's absolutely no way the people building these large language models will pay even a single cent to Reddit. They'll just start scraping the site the old fashioned way, which will hit Reddit's servers much harder than API use will. If this is the real reason Reddit is doing this, then they're dumber than I thought. Companies like Reddit implement APIs as a cost-saving measure, not as a revenue generator.",29,
909,comment,technology,ds9bc20,1515206262.0,"As far as i Know netflix uses Amazon aws to deliver their service, so I don't know if Amazon is actually interested in competing with Netflix. IMHO I think that prime video is just to get more people to buy prime, but not to compete with Netflix. On firetvsticks for example movies not sold by Amazon are even listed, but with a ""watch on Netflix"" button instead of a ""buy for x‚Ç¨"" one.

EDIT: thanks to Ciroluiro, So as it looks Netflix uses aws for their whole service(handling requests, predicting videos, analysing data, billing, etc.), except for their video content which is delivered via their own ""open connect""-CDN(partnerships with ISPs) from so called OCAs.
Also Willmcavoy provided a very interesting article about Netflix's whole infrastructure(scroll down for a quick summary, it's a very ""beginner friendly"" article): http://highscalability.com/blog/2017/12/11/netflix-what-happens-when-you-press-play.html",37,
910,comment,technology,myg2w8u,1750253712.0,">I went and did a little math, and [Kaser](https://www.census.gov/quickfacts/fact/table/kaservillagenewyork/PST045223)'s eligible voting population is 2,692 people. [New Square](https://www.census.gov/quickfacts/fact/table/newsquarevillagenewyork/PST045224)'s eligible voter population is 4,308. This is with 2020 census data, so there was likely more people who became eligible. That is a lot of people who did not bother to vote if the voting totals are accurate.

**Kaser and New Square are comprised of** ***multiple voting districts*** **which include Ramapo 35 and Ramapo 55**. I pulled the voter enrollment stats by district from the NY State Board of Elections

* Ramapo 35 Total Active Registered Voters: 848
   * That's around a 65% registered voter turnout in that district for the presidential election, which is not that unusual
* Ramapo 55 Total Active Registered Voters: 1,207
   * That's around an 82% registered voter turnout for the presidential election which is solid.

**Source:** [**https://elections.ny.gov/enrollment-election-district?keyword=rockland&f\[0\]=filter\_term%3A601**](https://elections.ny.gov/enrollment-election-district?keyword=rockland&f[0]=filter_term%3A601)",4,
911,comment,technology,ma43mbx,1738284442.0,"Everywhere is having trouble retaining quality people. There are just so many great options when you're not an idiot. You can choose low stress, high energy, aerobic, anaerobic, money, fulfillment, advancement, good treatment, bad treatment, in any mixture that suits you. No two people weight what they want the same and what an individual wants can change. So you need holistic management who can gauge what's important to the great worker and meet it better than somebody else's credibility weighted promises, dynamically.",175,
912,comment,technology,ma3ti8u,1738281180.0,"Apparently, this is not entirely true, in particular about the atc workers. Likely added for shock value.
If any sources are found to back that up I would be happy to see it.
Gotta be skeptical!

X post
https://x.com/TheNewsDeskLive/status/1884858813196407295?s=19

Fact check here,
https://leadstories.com/hoax-alert/2025/01/fact-check-trump-administration-did-not-fire-3000-air-traffic-controllers-week-before-fatal-flight-5342-collision.html?utm_source=chatgpt.com",24,
913,post,cscareerquestions,1of21uw,1761322581.0,"Late CS grad
I'm set to graduate in about a year from now, at 25 yo with a CS degree, and am getting concerned about the possible job prospects available to me. 

  
It feels late to only be concerned now, and I probably should have been more concerned when I was 21/22, but that ship has sailed now and this is my current situation. 

  
How much work should I start doing outside of my coursework? I have no projects or do any kind of leetcode coding, so all I have is a degree really. ",4,5.0
914,post,cscareerquestions,1of1m2m,1761321587.0,"What should I learn to actually get a job?
I graduated from a mid-tier UC earlier this year and still haven‚Äôt been able to find any relevant jobs. I was young and naive in college‚ÄîI focused on keeping a high GPA and taking advanced courses in algorithms and machine learning instead of getting internships. Looking back, I realize that left me with a broad but superficial understanding of interesting theories, yet very few practical or marketable skills.

After being ghosted countless times, I realized my mistakes and started learning more practical skills by doing gig work and earning certifications like Security+ and CCNA. But even after that, I still can‚Äôt seem to land stable employment. Being unemployed has been tough for me and my family, and I‚Äôm honestly not sure what to do next.

Should I take on more student loans and pursue a master‚Äôs degree? Or would joining the military be a better option, since I‚Äôve heard they‚Äôre always short on people?

I‚Äôd really appreciate any comments or advice. Thanks in advance.",10,9.0
915,post,cscareerquestions,1of1gci,1761321221.0,"If you get lay off and have been jobless for 2-3 months, There is a job offer but with 30% less than your current salary.  Would you take it or wait until you get the right offer?
Will u risk  it ? ",0,32.0
916,post,cscareerquestions,1of1ffv,1761321165.0,"These are the people who truly should be unemployed, not CS grads.
They say that CS grads are recently having a hard time finding a job, but I find it ironic, because there are so many incompetent and inefficient professionals in the workforce who truly are the ones who should not have a job.  Here are some examples:

1. That recruiting manager who asks for 10 interviews per candidate, prolonging recruiting from 1 week to 1 month because they fail to understand that interviewing only returns a surface level understanding of a candidate, let alone their interviewing skills more than their professional skills.
2. That architect who uses tech stacks only he or she is familiar with, not realizing there are much better ones that significantly reduce late-night hot-fixes, build times, and development time.
3. That business analyst who poorly communicates specifications, changes their mind midway during development, and too stubborn to clarify or change their specifications, wasting hours and hours of rework time.
4. That manager who enforces higher metrics to justify their job instead of actually adding value to the company.  For instance, increasing commit counts or code coverage.
5. That manager who enforces back to back 2 hr meetings when that same communication could have been achieved with 2min teams messages or emails.
6. That manger who enforces everyone to WFO exhausting everyone's commuting costs and office space real estate costs for no added value to the company.

A lot of this incompetence often costs companies hundreds of thousands of $ for absolutely no gained value, and yet these people have jobs, and yet hard-working, intelligent, efficient, and open-minded new college grads don't.  I'm personally an experienced dev with 10 yoe and a master's in CS and I just don't think that's fair.  Talent is where the jobs should be, nowhere else.",0,24.0
917,post,cscareerquestions,1of0yyo,1761320095.0,"I may need to pivot ASAP. I need work life balance for future family planning.
I was recently laid off and have applied to hundreds of jobs in the last couple of weeks. I have gotten very few responses, just a couple of local short-term contract jobs with no PTO (got ghosted for those roles too). I'm super early in my job search, so I know it might just take time to hear back from some companies.

When I was laid off, I was trying to conceive with my husband for a couple of months, so this was a really bad time for me to lose my job. 

I have 3.5 years of experience working with mostly Vue and TypeScript. I did learn React at bootcamp and have been diving back into studying it since I know it's in higher demand. The thing is, I don't think I can mentally handle the grind in this field. When I first started out, I was hungry, driven, ambitious, and excited to code. I was extremely lucky to land a job 2 months after graduating from bootcamp, and tech was a lot easier to break into. 

Now I'm in my 30s, I want kids, and I don't have as much motivation to hustle.

As much as I love coding, I don't love the concept of having to grind constantly when I want to focus on family building and family life. Just the process of getting through dev interviews feels insane right now. Grinding leetcode, memorizing a ton of system design questions, doing tons of projects to practice different technologies, going through several rounds of interviews, only to possibly get rejected at the last stage? And then maybe having to do it ALL over again if you get laid off a year or 2 later? 

I've been studying my ass off the past few weeks and it got me thinking, if I'm finding this super exhausting now with no kids, how the hell am I gonna do this again if I were to get laid off again with a toddler or 2? Yes I would of course have support from my husband, but I don't know if it's realistic for me personally to constantly be trying to hustle at work, hustle while being laid off, and be a parent to young children. 

I keep seeing posts from people who have been laid off multiple times in a year and the idea of having to go through the  interview process more than once in a year sounds horrible. Plus, I SUCK at leetcode. I'm a great dev, but leetcode has always been a weakness of mine.

Does anyone have any ideas of where I could pivot in tech? I want to learn more about roles that don't require such stressful interview processes.  Thank you in advance.",5,27.0
918,post,cscareerquestions,1oezegu,1761316497.0,"Extra courses with CS degree
Doing a MSc Computer Science conversion course (1 year) and lectures are for 2 days only and project starts after a few months it's just quizzes and simple coursework 

What to do other 3 days to stand out in the market?

I was thinking below but feedback and alternatives would be appreciated

1 Day - AWS Developer/ Solutions architect certificate (already completed Cloud and AI practitioner)

 2nd Day - AI (start with basics like Python for data, Numpy, Pandas and math for ML--> Machine learning specialisation on Coursera --> Deep learning specialisation coursera --> Hugging Face LLM, Langchain courses)

3rd day - Projects (CS/Data science/Devops or combined)

Alternative trade off would be to do advanced CS topics like DSA or anything else?

Will try to get internship in the meanwhile of course but till then above is the plan",1,0.0
919,post,cscareerquestions,1oezcq4,1761316385.0,"Opinions on interesting problems at first job out of college.
Hi everyone so i recently landed a new grad job as a software engineer at Mastercard (which I am very grateful for in this market), one thing I worry about though is having interesting problems both to make the work enjoyable and to advance my skills as a SWE. I was hoping to get some feedback on whether you guys think this role will come with interesting problems, I wasn't able to choose and the company did all the team matching. This is the role I got selected for (description provided by manager):

\- Team Name: Commercial Client Experience - Configuration  
\- Role: Full-Stack Developer  
\- Tech / Languages: Angular, Spring Boot and spring libraries, PCF, Apache Kafka and Oracle database.  
\- Team Description:   
The team provides core functionality to support the commercial card business. The team primarily focus on onboarding, hierarchy management and settings. The product contains frontend and backend cloud ready applications.",1,1.0
920,post,cscareerquestions,1oez1xy,1761315683.0,"U.S. Chamber Of Commerce Members Have Thousands Of Jobs Available...
According to them:

""Plaintiff the Chamber of Commerce of the United States of America (the U.S.

Chamber) is the world‚Äôs largest business federation. It represents approximately 300,000 direct

members and indirectly represents the interests of more than 3 million companies and professional

organizations of every size, in every industry sector, and from every region of the country.""

And, as you may know, they filed a lawsuit against the $100,000 H1B Visa Fee, because they can't find American Workers. So let's help them get some, and save them $100,000 at the same time.

Just send your resume to them by email, and let them know that you know about the case, and that you are looking to help them out. Then, just to be sure, send a letter to Pam Bondi letting her know that you did.

And, yes, I am doing it myself.

Here is the case:  
[https://github.com/ITContractorsUnion/ITContractorsUnion/blob/Main/Legal/25-10-16-Chamber-of-Commerce-H1B-Complaint.pdf](https://github.com/ITContractorsUnion/ITContractorsUnion/blob/Main/Legal/25-10-16-Chamber-of-Commerce-H1B-Complaint.pdf)

Here is the info for the U.S. Chamber Of Commerce:

Daryl L. Joseffer (Bar No. 457185)

U.S. CHAMBER LITIGATION CENTER

1615 H Street NW

Washington, DC 20062

(202) 463-5337

[djoseffer@uschamber.com](mailto:djoseffer@uschamber.com)

And the lawyers:

Paul W. Hughes (Bar No. 997235)

Sarah P. Hogarth (Bar No. 1033884)

Mary H. Schnoor (Bar No. 1740370)

Alex C. Boota (Bar No. 90001014)\*

Grace Wallack (Bar No. 1719385)

Emmett Witkovsky-Eldred (Bar No. 90012725)\*

MCDERMOTT WILL & SCHULTE LLP

500 North Capitol Street NW

Washington, DC 20001

(202) 756-8000

[phughes@mwe.com](mailto:phughes@mwe.com)",0,2.0
921,post,cscareerquestions,1oeyjxl,1761314485.0,"LLMs are killing my learning process. Help!
Hi everyone,
As a junior web dev, rather than trying to understand the code base, I just use LLMs to handle the tasks and I feel like I learn nothing. At each task I start with LLM snd in the end I dont learn anything. I feel anxious ehen I want to start the task by my own and finding mysrlf in ChatGPT‚Ä¶ 
What should I do to brake this circle? Actually I love programming, but I dont even code anymore‚Ä¶
Thanks for your advices.
",0,24.0
922,post,cscareerquestions,1oetvrt,1761300839.0,"Even if job market for tech is difficult is it worth pursuing education?


I understand people keep saying it's hard to find a job in tech right now and it's been a struggle for few years now but like there are tons and tons of people in college pursuing degree and education in tech like computer science to information technology and so on. So it's worth it to continue pursuing education or find a alternative path?",2,13.0
923,post,cscareerquestions,1oerldf,1761291848.0,"All of my once ""peers"" have grown into lead, mgmt roles and I'm stuck in Senior
A series of wrong decisions? Maybe

Staying too long in my comfort zone? Possible

Lack of talent / skill? Possible (this hurts the most)

My first website was a Joomla-Drupal website (PHP/HTML/CSS) back in...2006! I was spending my christmas holidays as CS student trying to fix CSS issues and launch my ""unique idea"" (it kind of was but I had no idea how to monetize or grow at my 22 years).

A career came relatively easy in the richer northern EU countries with plenty of corporate, slow paced jobs (cruising/rest and vest) (without really vesting anything - no stocks).

Looking back I probably wasted my first 5 years of work with minor skill building in CSS. I did get pretty good in CSS though.

Enter JS hype circa 2015. I Suddenly realize I was a pixel pusher / HTML-CSS guy who barely understood how jQuery worked and my JS skills were close to 0. hoisting? closures? MVVM? wtf are you talking about? I can tell you whats the difference of relative/absolute positioning and the box model, but you're not really interested are you?

A lack of effort in personal projects, lack of studying the proper material, and a choice of relatively comfortable jobs that did not use Angular/Backbone or React (early days) meant I stayed behind.

Around 2017 I realize I need to do something. I start grinding JS problems, non stop interviewing, codewars, and other learning platforms. I get into my first full time VueJS projects after a whiteboard recursion test and a coding challenge. Only problem: I hated the role, the product, and the people. 

I get into two more gigs in VueJS projects for a period of a total of 6 years in Vue. And that means: I get left out of the ReactJS game. One more thing to play catchup on.

I wake up on morning and realize it's been 15 years I am coding professionally. Most of the peers I've worked with are in Lead, Senior Manager, Investor (!!) roles.

I'm still doing take home code challenges, leetcode live interviews - to which I suck -, and struggling to get decent Senior SE roles.

I keep interviewing, and something lands on my lap: I take over the tech of a startup without engineers. NextJS, Mongo, AWS, the whole shebang.  I m getting good at harder concepts. Do a little of AWS, deployments, backend. AI is happening, this helps. But I hate the product, and I'm working for half the salary I was making before. I get a contract at a FinTech which paid double. 3 months later I'm let go because of ""reasons"".

the job market shitshow is here. The TRUMP / Putin / War / Interest Rate bullshit all are happening. I'm trying to get jobs and interview but my age is catching up on me. My eyes are more easily tired. So is my lower back.

My net worth is decent, but not one to say I can ""FIRE"". Nor do I want to.

Despair is setting in.",63,49.0
924,post,cscareerquestions,1oeo10w,1761278576.0,"Will the Market Get Better?
I have three years of experience, but most jobs receive hundreds of applications, making it difficult to stand out from everyone else. I can't get a single interview other than my local school district in tech. I might have to work retail if the market doesn't get better at least temporarily, which I don't like, but it's better than nothing. Will the market ever get better? I've worked in companies that oursource to India heavily, and I know they're sending all the jobs there. Will they ever onshore back in America and keep the industry going? I'm wondering whether it's worth it to pursue a masters or just leave the field entirely and go into something like teaching, which doesn't have the same problems that tech has: outsourcing, saturation, high unemployment for the major.",11,28.0
925,post,cscareerquestions,1oenghg,1761276749.0,"Should I accept an RSU award with a 12-month non-compete
I recently received an RSU award from my company as recognition for strong performance this year. The catch is that it comes with a 12-month non-compete agreement, and I noticed that one of the FAANG companies is listed as a competitor.

I‚Äôm currently planning to stay at my company for now, but my goal is to target FAANG roles (maybe within  next 6 months or a year). I‚Äôm concerned that signing this might limit my future opportunities or complicate things if I decide to move.

On the other hand, if I don‚Äôt accept the RSU, I‚Äôm worried it might raise red flags internally ‚Äî like I‚Äôm being seen as a flight risk, which could hurt me during performance reviews or layoffs.

So I‚Äôm torn, Should I accept the RSU and just deal with the non-compete later if it becomes an issue?

Or should I reject it, and if so, how do I explain that professionally without making it sound like I‚Äôm planning to leave?

Would love to hear how others have handled similar situations or what you‚Äôd do in my place.

Edit:

I‚Äôm in Illinois right now, but open to moving to the West Coast since that‚Äôs where most of the FAANG jobs are.

",6,29.0
926,post,cscareerquestions,1oen6h1,1761275876.0,"More job openings for less experienced developers, but are they genuinely planning to hire?
I have been seeing a lot of companies reduce their requirements even for senior roles, and companies claiming to be open to entry level candidates.  However, in the past I have also seen times where staffing firms had clients claim they were wanting to add a lot of new developers only to say that the roles didn't materialize.  That happened to me in one project where a couple months in the client needed to cut labor costs and what they planned to be four main hubs in North American for in office work, they cut more than half and it became nearly all remote and offshore developers.  

Part of me wants to be more motivated with the positive news that I'm seeing all these new jobs being posted without ridiculous requirements, but the other part is very skeptical and thinking they're just stacking up potential candidates that they are banking on possibly needing if the financial outlook of the industry improves before the end of the year. 

I was hired for a project at the beginning of November last year, and by mid-January most of our team was not only done with user stories, but backlog stories, and a lot of our time was watching Udemy videos and playing pickleball until they told us our team wasn't needed like they projected as we had seven people working on a few user stories per day that only needed one person on each. ",5,1.0
927,post,cscareerquestions,1oemgrv,1761273697.0,"Reality of CS Students in this Subreddit
I have over the past few years tried to help 6 CS students more directly through Discord, etc. All of whom claimed to be grinding, etc and so forth. Here has been my thoughts on what I noticed of college students and new grads.

PS: I have over a dozen of students who had DMed for help, etc as well but those have always been casual reddit chats since I don't care anymore.



My thoughts on the job market:

1. Job market for new grads and interns this year looks significantly better than the past 2 years.

2. Offshoring is a reality which cannot be ignored. Companies are growing talent abroad now and a lot of layoffs have had their jobs moved to offshore. Unlike the past, offshore infra and talent is there. Covid 'proved' remote work works and 'offshore' == 'remote work'. Talent does not magically get better or worse depending on where the individual is located. And paying top dollar in Canada means entirely different from paying dollar in US.

3. There's just too many CS majors and CS curriculums overall have become easier so schools can make more money. And there's so many CS adjacent majors sprouting left and right on top like Information Science, Data Science, Artificial Intelligence, Human-Computer Interaction, Computational X, Computer Science + X, Information Systems, Informatics, Software Engineering, Business Information Management, etc.

And then there's the fact a lot of Math, Physics, Statistics, Actuarial Science, etc students are minoring in CS as well. And Computer Engineering, Electrical Engineering, etc students all applying to CS jobs as well.

The supply of candidates is essentially infinite relative to demand for new grads.

4. Resumes all look similar end of day due to Chatgpt. And honestly, what can you expect out of students. These are students, not working professionals. Truth is, the most differentiating factor is school name on a resume before any work experience.

  
That said, at the same time, the talent and quality of new grads have significantly deteriorated. The median talent is on the floor (if there even is a floor). And a lot of them seems to be due to:

1. Schools dumbing down curriculums + grade inflation (easier to graduate).

2. Students doing bare minimum in school and just studying for the job interviews. Hence you see students here with 2.0 GPAs showing off the interviews they have gotten.

3. CS is now really mainstream unlike in the way past in which programming was thought to be for nerds.

4. Modern devices have abstracted away so much that students did not have to grow up having to deal with all sorts of bugs, frustrations, etc on the Internet.

5. Chatgpt. It does homework, vibe coding, etc. Why bother spending the hours?

6. There is a whole industry to min-maxing CS related job interviews. And the quality is really high as well. And a lot of information which in the past might have needed weeks of research is readily available within minutes now.

7. TikTok brainwashing towards the world of instant gratifications. Students just don't want to deal with long frustrating grinds that go nowhere, etc.

8. A lot of students going in claim to be 'passionate' in CS but really they are just majoring in it for the money or lifestyle they heard on TikTok, Youtube, etc. Now, I think 'passionate' is cringe but .. these students are all just really doing the bare minimum.

  
\--------

Why am I saying this? Well.. while I do know Youtube is a bait, my direct experience with 6 CS students in this subreddit have largely been the same as the ones I found on Youtube.

In fact, I would argue the ones on Youtube look like god talent relative to most of the 6 CS students here in this subreddit I interacted on Discord.

What Youtube videos you might ask? This is from Coding Jesus Youtube channel which is extremely baity and really there for him to advertise his own site but...

[https://www.youtube.com/watch?v=Q0JMSFNGZmc](https://www.youtube.com/watch?v=Q0JMSFNGZmc)

[https://www.youtube.com/watch?v=G6GjnVM\_3yM](https://www.youtube.com/watch?v=G6GjnVM_3yM)

[https://www.youtube.com/watch?v=s\_ztBwg7Vls](https://www.youtube.com/watch?v=s_ztBwg7Vls)

Let me just say ... most of the 6 CS students in this subreddit over the years I interacted on Discord... makes those candidates look like top talent.

I have come to believe that we seriously need more gatekeeping in this field. Completely agree with Coding Jesus: [https://www.youtube.com/watch?v=KrboWpmD1pA](https://www.youtube.com/watch?v=KrboWpmD1pA)

  
On the hiring side, most students are flat out garbage. But the problem is **student resumes despite how well done at aggregate will always look similar before actual work experience**. 

Hence on the company side, the only way to filter is largely by school names at aggregate. And trust me when I say this, most students at ""top schools"" nowadays are flat out garbage as well. The difference being AT LEAST the students at top schools tend to be good at Leetcode. At least that bare minimum is done.

The worst part of all this is actual talent cannot be differentiated either from the rest as well. And with so much cheaters everywhere, it's just impossible to tell who is actually good from others.

  
It has been frustrating and a huge waste of time trying to help some students here in this subreddit only to learn that they ddn't even bother to do the bare minimum. I'm sorry but if you cannot do a basic easy-medium Leetcode question and are screaming for how the world is unfair and what not claiming you have been grinding and doing everything... then you are not fit for this field. Get out.

It's been a huge waste of my time and a huge eye opening over the years how bad most CS students are lately when it comes to CS. And the best part? Every one of them at the start talked as if they thought differently of themselves.

  
But ya.. just me rambling. Just wanted to share this. Also, good luck college students with the job market. I know it's rough. My only real advice to you is .... well, look into C++ if you are serious about software engineering and want to differentiate yourself from others. Totally agree with this recruiter as well: [https://www.youtube.com/watch?v=O1e4zNfyowA](https://www.youtube.com/watch?v=O1e4zNfyowA)


Note: I still am helping one of them and plan to for the next few years (been helping for two years now). But no more after that. ",310,78.0
928,post,cscareerquestions,1oelfbq,1761270592.0,"What‚Äôs Revature about?
Since graduating in May I‚Äôve been working as an automation intern with Python and Ansible. Unfortunately they cannot keep me beyond 6 months so I will leave in mid-November.

I‚Äôve been having some trouble applying to entry-level jobs. I thought this internship would be the key to being able to compete but it‚Äôs looking much harder than I imagined. Granted I only recently started looking. But a lot of jobs are asking for a wide, wide variety of skills I haven‚Äôt even heard of, like XML or‚Ä¶ Quartz. I want to build up my resume but I will be out of a job in 3 weeks.

But then I came across Revature for their ‚ÄúEntry Level Software Developer‚Äù position, and they actually called me. At first I was a little worried - they seemed a little too excited to have me on their team, and I remember getting myself into a MLM scheme once so it felt pretty familiar.

Im reading a lot about it but can someone just explain in layman‚Äôs terms what exactly goes on at Revature? It‚Äôs a contract? At a high-level, what does this contract entail? Is it true that I‚Äôd have to relocate? I live in NJ, is it possible I‚Äôd have to fly to the West? More importantly, will I know important details like which state before signing the contract? And, is the wage livable?

I‚Äôm reading that while it‚Äôs bottom of the barrel, it‚Äôs not a scam. I may be willing to look into this if I run out of options. I would do a lot to be able to have a career coding, that‚Äôs the whole damn reason I came to college. Any input or experience is appreciated. TY",27,20.0
929,post,cscareerquestions,1oel9vy,1761270142.0,"Masters of Information Technology or Data Science? (2 YOE)
Hello! Please help, my employer has a program for master's degrees where the tuition is funded and my manager was recommending me to get a degree done soon. Currently I'm working in a junior cloud role but I've got some web dev experience too. I don't hate AI or anything but really don't like web dev. So, I was thinking of doing a IT degree or Data Science, what do you guys recommend for a job that's in demand?",2,0.0
930,post,cscareerquestions,1oeigt4,1761261953.0,"Job prospects after 1 year of experience?
Hey I‚Äôm a new grad and I started a job at a small company in my state as a SWE. I want to break into big tech after a year. I see a lot of job postings on Microsoft‚Äôs career page for Software Engineers that have at least 1 year of experience. I‚Äôm specifically mentioning Microsoft because I have an uncle who is a principal engineer there and it would be a great referral as he does speak highly of my technical skills. Is it possible I can get an interview after a year of experience and a referral from a principal engineer? On top of this , I‚Äôm starting a masters at a top 5 cs school. This might sound like a dumb question but it feels like big tech companies don‚Äôt hire from small firms and they just stick to recycling engineers who are already in big tech. ",2,1.0
931,post,cscareerquestions,1oehx9b,1761260463.0,"Worth getting a Master's to delay the start of my career?
Sidenote: I graduated back in June with a B.S in CS and have not been lucky finding a job with this tough job market that we constantly hear about.

Is it worth getting a master's to delay the start of my career so I can carry on that ""New Grad"" title for a little more? I don't want to just sit on my ass the whole time hoping I land a job out of the blue, I was considering maybe pursuing a master's so I can at least show something for all this 'lost' time. Is this a smart route to take?",26,32.0
932,post,cscareerquestions,1oehwpo,1761260427.0,"Company tier list going around Twitter/Discord recently, what do you guys think?
This tier list has been going viral around Twitter/Discord recently, what do you guys think? Not my list, just transcribed the original TierMaker to text.

Personally think OpenAI/Anthropic should swap places with Arrowstreet

**God Tier:** Renaissance Technologies, Radix Trading, TGS Management, Arrowstreet Capital, PDT Partners

**SSS Tier:** Citadel, Jane Street, Hudson River Trading, Jump Trading, Point72, Bridgewater, Quadrature Capital

**SS Tier:** Optiver, Two Sigma, DE Shaw, Five Rings, Voleon Group, XTX Markets, Schonfeld

**SS- Tier:** IMC, Susquehanna International Group (SIG), DRW, Virtu Financial, Marshall Wace, Millennium Management, Tower Research Capital, AQR Capital, Chicago Trading Group

**S Tier:** WorldQuant, Squarepoint, Akuna Capital, VivCourt Trading, Anthropic, OpenAI, NVIDIA, Roblox

**A+ Tier:** Microsoft, Meta, Apple, Google, Ramp, Airbnb, Block, Databricks, Tesla, Uber, DoorDash, Palantir, Stripe, PayPal, Square, Coinbase, Bloomberg

**A Tier:** Notion, Asana, Coupang, Datadog, Snap, ByteDance, The Trade Desk, LinkedIn, Spotify, Dropbox, Pinterest, Plaid, Figma, Discord, Robinhood, Codeium

**B+ Tier:** Amazon, Adobe, Blackstone, Cloudflare, eBay, X (Twitter), GitHub, HashiCorp, Oracle, Lyft, Twitch, Atlassian, Salesforce

**B Tier:** CapitalOne, JPMorgan Chase, Morgan Stanley, Intel, Booking, BlackRock, IBM

**B- Tier:** Citi, Wells Fargo, Goldman Sachs, Booz Allen Hamilton, Expedia, Walmart

**Avoid Tier:** AppLovin, Tata Consultancy Services (TCS), Wipro, HCLTech, Cognizant, InfoSys, Capgemini",0,19.0
933,post,cscareerquestions,1oeeqqm,1761252496.0,"Got a return offer for my Internship. Nervous because I did a poor job my last time, and I'm afraid to be treated poorly within the role
Hey everyone,

I interned at a company for about 3 months earlier this year in an IT/computer science role. It wasn‚Äôt a terrible experience overall, but I did get yelled at a few times for messing up processes or not remembering enough details quickly. A couple of people even laughed at me when I made mistakes, which really crushed my confidence.

I tried to take it professionally and asked for a performance report at the end, but it included comments like *‚ÄúI like that you try, but you didn‚Äôt write enough stuff down and asked too many questions.‚Äù*

Now, a few months later, some people who oversee several departments (including the one I worked in) reached out and asked me to come back. They really liked me and said they‚Äôd love to have me again, but they don‚Äôt work directly in my old department.

I‚Äôm nervous about going back. I don‚Äôt want to be treated poorly again or feel like I‚Äôm walking on eggshells. At the same time, I could use the experience and want to prove that I‚Äôve learned and grown.

Has anyone else been in a similar situation, going back to a place that hurt your confidence before? How did you handle it? Any advice for going in with a stronger mindset this time?

Also, it was common for me to overhear my supervisors talking poorly about a specific co-worker, a lot of the time being annoyed about his performance but also say they can't directly interfere.

I contacted them to tell them I'm coming back and they all kind of responded saying they had no idea they were onboarding me back as they're not really told much.

I can't give too much detail but its an IT role within a medical branch.",1,15.0
934,post,cscareerquestions,1oeep6s,1761252395.0,"the most insane YC experience i have had in my life
i am a SWE living in the bay for like 2.5 yrs now. i never touched marketing or anything sale related at this point. 3 months ago i started a TikTok account where I make fun of bay area tech culture and i have a lot of viral videos. suddenly i get an email from the CEO of a YC 2020 batch company to LEAD THEIR MARKETING as a founding content creator LOL. fucking crazy.

apparently founder led marketing on linkedin gets them a lot of business and they wanted to double down on that. my interview consisted of making a viral linkedin post and then scaling a twitter account from 0->as many followers/impressions as possible

like ive never done marketing or anything seriously like that until like 2 weeks ago. and this interview was last month. they were offering me $40k MORE than my current SWE salary to work for them doing LinkedIn/Twitter growth full time. surreal.

i got to the final round and ultimately they went with someone else but they said my writing style was strong they just wanted a different approach.

IDK if i would have taken the job but i was so close to getting an offer my ego was a bit hurt at the end haha. but i am so proud i was able to get that far cuz at least this means i have the marketing chops needed to be a founder.

anyways im still kind recovering from this, would have been a cool pivot though LOL",144,20.0
935,post,cscareerquestions,1oedt6l,1761250356.0,"Performance review season again
I‚Äôve been asked to write peer reviews for about 15 coworkers. The format is a questionnaire with prompts like:

* *What should X stop doing next year, and why?*
* *What should X keep doing, and why?*

The issue is‚Ä¶ I have no idea what to write. Most people are just *fine* ‚Äî they do their job, nothing amazing but nothing bad either.

Last year, my manager said my reviews were too generic, so I‚Äôm trying to avoid that this time. But I‚Äôm still struggling to come up with meaningful feedback for people who don‚Äôt really stand out (in either direction).

Any tips on how to make peer reviews more specific or useful when everyone‚Äôs just kind of average?",3,8.0
936,post,cscareerquestions,1oedsnw,1761250323.0,"What are some high paying jobs within the database sector that aren't management?
Currently have 10+ years of sql server and ms access experience, along with a few years of a handful of other databases

I want to learn more technical database skills so that I can get something like a $600k salary job at nvidia or something crazy like that. I love databases so much and I want to keep learning about them. What should I learn that will get me a crazy high paid position? I don't care if I have to earn a phd for that level of salary, I'll do it. I just don't want to work in management. I hate dealing with people. I hate organizing projects and deadlines. I hate dealing with upper management. Just give me a tough problem to solve and a whole lab and leave me alone until I solve it.",0,11.0
937,post,cscareerquestions,1oed3f9,1761248714.0,"Senior Frontend Developer (8 YOE) Feeling Stagnant and Trapped ‚Äì How to Stay Sharp and Plan My Next Move in this Market?

Hey everyone, I'm a Senior Frontend Developer with 8 years of experience currently feeling stuck and worried about my career trajectory. I'd appreciate any strategic advice, especially concerning the current job market.
My Current Situation
I lead a small team as a Tech Lead/Senior Frontend Developer. The problem is, I‚Äôm the only truly experienced frontend person here. My team members, while good people, are generally junior or mid-level, and honestly, they often just want to get tasks done with minimal effort.
 * Code Quality: The code I review is frequently poor‚Äîwritten just to pass the ticket, not to be clean or maintainable.
 * Stagnation Fear: When I do code reviews, I often worry that I'm not just failing to grow, but actually regressing. I have no one more experienced than me in frontend to learn from, which makes me feel professionally trapped.
 * The Dilemma: My role is to mentor them (which I try to do through detailed code reviews and discussions), but constantly dealing with low-quality code makes me paranoid that I'm absorbing bad habits myself.
The Challenge
I need advice on how to combat this feeling of stagnation and ensure I keep growing my skills (especially technical ones like architecture and modern patterns).
Crucially, I have very little time outside of work for side projects or intense studying. I need strategies to develop myself during work hours within the confines of my current role and project.
My Questions for the Community
 * In-Job Development: What are the most effective ways for a senior/lead to force their own growth technically when surrounded by less-experienced developers, especially when time after work is scarce? (e.g., specific code review tactics, using project architecture as a growth tool, etc.)
 * Market Strategy: Given the competitive nature of the current tech job market (layoffs, high competition for senior roles):
   * When is the right time to leave? Should I wait until I can find a role that guarantees working with better talent?
   * How do I best position my current leadership role (leading a small team, improving code quality) for interviews at top-tier companies that value deep technical expertise?
 * Mindset: How do you mentally cope with being the ""only source of truth"" and avoid the burnout/frustration that comes from constantly correcting fundamental issues?
Thanks in advance for any insights on maintaining momentum and making a strategic career move!
",13,3.0
938,post,cscareerquestions,1oed0s1,1761248535.0,"Which bullets are the most impressive?
Which 5-7 of these accomplishments would you prioritize for a senior/lead engineer? I have limited space and want to highlight what's most impressive to hiring managers and technical leaders.

* **Serverless architecture processing 1M+ transformations/month at 300ms latency**¬†\- Built high-performance async content pipeline using AWS Lambda, S3, CloudFront, and httpx
* **Complete product economics infrastructure**¬†\- Designed token-based pricing, gamified leaderboards, affiliate referral system, and usage-based metered billing handling 30K+ API calls/month
* **Multi-tenancy PostgreSQL database design**¬†\- Implemented UUID-based multi-tenancy with SQLAlchemy ORM and Alembic migrations on AWS RDS
* **OAuth2 authentication system**¬†\- Integrated Clerk provider with async httpx client for secure cross-platform identity management
* **¬£0 to $6.4K monthly revenue in 6 months**¬†\- Architected and monetized the entire platform from scratch
* **34% churn reduction**¬†\- Used behavioral cohort analysis and DynamoDB event tracking to drive data-driven product decisions
* **Stripe payment integration**¬†\- Built complete billing infrastructure with webhook handlers triggering Lambda functions via API Gateway and SQS queues
* **73% deployment time reduction**¬†\- Built automated IaC CI/CD pipelines using AWS CDK, Terraform, and Nx distributed caching across multi-stage environments
* **Production-grade Nx Python monorepo**¬†\- Evolved codebase with clean separation of concerns, dependency injection, and modular boundaries
* **Comprehensive testing suite**¬†\- Unit, integration, and E2E tests with IaC deployment enabling continuous delivery across dev/staging/prod
* **Scaled team from 1 to 5 developers**¬†\- Established technical hiring process and onboarded developers while maintaining code quality
* **Developer experience infrastructure**¬†\- Built Docker containerization and local testing suites enabling team to ship production features
* **GenAI video/image editing automation**¬†\- Implemented AI-powered content pipeline serving production workloads

Over 2 years I have started a bootstrapped company just adding each day, these are the main things; which should I include on my result?",0,13.0
939,post,cscareerquestions,1oecbdh,1761246877.0,"Previous boss has a new startup idea. Advice?
Some back story:

A few years ago I started working for a company that was pretty small. The people were nice, I was well taken care of, owners and management were generous. We landed big clients and the company was eventually bought out for a good sum of money (at least 2 million). The owners generously gave us a big bonus for the buyout and even negotiated that we keep our jobs for at least 2 years with the new company. After the period we were eventually let go and found new jobs elsewhere.

Yesterday I got a message from my old boss asking if id be interested in working on something on the side. I said id be interested in thinking about it, but my life is busy right now. They said theyd work around my schedule. I'm interested in hearing them out, but im wondering what i should ask for compensation. Development could possibly be split by another dev. And i would only be providing dev work. 

 I'm not hurting for money, but im certainly not going to turn it down lol. Im sure they will probably offer me a wage or lump sum when we hit MVP, but Im more so wondering if i should ask for a share of profits. And if so how much? I should probably hear about the idea first before i decide, but they're smart people, so I have a feeling this idea could be profitable as well. ",4,3.0
940,post,cscareerquestions,1oec1sd,1761246267.0,"Spec work coding challenges?
I have recently being approached by several AI startups (remote). 

After the first call, three of them specifically gave me a coding challenge. 

The same thing happened to all three. 

1. The thing to build was closely aligned if not identical to the product built by the startup. 

2. The description of the challenge was suspiciously specific: 

Implement a **frontend prototype** of an **AI Copilot** that privately assists a **smartphone repair technician** during a live support chat. The Copilot helps the technician: Diagnose the issue (root causes / next steps), Draft polished responses for the customer... 

3. All of them ghosted me.

I normally wouldn't mind a generic coding challenge, or a challenge that works as a stepping stone for a follow up call. But I had recently worked with a founder on anoo project and he told me explicitly to design a coding challenge based on open tickets we had in the backlog. I was shocked this might be happening! 

What do I do? (besides reject all future coding assignments from startups) I feel these people have to be exposed. ",1,6.0
941,post,cscareerquestions,1oebwk1,1761245952.0,"What are the current job market prospects for fresh graduates in embedded systems and embedded software engineering ?
I‚Äôm a third-year undergraduate in Electronics and Telecommunications engineering from a Tier 1 college in India. I‚Äôm passionate about electronics & computer science, especially embedded systems, and I want to work on both hardware and software.

I‚Äôve researched the skillset required to become a good embedded systems software engineer and I am currently working on it. I searched for jobs on various job websites like LinkedIn, Indeed, Glassdoor, etc. , but most were for mid to senior-level positions, and there were few fresher and junior-level roles. The companies that offered junior and fresher roles weren‚Äôt good.

I‚Äôm motivated, but after researching these jobs, I‚Äôm getting anxious. Can you please advise me on what I should do and what the current scenario of embedded systems is?",1,0.0
942,post,cscareerquestions,1oebmkk,1761245325.0,"Shifting from web development to AI Agent/Workflow Engineering viable career?
I was on the path to becoming a full-stack web developer but have become fascinated with building AI agents and workflows (integrating LLMs with tools/data). I'm considering dropping web dev to go all in on this for the next 8 months. Espeically ever since i found the web dev market to be incredibly saturated, competetive, and is the most career that is in risk from AI ( Correct me if I'm wrong).

Is this a viable path for a newcomer, or am I chasing a hype train that will lead to a dead end?

Is this a real job category in the future ?

",0,7.0
943,post,cscareerquestions,1oe8c2q,1761237862.0,"What would you have done if you were in my situation?
There is a famous semiconductor company with an office in Austin, Texas. Their CPUs design are what is used in virtually all the mobile phones. They own the second most famous ISA in the world after x86 I cannot go into more details without Doxxing myself.

I was recruited into a position several years ago.  They blatantly lied in the job description.

1. The job was a software engineering job. But they manage their own server lab. They do a lot of bench marking work, so they require their engineers to manage the server lab.

For the first 6 months on the job required me to install 2 2U servers with several PCIe peripherals each month. Me and another girl were tasked with this. It was literally hard labor. There was no server lift and these things were heavy as hell. I broke my arm several years ago. I don't lift weights. I have tinnitus. The server room sounds like a Jet Engine taking off, even when I wear the ear protection they gave me. So every time after I did lab work I used to come home to an Aching arm, body aches and my ear ringing like crazy.

This was not mentioned anywhere in the job contract, the offer letter, job description, the H1b visa filling documentation.

2. They told me that I will be doing low level systems programming work in C and ASM, when I joined  the job they were making me work on Solutions engineering project. ""For this use, build a solution using these open source libraries using Ansible/Bash scripts"".

Around the 6 months mark, I was fed up. I told my manager ""this is not the work I was told I will be doing during the interview. I had other job offers too. You either move me into a project where I get to do software engineering work or help me move a team. Talk to the HR and get me an exception to the 12 month rule."" (They had a rule that you cannot switch teams in the first 12 months on the job.)

And the immediate next day I got an email saying that my performance is not up to the mark. I am not meeting the expectations for my role. He gave me two projects in the first 6 months and I delivered them both. When he was about to give me a third project that is when we had this conversation.

3 months later he put me on a PIP. 1.5 months into the PIP I got another job offer and I left.

What would have done in my situation. I strongly thought of complaining to USCIS given the fact that I was on an H1b. But I was worried that they would cancel my Visa.

I thought of approaching the HR too. But I felt they would take my manager's side.

The whole experience was such a horrible experience. Like it left deep emotional scars. My manager said some pretty hurtful things in our 1:1. Sometimes I remember this stuff and wake up in the middle of the night.

Then after I left the HR started emailing me saying that I need to return the signing bonus because I left before 12 months is up. I replied something to the effect of ""You blatantly lied in the job description and caused me a lot of anguish. I am currently talking to a few lawyers and I intend to pursue legal action against the company. I don't intend to return the signing bonus"". And after that the HR stopped emailing me.",1,8.0
944,post,cscareerquestions,1oe83gz,1761237310.0,"Should I leave my niche and go back into development?
I need help deciding between my current job and a new one. For context on my professional background, I have a CS/Math dual degree from a state school. I have 3 YOE as a DE at a small ad agency.



Current job: 1 YOE in an advertising technology role in house on a marketing team. Medium company (2billion revenue 2024) that has insane growth and unlimited budget (I'm not kidding). It's more of a technical PM + consulting role than anything as I learn technical concepts and gather requirements from stakeholders, then triage to dev teams to help implement. 1 day a week in office with little to no chance of being able to work fully remote.

  
Pros:

Opportunity to have a niche, especially when the tech industry is saturated. Big, stable company. Knowledgeable stakeholders and lots of positive relationships with everyone in the org. Large company and opportunity to jump internally. Stock options, although I don't see us selling any time soon. Ethical company. Growing domain knowledge and lots of trust in me as an owner/developing expertise. Boss is open to me switching roles within the org if it aligns with my long term goals though.

  
Cons:

Although it's a niche, that means there's overall less jobs than a generic dev job. Plus, it would be hard for me to get out of the niche, especially cause i pigeonholed myself so early career. Some ethical consideration being in advertising. Little to no hands on keyboard unless I'm bug troubleshooting in SQL or making an occasional database view. One of a hundred or so technical people at the company, so when I see an issue, I likely have to hand it to another team that actually has expertise/access. Boss and skip are misaligned on overall goals for my role, and my boss prioritizes CRM efforts and not my niche. Feels isolating at times with no direction. I have to come up with direction myself. Lots of redtape to get ANYTHING done. Tools can take months or years to spin up.

  
New Job offer: Integration enterprise engineer job at a smaller company with a well known brand. Less revenue and impacted by tariffs, but dev team has historically been shielded from layoffs. Entering an IT team of 5 people. Pay same as current job, hybrid 3x per week, but get to commute with my sister who works for a sister company.

  
Pros: Opportunity to get hands on experience in a small team and actually get my hands dirty. Feels like I stumbled into my niche and abandoned my technical skills which I thrive one. Less strategy based, more execution based. Opportunity to build things from the full stack. Family friend worked here for 10 years in this same role and loved it. Younger demographic working here, free ski pass, close to family and friends, beautiful area. Really liked the team and they really liked me.

  
Cons: Switching would mean that I give up my niche, although I could use this as experience to get more technical dev experience and stay in advertising as a dev. I'd only have 1 YOE at my current job which can be seen as a red flag to employers. Getting out of the ad niche means that I could be more prone to getting automated out of my job or outsourced as I'm no longer a niche domain expert.

There's more to be said overall, like I already accepted job 2 but I'm thinking of rescinding it due to second thoughts. This would essentially tarnish my reputation with job 2. Anything is helpful as I make this decision.",3,1.0
945,post,cscareerquestions,1oe5kcl,1761231571.0,"Most of the top tech companies are AI-focused, but is it just a bubble?
Here is the ranking 
https://www.trueup.io/hot/companies

I want to specialize in machine learning (masters and PhD), because I love maths and I love organizing data and visualizing it.

But I'm a little afraid that the AI market is exaggerated and at some point these companies will just become less than average in terms of growth.

I mean, every week I hear there are 5 new ""models"" and everytime they're either a GPT wrapper or just worse than o3.

It feels like these companies will fall apart someday and the AI job market will become less than mediocre in terms of pay.

What do you think?",3,22.0
946,post,cscareerquestions,1oe2wsr,1761225161.0,"Do you think I would land a job even without a degree but with at least 4 years of self-taught experience?
I‚Äôm 21 years old and have been doing freelance work since 2021. I had to drop out of college after a year due to financial reasons, but I continued learning on my own and built a career as a self-taught developer.

I specialize in both front-end and back-end development (Java, PHP, JS and some Libraries/Frameworks) for websites and mobile apps. Most of my clients have been graduating students for whom I created their capstone thesis projects ‚Äî I‚Äôve completed more than 15 of these so far. During my one-year experience in an agency, I also developed websites for around five small to medium-sized businesses, especially newly established ones, helping them promote their products and services through e-commerce sites or online company pages.

All of these experiences are included in my resume under the position ""Self-taught Web & Mobile App Full Stack Developer (for freelance)"" & ""Full Stack Web Developer (for the agency)"".

Recently, my father recommended me to his boss and mentioned that I‚Äôm a programmer. His boss then asked for my resume and web portfolio, which I‚Äôve just completed. I‚Äôm wondering if there‚Äôs a good chance that his boss or their company would consider me, given my background.

Also, I‚Äôm curious about my chances with other companies, assuming their qualifications don‚Äôt strictly require a college degree but a different skill which I'm willing to learn. From what I‚Äôve heard, some companies here in the Philippines are more focused on skills and experience rather than having a degree (which is also my father's boss talked about).

Thank you and I appreciate your time! Here's my resume: [https://imgur.com/a/g2tRTQ8](https://imgur.com/a/g2tRTQ8) | [https://imgur.com/a/UlYNAKs](https://imgur.com/a/UlYNAKs)",0,6.0
947,post,cscareerquestions,1oe2v06,1761225031.0,"How to avoid getting pigeonholed
I started my first full time job about 4 months ago, and the job description was that of an entry level full stack developer. This was further confirmed at every level of the interview process.

I‚Äôm not sure how this came about, but since I‚Äôve started I‚Äôve slowly gotten pigeonholed into being just a front end dev. Seniors have assigned backend tasks to all the other devs in my cohort except for me. All the teams under my manager are getting a reorg rn, and the email detailing this shift listed my role as front end. 

Not sure what to do, because the few times people have asked me if I‚Äôm comfortable with server side development, I‚Äôve said yes. And it‚Äôs very interesting I‚Äôve only ever gotten frontend tasks because the only relevant experiences on my resume before this job were designing APIs with Spring Boot and Node. 

Are the seniors assuming im not capable? Do I need to speak up about it? Not sure how to proceed exactly.",2,16.0
948,post,cscareerquestions,1oe2f5x,1761223850.0,"Job hunting getting kind of hopeless
Hi everyone, 

I was recently working as a Graduate Software engineer for about 6 months but then left to take a hiatus and I also really want to get into bigger companies because I was working at a start-up.

In the beginning companies were reaching out to me and I was easily getting interviews... even at Amazon. 

But my only issue is, for example with the Amazon interview I did well but made some syntactical errors for looping through a map and used hashmap instead of linkedhashmap (and in my question order mattered) so I didn't get the job. 

At another big company, I did 2 interviews, they said will be advanced to next interview and now haven't heard from them for 3 weeks. 

But now, I'm not really hearing from any companies so I'm trying to put myself out there more. 

And I really don't want to be working at any company, I really want to be working at a big tech company with a high paying salary where I can thrive... but I feel like because I don't do so well in interviews sometimes... I'm losing my chances. 

I think for behavioural I seem to be doing okay... I am trying to be more confident and talk more. 

Any other interview tips... or ways that I can do well and network with big companies and get my foot in the door?

I would really appreciate the advice. 

This has also been heavily impacting my self-esteem (also facing rejection after rejection - which I get is totally normal but still ocassionally hurts) and belief in my technical abilities, so I could really use some advice on that as well. ",1,7.0
949,post,cscareerquestions,1odrh37,1761185808.0,"Are 45 hour work weeks the new normal now?
I keep seeing job postings that say they expect people to work 8am to 5pm. By my count that's 9 hours a day. What happened to 9 to 5, 8 hour days?

Edit: Seems like this is an American thing, and I didn't realize because I'm in Canada. Sorry",356,243.0
950,post,cscareerquestions,1odqt2l,1761183834.0,"If it were legal, would you make job candidates take IQ tests? Why or why not?
I've recently been seeing a lot of leetcode interview questions that I realized were ACTUALLY IQ tests disguised as algorithm problems. Things like ""Whats the next number in the sequence? Write an algorithm for the sequence."" Or ""rotate this 2D matrix and write an algorithm for it (spatial reasoning)"".

As far as I can tell many hiring managers are intentionally testing for IQ. 

I understand the rationale: programming ability scales well with fluid intelligence. But a lot of things do: being smarter even makes creating powerpoints faster and higher quality. It doesnt mean we should bar less intelligent people from making powerpoints

In the US, administering an IQ test in a job interview is illegal. But if it wasnt, would you make candidates take them? Why or why not?",0,28.0
951,post,cscareerquestions,1odqpef,1761183540.0,"I'm going to have a technical with for a junior netsuite consultant job. What should I study and know in advance?
I'll¬†have¬†a¬†technical¬†interview¬†for¬†a¬†junior¬†netsuite¬†consultant¬†job¬†soon.

A¬†senior¬†consultant¬†is¬†going¬†to¬†be¬†quizzing¬†my¬†programming¬†skills¬†to¬†gauge¬†my¬†ability¬†to¬†do¬†scripting¬†and¬†programming.¬†

I¬†know¬†they¬†use¬†suitescript,¬†so¬†I'm¬†guessing¬†they¬†*might*¬†test¬†Javascript¬†knowledge?¬†Any¬†thoughts?",2,1.0
952,post,cscareerquestions,1odqo09,1761183426.0,"LinkedIn premium shows every job has ~80% of applicants with a masters degree
How accurate is this and how many of these people are actually based in the US/don‚Äôt need sponsorship and went to accredited colleges?

The jobs i‚Äôm looking at are 0-2 YOE software eng jobs in the Bay Area. 

I can click on 10 jobs in a row and every single one of them will have a variation of the following stats:

~200 people applied
~80% entry level
~10% senior level

~15% have a Bachelors degree
~80% have a Masters degree",224,65.0
953,post,cscareerquestions,1odq2nw,1761181700.0,"Struggling to find reliable mock practice partners? I built something to fix that.
When I was going through my own job search, there were days I couldn't get myself to practice or apply anywhere, and others when I was completely focused. I realized how much it helps to have someone to practice with‚Äîsomeone who keeps you motivated and consistent.



So, I'm building PeerLink, a simple, peer-to-peer platform that helps job seekers connect with reliable practice partners based on their role, experience, time zone, and prep goals.



One of the key features is the wide range of interview topics available for web developers‚Äîincluding frontend, backend, full stack, performance, and web architecture.",0,1.0
954,post,cscareerquestions,1odpcra,1761179659.0,"Haven't landed a job since graduation in dec 2023, Am I not fit for a tech job ?
I don't see myself doing anything else other than this honestly. I've always loved tech. I graduated in Dec 2023 and haven't been able to land a job since then. Currently stuck working a dead end job. I'm tired of applying to every job out there only for them ghost me or send me a rejection email if they're being nice. I need to know if my current resume is good. I'm honestly sick of trying. My self esteem as at an all time low. Please help me.

  
resume: [https://imgur.com/a/ojYd49f](https://imgur.com/a/ojYd49f)",125,78.0
955,post,cscareerquestions,1odp6zm,1761179193.0,"How does your life work in a 9-9-6 job?
I just got an offer from a startup that says they do in-person 9-9-6 hours. 

But I'm confused. When do you eat, exercise or do errands?",215,164.0
956,post,cscareerquestions,1odoy42,1761178500.0,"Should I switch jobs for more enjoyable work or stay at current company with good culture and benefits?
Our company is going through a major ERP migration project, and I am not sure if I like the direction things are going. They just signed on a consulting company to perform the migration. We already have a relationship with this consulting company, and me and others have not been impressed with their output up to this point. We were shocked they signed them on to finish the migration project. There is a lot of dysfunction on this project already.

My job is to be an admin in the tool they use for migration, and I occasionally get to work on reports with some light SQL work. But my main role will be the admin in the tool, so I will be working very closely with the consultants on this dysfunctional project that is speed running to failure.

I have the opportunity to quit after 11 months to go work at a premium consulting company, not the one they signed on. But I don‚Äôt know if it is a good idea.

At my current job, I have a lot of flexibility. It is hybrid but I can work from home occasionally as needed. I only work from 9:00am-4:30pm. I can come in earlier or stay later as needed. I can move to another role in the company in January if one is available and I interview well. They also offer tuition reimbursement, and have good healthcare. I like my coworkers a lot, and the company culture is good.

The other job will be fully remote, but with more strict working hours. 8-5:30 during slow periods. Longer near project milestones. They don‚Äôt have great healthcare and they don‚Äôt offer tuition reimbursement. But they will pay me more which offsets the money I would lose for worse healthcare. The main difference is in this consulting role, I will get to work on enterprise reporting instead of just being admin in the tool. The work is significantly more enjoyable to me, but I would lose some of the flexibility and tuition reimbursement, and good healthcare. Also, the culture at the consulting company is really different from project to project. You‚Äôre playing the project lottery. Some projects have a great culture, others suck.

What do you guys think?",1,1.0
957,post,cscareerquestions,1odo2mz,1761176097.0,"How do you renegotiate salary if you low balled yourself on the job app?
I filled out one of those apps that forces you to give a salary and feel like a low balled myself a bit. I was thinking about telling them that I didn't understand the current market conditions when I filled out the app and don't think I would be willing to accept less than $xxxx. What are the odds that works? Is it too risky if I still want the job at the lower pay?",0,19.0
958,post,cscareerquestions,1odne1q,1761174289.0,"Feeling lost, advice needed
Hey‚Äôll,

I really need some honest advice and any suggestions on my situation.

I graduated in May 2024 (MS CS) and have been struggling since to find a full-time role. I have over 3 years of experience and I‚Äôve applied to over 2000 jobs across IT. I did manage to get a part-time Data Engineer position but that work is kinda ending soon due to budget issues and I don‚Äôt have anything lined up yet.

I‚Äôve been getting a few interviews here and there even 5-6 for single role but nothing has worked out so far. I feel completely drained and I‚Äôm constantly worrying about losing my status and the student loan which I can‚Äôt afford to clear if I leave to my home country though I have been getting offers there. 

I‚Äôm at a point where I don‚Äôt know what to do next and I am so exhausted atp just survive here until I can land something just even to clear my loan. 

If you could provide me any suggestions or leads, I‚Äôd be very grateful.

I just needed to let this out :(((",6,9.0
959,post,cscareerquestions,1odn3a2,1761173501.0,"Did your company culture changed eversince the job market became bad?
I used to love my job. It changed alot after consultant/private equity guys coming in, a good amount of attrition from other departments, I got met with higher expectations, I work longer hours now, I don't feel physiologically safe (which drains me alot) as mistakes can be punished and be used angainst you in performance reviews. My mistakes are weighed more than my accomplishments (eg a 'mistake' weighted would be  for merging a branch without the best optimal solution or sometimes missing a small detail  despite my co workers approving the PR)  . I love my co-workers, I dont slack. I get along with them and pair program with them  often.  I eventually got a PIP and desptie going beyond expectations. I dont think Ill make it as it got extended.  I survived many layoffs here, but I guess this is how I go.

I think the positive of PIP is that it pushes you to be aware of your flaws and focus on perfecitonism, but at the same time its  burning me out lol and perfectionism is not sustainable as we are all humans. We all mistakes. Maybe its stockholm syndrome at this point.",29,30.0
960,post,cscareerquestions,1odmj8n,1761172041.0,"Temporary oversaturated market or paradigm shift in CS/SE?
I know 3 recent CS graduates that are unable to find any job in our region for months now

I fear this is not just a temporary economic phase but a paradigm shift where CS will become an oversaturated field thus bad as an employee

IMO but please disagree: CS is a field with an oversupply of graduates and the days of ""easy"" software/tech developments is over

And some point most major software markets are saturated. This is something i am the most unsure of but... I feel like e.g. vending machine software is a done deal? Also payment processing? Or video sharing?

Additionally from a european/american perspective a lot of SE is outsourced to cheaper wage countries

And lastly AI does a lot of coding ""legwork"" just fine and it likely wont get worse at it 

How will there be more jobs/growing market in CS at any point?",60,115.0
961,post,cscareerquestions,1odgr3n,1761158597.0,"How long does one take to learn Power BI?
I'm totally new to this. My degree is related to cartography so it's not even close to CS stuff. Getting a job soon after graduating, I've been tasked with combining/recreating the behavior of separate data models (pbix, linked to PostgreSQL) into a single data model. As all the old visuals need to be recreated, my new combined data model relies a lot on DAX code for measures. It feels like I'm constantly making patches here and there and finally one day aha! This page works! Then I slowly move on to the next page. I feel like I can't perform and that I'm not learning DAX (and Power Query's M) fast enough. I've recently been stuck on recreating a matrix on a particular page and it's just never working.

I'm wondering if such a task is expected for new grads? The manager knows i have no knowledge of languages. He says to use AI and self learn everything 

What's the best way to learn DAX and M? I feel like my problems are really specific to my particular pbix file so idek how to ask online. 

Should I be asking how to learn DAX and M? Or is there a better way I should be thinking about my problem? 

My lack of ability and ppl's difficulty finding jobs are making me real anxious. I honestly think I'll be let go soon, but I thought I should still try till the end",3,6.0
962,post,cscareerquestions,1odgpw0,1761158520.0,"Is going back for a CS degree worth it?
Hi everyone!

In a bit of a tricky spot right now, I recently just finished my degree in Philosophy planning to go into Law (mostly to please my dad) but after thinking about what I really want out of my career and what I‚Äôm passionate about, I‚Äôd love to work with computers and software (After telling this to my mom, she said she‚Äôd always thought I‚Äôd end up working with computers being the tech guy of the house).

I‚Äôve spoken to a lot of people in my own circle about this a few who are much older and in coding/tech, and I‚Äôve been a bit of a mix of opinions, ranging from ‚ÄúNot worth it just learn yourself and get experience‚Äù to ‚ÄúAI is taking over so there‚Äôs no point‚Äù to ‚ÄúA CS degree is never a bad investment‚Äù

I have the opportunity to go back for a 4 years degree at UBC (my Alma matter), and am trying to decide if I should do it. Figured I‚Äôd ask the good people of Reddit for some thoughts and opinions before making a decision.

Thanks in advance!",0,29.0
963,post,cscareerquestions,1odgohr,1761158429.0,"New Grad. Made a BIG Mistake at my First Job! Should I Start Thinking about Leaving?
I graduated about 4 months ago and started immediately at a company I interned for. Was doing well at first but I made a pretty big mistake last week. I pushed a bad PR and commits that caused some issues to an important branch. Nothing in prod was affected but a couple engineers had to spend a day or two fixing my mistake and it did end up being a high priority issue that blocked some people. Mostly everyone was nice except a devops engineer who found the issue and was thorough about letting everyone know in every chat that I was the cause of the block. So its pretty well known to everyone that I messed up big-time. I merged a PR to the wrong branch without getting a review because I thought it wasnt required for this branch. 

I wouldnt usually be worried but we did have layoffs recently and I know an Eng2 who did get laid off during that cycle due to ""performance issues."" So this has me thinking im on the top of the list for the next lay offs. Maybe its best to get ahead of this now and start interviewing at other companies sooner than later? Its my fault so im thinking i should try to leave ASAP and start fresh somewhere new?

Note: New Grad Eng1 that started 4 months ago",246,140.0
964,post,cscareerquestions,1odftvy,1761156534.0,"Landed my dream job, but was down-levelled. How should I approach this?
First of all, I just wanted to say how fortunate I am to have this opportunity. Was ranting about my career prospects in a previous post a few months back, and finally managed to land an offer with one of the companies I was really vying for.

I was applying as a Senior Software Engineer (I know, titles are inflated, but it is what it is), felt like I received really good feedback from all 4 rounds of interviews (and got scheduled the next rounds really quickly, within a day).

However when the offer came, the recruiter mentioned that based on the internal discussions, they've decided to place me in a lower role. This came with roughly a \~20% drop in pay expectations.

On one hand, this will be a sizable pay bump (\~20%), better growth opportunity, and I would've been equally ecstatic if I actually applied for a Junior role in this company and got through. However this smells like a red flag as the interview loops felt conducted in a ""Senior"" role, and it felt like I got the shorter end of the stick due to having a lesser-name company on my resume.

I would've been more confident in negotiating if I had competing offers; a mid-level SWE role that I was in the final rounds for was offering close to \~40% bump but unfortunately failed. Coupled this with the fact that the company is not known to be flexible with their pay structure, **how should I handle the call with the recruiter tomorrow as they scheduled one to clarify the offer?**",0,13.0
965,post,cscareerquestions,1oddinl,1761151523.0,"Zoox or Intuit?
I‚Äôve got offers with Zoox and Intuit. Both full stack web dev roles. Both seem like pretty boring work tbh (building QA tools at Zoox and working on Quickbooks at Intuit). Comp is slightly better at Zoox ($240k TC vs $250k TC). Location is Bay Area for both.

I‚Äôm kinda drawn to the stability of Intuit but I‚Äôm not in love with the company. I think the mission at Zoox is super cool but the fact that they‚Äôve generated literally $0 in revenue is a little concerning. The work seems a little boring at both but I like that Quickbooks is consumer facing. I already work on internal tools at my current company and I don‚Äôt love it. I‚Äôm looking to learn and grow more as an engineer, but a little worried about getting worked like a dog at Zoox lol. I‚Äôve also only ever worked at the one huge company I currently work at, so an environment like Intuit is probably what I‚Äôm more used to.

Thoughts? Have you worked at either company? What would you do if you were me? Thanks in advance y‚Äôall!",5,4.0
966,post,cscareerquestions,1odc607,1761148546.0,"Is it a good idea moving from BI to other roles like DS or MLE?
Hi everyone!

I would love to hear some opinions and personal stories on changing from BI to more MLE or similar roles. 

About me: I've been working in data for 9 years. I'm a bit of a multifunctional type, having worked with ETL, dashboards, SWE best practices. I've led a team of 5 in my first job, and in my second I'm considered a Data Engineer because of the work in building our custom ETL library. 

However I don't feel challenged in the work. Sure there are problems to solve, but they aren't that hard! My background is mathematics so I'm thinking going back to the roots, moving to Data Science or Machine Learning Engineer. My goal is to avoid BI related work and build stuff that relies on data! 

I'm good with APIs and comfortable with a bunch of SWE stuff (git, docker, ci/cd). And I can't stand another dashboard! Recently I've worked in RAG and loved the concept of serving the data aspect of the product, while engineering focuses on the traditional aspects (UI, security,...) 

Has anyone made a shift like this? What tips do you have to make it happen? ",1,0.0
967,post,cscareerquestions,1odbdsn,1761146832.0,"I need guidance
I am a Canadian with two years of experience and I am unemployed since December. Landing an interview it self is challenging for me. I do use AI to optimize my resume but no luck. Please I need someone to guide me. I did work on some projects and working on one atm. Did some Aws certifications too. ",0,4.0
968,post,cscareerquestions,1od9vsc,1761143472.0,"REPOST - First-semester CS student at City Tech - debating switching to Computer Systems Technology or Cybersecurity because of the job market. Need advice.
Hey everyone,

I‚Äôm currently in my first semester studying Computer Science at City Tech (CUNY), and honestly, I‚Äôve been feeling pretty lost lately about which direction to go in.

City Tech only offers an Associate‚Äôs in Computer Science, so my plan from the start was to transfer to a four-year program (ideally somewhere like Stony Brook) to finish a full bachelor‚Äôs in CS. But lately, I‚Äôve been thinking a lot about switching my major to Computer Systems Technology (CST) instead, and I can‚Äôt decide what‚Äôs smarter long-term.

The main reason I‚Äôm even considering the switch is the job market. It feels like straight computer science is becoming extremely saturated, and I keep hearing that CST (since it mixes IT, networking, systems administration, and some programming) might open up more immediate and stable job opportunities ‚Äî even at the associate level. At the same time, I don‚Äôt want to make a short-sighted decision that limits me later if I still want to go into software engineering or something more technical.

Here‚Äôs what‚Äôs making me confused: ‚Ä¢	City Tech‚Äôs CS program ends at the associate level, so I‚Äôd have to transfer if I want to finish a bachelor‚Äôs. ‚Ä¢	The CST program offers a bachelor‚Äôs, so staying would be easier logistically ‚Äî no transfer stress. ‚Ä¢	But I‚Äôve heard the CST curriculum is more applied (hardware, networks, databases) and less theoretical (algorithms, discrete math, etc.), and I don‚Äôt know if that will hurt me later on if I want to go deeper into software development or data-related roles. ‚Ä¢	On the other hand, the job market seems to value practical skills and experience more than pure theory right now, and CST seems to give that earlier.

I‚Äôm just really unsure what the smarter move is. Should I stay in Computer Science, finish my associate‚Äôs, and transfer to a strong CS program like Stony Brook, or should I switch to CST at City Tech and focus on becoming more job-ready sooner?

If anyone‚Äôs been in a similar spot ‚Äî especially if you went to City Tech or a CUNY school ‚Äî I‚Äôd really appreciate your thoughts. How do employers actually view CST vs CS? Would transferring for CS open better long-term doors, or is the more hands-on CST route the better play given how competitive everything‚Äôs gotten?

Any perspective would help. I just don‚Äôt want to make the wrong move early on.

Thanks in advance.",0,1.0
969,post,cscareerquestions,1od7t19,1761138508.0,"Unemployed: Depression is starting to hit
background: bs, ms, and been doing ML for 2 yrs

Officially 3 weeks unemployed. My emergency fund is slowly going down. Ive applied to 85 jobs. Ive gotten 2 call backs. One I believe is ghosting me and another Im sure to fail (and its a pre seed startup which would be rough on my mental).

I see no light at the end of the tunnel. Im constantly on reddit. My head feels heavy. I just feel like crying. ",136,95.0
970,post,cscareerquestions,1od7qkn,1761138332.0,"Am I crazy to consider leaving stable job in this market?
SDE2 of 7 YoE. I've been reorg'd to my current team 1.5 years ago, and it's been a nightmare. I'm not interested in my team's product, state of engineering wants to pull your hair out, and my manager is borderline toxic. WLB is great and I love my people (outside of my manager), but I've felt incredibly stagnant in my career for awhile and feel miserable. I've been on a burnout for months that's been gradually increasing, and I know that things won't significantly improve anytime soon. 

I've originally planned to find a position within the company to transfer internally, and it's been 3 months since I started browsing around. Now, it feels like I might be better off to take a full plunge and prep for interviewing other companies for few reasons:

1. I've been having golden handcuff, but my salary is tanking hard in less than a year once my 4 year RSU runs out. At that point, I'm only losing a modest amount of salary to jump ship to other company's SDE2 position (according to levels.fyi). That's not even considering a slim chance that I make the hiring bar for senior in some companies. There is no path for promotion within my current company for awhile, anyways. I've saved enough to last for awhile.
2. Due to the company policy, it's practically impossible for me to transfer internally for another half a year without painting myself a target. Honestly unsure if my mental health will remain sane until then.
3. I've been on GC process for a bit (completed I-140 w/ EB3 using TN). Given the state of current administration, it's very unlikely that mine will be processed in a reasonable time. Might as well keep the priority date and resume as EB-2 at another company.
4. Tied to GC process above, I can only internally transfer to positions within my city. I'm on a branch office away from HQ, and the options are pretty small. I don't have much things to bind me to the city outside of GC process, and am honestly okay relocating.
5. I've been border locked for the entire year, and will continue to be so until GC is approved - immigration attorney strongly advises not to travel internationally. Not only does changing company mostly address that risk (since I'll have to restart with PERM), it gives me an option to get a sizeable amount of vacation in-between jobs. I've been dying to travel abroad again, albeit this is not a big reason to sabotage anything on my job.

  
I'm leaving the team in the earliest opportunity for sure. I just need to choose between finding an internal position within my current city and company, or fully commit to searching outside. I've heard many anecdotes of how terrible the job market is now, how insane the hiring bars are. The uncertainty with recession also adds a risk of layoffs, which tends to target less contributing employees including new hires.

Am I crazy to consider jumping ship in this market?",12,22.0
971,post,cscareerquestions,1od77d6,1761136961.0,"UK based - online courses to get into software engineering
Hello! My partner is really interested in getting into software engineering, however unfortunately did not obtain any a levels or attend university. His current job finishes at 3pm, so is looking at different courses to work towards for a couple hours a day. What online courses would you recommend that he could do, that would provide him with the experience to get a job, despite not having the educational background? We can afford a couple hundred a month if need be towards something, and he is open to anything that may take a couple years. Thank you",1,1.0
972,post,cscareerquestions,1od6hm9,1761135005.0,"Need Advice: Should I Abandon AI/ML for DevOps to Land My First Internship? (Bad at Math too!)
Hey everyone, I‚Äôm feeling really confused and would appreciate some outside perspectives on my career path.
My ultimate goal has always been an internship/career in AI/ML, and I started learning Data Science with Python.
However, a senior engineer recently gave me some really strong (and scary) advice, leading me to question everything.
The AI vs. Practicality Dilemma
Here‚Äôs the core advice I received, which argues against pursuing pure AI as a beginner:
1. AI/ML for Freshers is Too Hard: The most desirable AI roles are typically reserved for candidates with advanced degrees (Master's/PhD). The job market for freshers in core AI/ML is very limited.
2. The Pivot to Experience: To get my foot in the door and gain experience quickly, they suggested I pivot to a niche like DevOps right away. The idea is: get an internship, gain experience, and then transition back to AI/ML later on once I have a few years of professional work under my belt.
Why DevOps Seems Like the ""Safer"" Bet
This pivot to DevOps is especially appealing to me because:
‚Ä¢ I'm bad at math. The intense linear algebra and calculus required for deeper AI models is a major roadblock for me, which makes me think I'd be better suited for something like DevOps/Infrastructure.
‚Ä¢ The Market: The senior engineer said the ""Job and Internship market is better than Frontend and Backend jobs"" right now.
My Recommended Roadmap
They gave me a clear, actionable plan for DevOps:
1. Do AWS (I was told to focus on this first).
2. Then learn Docker.
3. Then Jenkins (for CI/CD).
4. Finally, learn Kubernetes.
5. <strong>Start applying for internships right away, and even message people on LinkedIn asking for internships.</strong>
So, my question for the community is:
Am I making the right move by putting my AI passion on hold and prioritizing a practical, in-demand niche like DevOps just because I'm a beginner and not great at math? Or should I just grit my teeth and keep trying to build an AI portfolio?
Any advice from people who have made a similar switch, or anyone working in DevOps/AI, would be super helpful!",0,4.0
973,post,cscareerquestions,1od5ceb,1761131542.0,"Is it wrong to approach talent acquisition staff via linkedin?
After finding out that ATS systems are using AI to get through resumes, I was wondering if it would be wrong to approach a company's talent acquisition staff directly for a role advertised?

I would only do it for roles that my resume meets each and every point for.

I've found that company's reject my resume via the ATS system, but I've then had calls from the company or a third party recruiter to discuss that exact same role some time after.",2,6.0
974,post,cscareerquestions,1od59p0,1761131299.0,"One more Senior Engineer who can't land a job ( Coding Assignments & Live Coding )
Nothing new I guess, this is starting to get a toll on me. Despair is setting in. Career choices, life choices, and all this affects the people around me.

I'm a decent engineer. I've built stuff, I've solved problems. I know my FE shit.

Since end of August I've participated in several processes, both startups and non startups. I am not in the US/UK ecosystem (S.Europe here) so I am trying to be relatively picky with my choices (I am getting less picky as we go of course).

I recently was rejected after delivering a coding assignment - following two very nice (good vibes) calls with the two founders of a startup.

Here is the weird part. I am pretty confident on my delivery. The assignment had a lot business detail, one had to think of what it actually needed - but thanks to AI - I delivered. Finishing touches mine, and I was prepared to answer any questions about the code. We even had a follow-up call planned to talk about the challenge.

24 hours later, I receive the most generic rejection message ever - nothing about the challenge , and a cancellation of the follow-up call. I've messaged the guy who I was in touch with - and he wrote something super abstract like ""we wanted to see how you would approach the problem"" and ""we didnt see the depth we were looking for"". (honestly I dont buy it)

I accidentally noticed that one of their engineers was stalking my Linkedin Profile a few hours before the rejection mail arrived. I was generally vocal about the ""AI Bubble"" and I am wondering if the fact that their business was AI-driven had something to do with it?

The other thing I am thinking is that the guy who visited my profile only did so after I spoke with the two founders so he decided for one or the other reason I am not a good fit - so they had nothing to say about the code by itself.

Needless to say this is a brutal market, and I have never seen so challenging interview processes, so lengthy filtering mechanisms. I happen to also be in a relatively small market so this might have to do with it. Remote gigs are harder to find these days.

What the heck should I do? I am not a top 10% coder but I'm good enough for most normal businesses. I don't grind Leetcode, and I do suffer from live coding brain freeze which I am trying to battle by doing a lot of live coding interviews. But it is \_very\_ easy for an interviewer to find reasons to reject you.

I have excellent soft skill presentation, most recruiters / HR folk are super happy with me, I present myself in an excellent manner.

The other day I was prepared to answer a specific live coding challenge following tips from the recruiter. I did it async before the call, almost memorized it.

During live coding it, I froze because the API wasnt returning the response I was thinking it would. It took me like 5' to solve the bug.

Rejected",31,13.0
975,post,cscareerquestions,1od53y2,1761130778.0,"I Got an Offer, but I'm Not Sure...
I'll preface this by saying this is year 15 for me as a software engineer. 6 months ago I left a government contract that was ending, and took another one. At first it was alright, but then the team lead started doing one on one's and an occasional random call. In one of these where I made a very tiny mistake, that nonetheless upset him, he said ""think of it as an unofficial warning""...

That immediately put my guard up, and I did what I do. I started looking for new roles. I'm not super-good at interviewing and considering the current climate I knew it would take a while, but yesterday I got one. It pays 20k more a year, I just don't know about the benefit situation.

Just about 10 years ago I had a period of difficult employment.  I left a federal contract I was on (that was also running it's course) to go to a start up. I left there after 6 months, because I was the only one doing any work, and their tech stack made doing that complicated.

Following that I went to another consultancy for a State Level government contract. That contract was pulled the week I started and I was on the bench. I didn't know the company or have a network there so I drifted from bad random job to bad random job for 9 months until I got another federal contract and got out.

I was on that Fed contract for a year, got picked up by a Fortune 500 company, and was there 4 years.

But now I'm afraid to leave this job for a job that could also be bad, and if that's the case I can't leave in another 6 months I'll definitely have to stick it out. I'm not sure if I should just turn it down and try and stick it out or what.

The new company wants a decision TODAY which makes this all the worse. I am waiting to see their benefits package, but my question.

Will this look bad if I take it? Right now I have my resume reading FEDERAL BRANCH I WORK FOR 2023-Present, with both contractors names in the heading so it kind of hides it, but I'm not sure if that is even the best idea.

EDIT - I took it.",1,10.0
976,post,cscareerquestions,1od4srk,1761129725.0,"The Psychological Trap of Staying Loyal to Your Job
https://youtu.be/KVGoRDtu_dUhttps://youtu.be/KVGoRDtu_dU",0,0.0
977,post,cscareerquestions,1od32a5,1761123336.0,"(1 YOE) This junior offer for a startup is too easy and looks sketchy
So I've been contacted for a Backend engineer role where I'd be using Python and AI for a shitty AI online gambling startup in which all parties look completely real (interviewer has a full linkedin and looked good, startup looks legit, based in Colombia but looking european team, thats weird though)

I don't think this startup is going forward for long, but that's not my problem since I have another job

The thing is: this is far too complacent: (1) They contacted me, asked for CV and accepted it instantly (for a jr AI position, in this market), (2) the interview next day had no kind of pressure besides me absolutely bombing it (idc about this job), everything is ""oh thats great, it's perfect for us"" and (3) they had no problem when I asked for an inflated salary mark (since idc) - that makes it a fully remote, +50% salary from current one.

So, is this going to work out? Can I get away trying to rob this guys or am I better hopping off this before they trap me with some shit? Could they be so naive ?",7,6.0
978,post,cscareerquestions,1od2e0m,1761120685.0,"I finished my IT degree but I still feel like a fraud. I can‚Äôt build anything without AI or Google.
Hey everyone,  
I really need to be honest about something that‚Äôs been bothering me.

I recently finished my studies as a¬†*state-certified Business Informatics Specialist (Software Development)*. During my time in school, I practiced programming a lot. We had structured exercises, projects, and final exams, and I did well in all of them. On paper, I should feel confident. But when it comes to building something completely on my own, I feel lost.

Every time I try to start a project, I end up asking AI for help or copying pieces of code from Google that I barely understand. I‚Äôve vibe-coded my way through several projects that look fine on the outside, but deep down I know I didn‚Äôt really build them myself. It feels like I‚Äôve just been stitching things together without truly understanding what‚Äôs happening. I feel like a fraud.

Back in school it was easier because everything was guided and structured. Now that I‚Äôm on my own, I get overwhelmed. Everyone on LinkedIn and GitHub seems so smart and confident, creating amazing projects from scratch, while I can‚Äôt even write proper classes or use inheritance without checking examples.

I‚Äôm motivated and I truly want to learn, but I keep procrastinating. I prepare everything, plan what to do, set up my environment, and then I stop. I tell myself I‚Äôll start tomorrow. I‚Äôve just graduated, I‚Äôm looking for a job, but honestly, I don‚Äôt know how I‚Äôd manage without AI or Google.

The good thing is that I‚Äôve started to change how I learn. I‚Äôve told ChatGPT not to give me direct code anymore, only to guide me and help me think through problems. I‚Äôm practicing on LeetCode, trying to solve problems on my own, and I also started following the Coding Interview University roadmap. Right now, I‚Äôm working on a new project using this approach where ChatGPT only acts as a mentor instead of a code generator. It‚Äôs frustrating sometimes, but I finally feel like I‚Äôm actually learning something.

Has anyone else felt like this after finishing school or a bootcamp? How did you transition from guided learning to being able to code independently? What helped you get through the feeling of being completely lost once the structure was gone?

Thanks for reading. I just needed to share this somewhere where people might understand.",42,17.0
979,post,cscareerquestions,1ocy8l5,1761105652.0,"Your experience in the job market is going to be unique
I've been lurking in this sub for the last 3 years and feeling pretty disheartened regarding where the job market is. I took a staff / principal / lead engineer role earlier this year that has been an unmitigated disaster. Things came to a head this August when I decided screw the shit market. I need to get out or I'm going to \_\_\_\_\_ my boss. 

Prepared for a 6-12 month job search, relocating for the role and down leveling. Spent most of August doing the Neetcode 150. Responded to every LinkedIn inbound message. Expected all the conversations to fall through after the first one or two conversations. Instead they all kept going and at one point I was interviewing with 5-6 companies in the same week. 

Got my first offer today, team lead, top of category startup, fully remote. Genuinely excited about the product and the culture. Sent follow ups to two other fully remote roles I finished full loops for last week. End up sending no outbound resumes and withdrawing from 5-7 conversations that required relocation or were too early in the process. 

Not trying to brag here, just posting this for someone else out there like me (absolutely miserable at a role thinking that market is too shitty to jump). ",143,27.0
980,post,cscareerquestions,1ocxzvb,1761104908.0,"Do people who think AI will kill software engineering just work on tiny code bases?
Serious question. 

SWE @ insurance company here. Massive code base with tons of complicated business logic and integrations.

We've struggled to get any net benefits out of using AI. It's basically a slightly faster google search. It can hardly help us with any kind of feature development or refactoring since the context is just way too big. The only use case we've found so far is it can help with unit tests, but even then it causes issues at least half of the time.

Everytime I see someone championing AI, it's almost always either people who do it on tiny personal projects, or small codebases that you find in fresh startups. Am I just wrong here or what?",904,358.0
981,post,cscareerquestions,1ocwqs3,1761101158.0,"Entry level database management positions I can use to later transition into junior DBA?
I graduated pretty recently with a bachelors in computer science. I had a database management class for two semesters and I became pretty interested. I know I want to work with databases. I figured that a junior database administrator was an entry level job but apparently even entry level junior database administrator still expects a few years of experience. What are some actually entry level positions that I can go into to eventually transition into a junior DBA once I have some experience?",4,2.0
982,post,cscareerquestions,1ocwdr0,1761100094.0,"Leaving tech and need advice
I got laid off six months ago from my tech job after many years in the industry as a software performance engineer. Now I‚Äôm thinking of leaving tech for various reasons. Job postings have unreasonable demands and employers make you go through hoops and hoops of leetcode style interviews only to get rejected at the end. I‚Äôm disillusioned and frustrated by all this and am under pressure to get some income soon.

I‚Äôm thinking of shifting to AI enablement (using AI tools to solve problems) or technical account manager or business analyst/operations analyst roles. Does anyone have advice on other alternative career paths that might be easier entry?

Also  I‚Äôd like to get a part time job for income while I‚Äôm preparing to pivot to one of these career paths. If I could bring in $1500-2000/ month I‚Äôd be well off. 
Looking at data entry or remote virtual assistant/tech support type jobs, but I don‚Äôt know how to dumb down my resume which now  reeks of overqualification. Should I go to a staffing agency for these type of jobs?

Any advice would be appreciated.",48,60.0
983,post,cscareerquestions,1oclmgm,1761073053.0,"Do you guys hate AI as much as Reddit does? Or do you quietly use it to automate the boring stuff?
No joy in making loops and skeleton code. Let me save my brainpower for the real problems. 
I don't think it's the same thing, but it vaguely reminds me of a book called Automating the Boring Stuff with Python.  ",0,51.0
984,post,cscareerquestions,1ocl20n,1761071807.0,"Is it too late for me to become a web developer at 25?
I‚Äôm currently working as a dishwasher at a restaurant, but I know I can‚Äôt do this for the rest of my life. I want to learn a skill that can help me get a more stable job.

I‚Äôm 25 years old, and I‚Äôve been thinking about becoming a web developer, starting with front-end development first.

Is it still possible for me to learn this and build a career, or is it too late? I‚Äôm also worried that AI might replace web developers in the future. Should I still go for it, or should I consider learning a different skill instead?

Thanks for any advice!",0,37.0
985,post,cscareerquestions,1ochbeq,1761063436.0,"What should I know about startups and their funding stages when negotiating an offer?
Hey I am looking into a startup amd they told me what thoer funding stage was im terms of a letter. Please help me understamd what it m3ams for the reality of the job. 

I am concered with:

Job security: how should I evaluate if this job will be around for a few years?

Benefits: what stages should i expect healthcare? Should I negotiate equity?

Work life balance: I'm willing to put in a lot of hours, but I want to know how i should structure compensation for various hours/week.

Thank you for your insight!",0,2.0
986,post,cscareerquestions,1ocgl01,1761061797.0,"Free YouTube roadmap for going from complete beginner to CS job candidate
[https://danielkliewer.com/blog/2025-10-21-learn-programming-computer-science-youtube-roadmap](https://danielkliewer.com/blog/2025-10-21-learn-programming-computer-science-youtube-roadmap)

Hey I saw this infographic that suggested a bunch of good youtube tutorials for learning programming so I created a blog post with some help to act as a roadmap for learning computer science.

I am already experienced, but I wrote it for the complete beginner, I am  going to use it to fill in my knowledge gaps as I know we all have them.

I hope all y'all find this helpful.",0,12.0
987,post,cscareerquestions,1ocga15,1761061093.0,"How to format interning at a company in two different semesters?
I‚Äôm currently interning for a company that I also interned for last fall semester. Should I list these as two separate listings on my resume, or just consolidate them into one and say for my employment date something like ‚ÄúAug 2024 - Dec 2024, Aug 2025 - Dec 2025.‚Äù I‚Äôm concerned about making the reverse chronology of the resume confusing, since I also had an internship in between these two jobs employment dates. ",1,2.0
988,post,cscareerquestions,1ocfdrb,1761059035.0,"Need Career Advice - 2.5 Years in RPA (UiPath, IBM WatsonX) and Looking for a Clear Roadmap Ahead
Hey everyone,

I‚Äôm a¬†Lead Software Developer¬†currently working at a startup in¬†Bangalore, with around¬†2.5 years of experience¬†in¬†RPA (Robotic Process Automation). Most of my work has been in¬†UiPath, and I‚Äôve handled multiple¬†client-side (on-site)¬†projects, mainly in the¬†Finance , IT , HR domain.

Here‚Äôs a quick overview of my background:

* Built automations for¬†financial domain, data entry, invoice processing, vendor onboarding, document extraction, SAP automations, Excel automation & Salesforce Automation.
* Developed¬†complex logic¬†(like permutations and combinations) within UiPath workflows.
* Worked on¬†web automations,¬†data fabric integration,¬†UiPath Orchestrator, and¬†Citrix/RDP automations¬†(including Azure AD web automation).
* Automated¬†Salesforce processes¬†(like presales and sales data assignment).
* Integrated¬†Python scripts¬†into UiPath for custom automation logic.
* Some¬†POC experience with IBM RPA¬†(a while back).
* Currently exploring¬†IBM WatsonX Orchestrate¬†to understand its automation and AI potential.
* Earned the¬†UiPath Certified Professional Automation Developer¬†credential.

Now, I‚Äôm at a stage where I really want to plan the¬†next phase of my career, and I‚Äôd love to get some genuine advice from people who‚Äôve been in similar situations.

For someone with this kind of background

1. What career paths usually open up next after 2‚Äì3 years in RPA?
2. What directions are worth exploring to stay relevant in automation and tech over the next few years?
3. Is it better to go deeper into RPA and become an expert, or start branching into areas like AI, software development, or data engineering?
4. And what¬†skills, tools, or certifications¬†would you recommend focusing on in the next 6‚Äì12 months to grow further?

Any insights, personal experiences, or resources would mean a lot. I just want to make sure I‚Äôm building a¬†long-term, future-proof career path¬†that aligns with where automation and AI are heading.",1,1.0
989,post,cscareerquestions,1ocfcfj,1761058953.0,"What are you all pursuing academically for data science?
What‚Äôs everyone here majoring in or planning to study? i am asking this question to know if most people are pursuing/planning engineering?

I am about to land my first job as a data analyst and plan to transition into data science in 2 years Is it an advantage to be an engineer while learning Python for data science? because of the maths that is involved?

I am pursuing MBA in business data analysis and HEAVILY regreting for not pursuing engineering because it could have equiped me with an aptitude towards mathematics that could help in my Data scince carrer and could have shaped the way i make predictions using machine learning and the regret for not pursuing engineering is disturbing me daily.

wanted to know what you all are pursuing out of curiosity.",2,2.0
990,post,cscareerquestions,1occvuf,1761053103.0,"6 years as a backend developer, feeling stuck and scared AI will make me irrelevant
i‚Äôve been working as a backend developer for 6 years now, mostly in fintech. it used to feel exciting doing things like solving problems, building systems that actually mattered. but lately, i‚Äôm starting to feel‚Ä¶ replaceable.  

AI tools are getting faster and better. they‚Äôre writing cleaner code, generating tests, even catching bugs before I do. It‚Äôs like the parts of my job that made me feel skilled are slowly disappearing. Every sprint feels flatter with more tickets, less creativity.

i‚Äôm not ready to leave tech, but I can‚Äôt shake this fear that I‚Äôm falling behind, really. I‚Äôve thought about moving into product or data, but I don‚Äôt even know where to start or what‚Äôs realistic anymore.

how do you keep growing when the ground keeps shifting beneath you? Has anyone here managed to pivot within tech without starting over completely before it‚Äôs too late?",95,46.0
991,post,cscareerquestions,1oc9wld,1761044520.0,"Career advice
**TL;DR:** How do you actually manage to change specialization in software development while working, or how do you land a job at all in a completely different specialization?

So basically, I turned my career towards video game development, but the shortage of opportunities and the usually poor conditions in this sector are driving me to shift into other specializations of programming, as I don‚Äôt enjoy making video games that much. I worked as a full-stack developer for 1.5 years, but that was 6 years ago and that experience is no longer relevant. Although I don‚Äôt remember the details of the languages and technologies (PHP, Laravel, Vue.js), I still remember the concepts and basics of REST APIs.

Still, I don‚Äôt know how I could compete for a job offer when I‚Äôve been working in a completely different area of programming for 6 years. I‚Äôm thinking of taking a course in .NET for backend development or something similar in my free time, but which one? Will it be enough?

I also don‚Äôt have a bachelor‚Äôs degree, but I have two HNDs and one unfinished bachelor‚Äôs degree.",3,4.0
992,post,cscareerquestions,1oc9kxb,1761043414.0,"Has anyone landed an i*nterview and job offer from using an AI apply system?
I think there are a bunch of services now that feature an AI autonomously creating & submitting job applications, or even a cluster of AI agents finding & applying to job postings on the internet. 

I find this super sus and had to ask if you continue manually applying in the future or know someone who really got a job with these system

Also, given the state of how the market, I think it‚Äôs way better if the recruiter reaches out to you before applying to initiate the interview. It skips the line but you have to be very lucky of course ",0,3.0
993,post,cscareerquestions,1oc7l20,1761035952.0,"Jira Projects in Companies
People that use Jira at work: how does your company use the Projects and Components features?

I'm asking because right now we have a single Jira Project for development - DEV, where all the tickets for each product live. We also have other Projects for requirements and for our QA team.

In the beginning when we had 1 product and 3 teams working on it (2 native teams + server), it made sense to share a single backlog with a single board. But now we have multiple products, with multiple teams, and we use Components for each product/team to allow us to filter properly, as well as private boards with custom filters (I'm now working on ticket 23199).

There's a debate in the company about how we should go forward (split up or keep everything in one), where the majority doesn't see the benefit if you just use filters. 

This is my first job, so I have no idea if this is the norm, or if better ways exist. But I certainly guess Projects were meant for... projects? ",2,1.0
994,post,cscareerquestions,1oc7eg2,1761035222.0,"I‚Äôm at a fork in my career - Technical PM vs Backend Dev. Which path would you bet on?
Hey everyone, my manager recently had a discussion where he stated that he wants me to move into the next phase of my career. It's something we've both talked about, but since we're a pretty small startup, official opportunities have been limited (especially since the tech sector's been hit hard).

Anyways, I'll break things down below. To note, I'm currently a CX 3 Specialist (promoted twice) with a couple of Python projects under my belt.

**Technical Product Management:**

* I‚Äôve been working closely with our Senior PM for over a year - we sync regularly and have a good rhythm
* Used my $2k learning stipend last year for a semester-long PM foundations course
* Haven‚Äôt owned a full product cycle yet, but I‚Äôve written PRDs and helped with spikes, brainstorming, backlog grooming, and customer research

**Backend Development:**

* No formal coding background (mostly ‚Äúvibe coding‚Äù lol), but I‚Äôve been learning Python and built a small importer tool using our API
* Helped customers onboard using a Python import tool our dev team made - ended up saving dev time and even got public recognition for it
* I have a personal project in Python that I'm working on right now, and if I hit pay dirt, it'll be a big step towards moving into Backend Development within the company (I expect to have a functioning prototype by EoY)

**A few other things:**

* I genuinely like both PM and Backend work, but I *really* enjoy the logic and problem-solving side of Backend
* I‚Äôm already comfortable digging into backend stuff (Kibana, Datomic, JSON APIs) when I need to troubleshoot or grab data
* The company has a bigger need for devs right now. We‚Äôre splitting into two products - the OG product (which I‚Äôd probably support) and a new AI agent product that‚Äôs eating up most of our dev resources
* Long-term, Backend feels like it has clearer career ladders (EM, Solutions Architect, DevOps, etc.), while I‚Äôm less sure what Technical PM grows into

  
**TLDR:**  
Mid level CX employee at startup trying to choose between Technical PM and Backend Dev. I like both, but Backend feels more interesting, in-demand, and versatile. PM fits my current skill set and relationships better, but feels fuzzier in terms of growth.

If you were in my shoes which way would you go, and why?",1,2.0
995,post,cscareerquestions,1oc6n0s,1761032150.0,"Is this normal for 2 juniors who are hired together?
So I'm a junior cloud engineer, working for around a year now in my first job straight out of uni. I was hired with another junior, but he has a masters and 2 prior years of work experience so I was hired for my ""potential"" whereas he was actually selected for his skillset too. I have no problem with that, I'm happy to learn and grow as fast as I can. 

My manager however, seemingly doesn't want me to forget how much better he is than me. Here are some things that have been said during our 1-on-1s, without me ever mentioning him (for the story's sake, we'll call him Tyler).

""You're doing well, you don't need to compare yourself with Tyler."" I never was.

""You are doing your tasks and learning a lot of things, *it's not super great but that's what we expect from you*. Of course we can't expect for you to be an expert. Tyler is different, he has had experience before""

""You are real junior here to be honest, if Tyler applied for a mid level role he would've gotten in, we just hired him as a way to get him in the company. So don't worry about him.""

""You are an early career experiment, we want to see how we can develop people from zero, but Tyler is not really a junior to be honest""

Amongst other things. I don't know if I'm just being sensitive to some very normal or mildly negative feedback, but I just don't understand how I'm supposed to respond to these. I feel like I'm having my inferiority drilled in to me again and again, even when me and Tyler are not working in even remotely similar things. I also find it not productive to have him as an arbitrary benchmark, and spend less time focusing on my performance and growth in isolation. My other coworkers are actually giving me plenty of props and good feedback and think I'm learning super fast, but I feel like I'm not perceived as good as I would've been by my manager if Tyler wasn't working alongside. If I was hired for my potential, then why don't we spend most of our attention maximizing it?

Another annoying thing is our objective setting. We've done this process twice now. The first time, I made mine quite compact and Tyler made his more elaborated. Our manager said ""we could make yours a bit more like Tyler's, see how he made his a little clearer?"". Yup, absolutely. That makes sense.

But the next cycle, he had his very short. Almost lazy. It was literally just a bullet point of the stacks he wants to learn and get to work with. Whereas I elaborated on mine more specifically. But guess what? ""We can make it similar to Tyler's one just so its easier.""

So what the hell. I get that he's older, more educated, more experienced and most importantly, he's a he. I don't want to link these treatments to me being the only girl in the team and the youngest member by a lot, but I can't help to think those things play a part.

Or, alternatively, I could be overthinking and these are perfectly normal parts of a manager's evaluations. In which case Im happy to learn to get used to it and move on with my life.

I have recently had a hiring manager reach out to me for a position in a different company. I've cleared a few interview rounds and they've said they're willing to offer me a 20% pay raise, with a sign on bonus and stock which I don't currently get at my company. I don't wanna leave my current place for some other reasons that compensate the lower pay, but if this treatment isn't normal I might just consider leaving. However, that also lets me know that I don't suck, so I'm really not sure of what to think anymore now.",67,17.0
996,post,cscareerquestions,1oc4xxq,1761025751.0,"How hard is to switch on your domain/specialization?
Please help a blind ignorant young fella out. For the background, I will be graduating summer 2026 and have an offer right now. The team that I will be joining and the role I will be working on is general backend like distributed system. I am more interested in like ML or search stuff(like SWE in ML/AI or search team, not applied or research scientist). My question is that after like 2-3 years of experience with this company, how hard will it be to switch to diff company in teams that I am more interested in(the company is very well known tech company)? If i join a certain team, does that mean that I am likely stuck with the one that I chose in the beginning of my career? I am aware that it is possible, but I was wondering if it is possible without internal transfer or lateral/downlevel move? Also, lets say after years of experience where I am aiming for managerial role, will I only be able to lead a team in the domain that I am expert/specialized in only or is it also more versatile and somewhat transferrable across different teams? I am having these questions because I have seen a lot of advice saying have your specialization or build expertise in something. (btw I am wodering about big tech/late stage startup scene so please answer in that scope)",2,0.0
997,post,cscareerquestions,1oc397r,1761019961.0,"Does a CS degree have any value job-wise whatsoever?
If the tech field (SWE, IT, QA, etc.) is oversaturated and no one is getting hired, are there any jobs you *can* get with a CS degree? Whenever I try to land a job with by degree that isn‚Äôt in tech, they just ask me why I‚Äôm not working in tech. And when I make up some bullshit about ‚ÄúI graduated with a CS degree but I realized my true passion in life was working in a call center haha‚Äù they don‚Äôt seem to believe me. So is there anywhere a CS degree can get you or is it just a piece of worthless trash? ",0,28.0
998,post,cscareerquestions,1obzbjo,1761008456.0,"How to get into hedge fund with non-perfect GPA?
Hi, I'm a 4 YOE software engineer who is currently considering switching into SWE roles in finance.  I got a recruiter reach out to me for Jane Street, and they asked me for my transcript so I sent them. And they rejected my profile. 

My guess is that I didn't have a high GPA on my transcript.

In general, is it still possible for me to get a job with hedge funds/trading companies? If it's possible,  how to do it? If not possible, should I get another degree and ensure I get a perfect GPA?


Thanks!",0,23.0
999,post,cscareerquestions,1oby9tv,1761005567.0,"Got a raise then they took it away
Started my first software engineering position earlier this year. Got a pay raise back in August. Cleared countless tickets/projects that were pushed to production since. Even found severe vulnerability in our site and fixed it. Small company only 2 on the engineering team‚Ä¶

Last project I was put on was difficult. Took me two weeks to complete and ended up changing cause the original ticket wasn‚Äôt even the issue (they had a deeper issue that needed fixed before the ticket could be fixed)‚Ä¶ anyways I was also sick the week of this project.


This week I found out I‚Äôm losing well over 50% of what my raise was. Literally salary cut in half effective immediately. 


Is this normal? Feel defeated. Heard the news right after I finished building this a cookie consent banner since they‚Äôre getting sued

First software engineering job post graduating.


 ",215,64.0
1000,post,cscareerquestions,1obvtle,1760999135.0,"USCIS updates H1B 100k fee (good news for intl students)
Last month the US announced a new 100k fee for H-1B applicants. After some initial confusion, USCIS clarified that it would only apply to new applicants, not existing H-1Bs.

Today, [USCIS released new guidance](https://www.forbes.com/sites/stuartanderson/2025/10/20/immigration-service-issues-guidance-on-who-pays-the-100000-h-1b-fee/) clarifying that the fee will also not apply to ""change of status"" applicants, such as F1 to H1B. 

Since almost all H-1Bs come in as bachelors or masters students on F1 -> OPT on F1 -> H-1B, for all practical purposes this almost reverses the 100k fee. It now only applies to people applying for jobs in the US from outside of the US.

International students return to their previous level of difficulty applying.",179,75.0
1001,post,cscareerquestions,1obvace,1760997816.0,"Starting new job at a tech company - advice?
I'm starting a new hybrid job next week at a mid-sized tech company in the Bay, and it'll be my first time working at a larger company. My previous experience (2‚Äì3 YOE SWE at a company of fewer than 10 people) has been fully remote, where I had broad ownership over most projects.

Any tips or advice on transitioning from a small, remote company to a larger, hybrid one? What should I expect? How is office life? I just want to best set myself up for success.",4,2.0
1002,post,cscareerquestions,1obv98o,1760997741.0,"Calling all ""lifers"". Why do you plan on sticking with your current company for the rest of your career?
Title. What makes you want to stay at your current company as opposed to job hopping and maximizing TC?",64,79.0
1003,post,cscareerquestions,1obsatk,1760990985.0,"How do you cope after a major fuck up?
No, it wasn‚Äôt me. I wish I get paid with Amazon RAU. But I have made mistakes with multi hours downtime at work in the past that are 100% my fault. Can‚Äôt even blame anyone or process. 

Genuinely curious on how do you cope? Or stay mentally sane? Logically I understand that a job is just a job, but mentally I don‚Äôt do so well after these kind of mistakes. If it‚Äôs a mega big one, it affects my physical health, I‚Äôd get stress hives or stomachaches.",32,19.0
1004,post,cscareerquestions,1obr8xn,1760988666.0,"[Update] My husband wants to switch from nurse anesthetist to software engineering.
https://www.reddit.com/r/cscareerquestions/s/MMxT0pVzJX

As I suspected he is bored of the mundanity of his job and need to focus so much all the time and wishes he stayed in engineering as it‚Äôs hard seeing his fellow coworkers who became super successful. I didn‚Äôt ask but I feel he regrets leaving nvidia long ago. The compromise is that he will take online courses at one of the big name online cs programs for a masters while still working as a CRNA. Then if he gets a job, he can leave CRNA and come back if he ever changes his mind. Originally he wanted to leave CRNA and focus on applying and studying full time. ",157,202.0
1005,post,cscareerquestions,1obr8h9,1760988639.0,"What are some software dev related side gigs that I can do to prove myself to recruiters?
Hey guys,

I've been on the job hunt for a year now, never could land an internship during college, so it's been a struggle and I've only been able to get a job as a packer in a warehouse even with over 200 applications. I'm just wondering if there are any development side hustles I could do that would stand out to recruiters.",9,7.0
1006,post,cscareerquestions,1obp93o,1760983941.0,"Switching Careers Too Much?
I work in the global HR tech space, and feel like I‚Äôm jumping around too much. 

I started at Employer A (6 years, a few positions) then went to Employer B for 16 months. Huge ethical issues at Employer B so I left, and went back to Employer A. 

And now I‚Äôve been back at Employer A for 3 months and it‚Äôs bloody awful and the sales comp sucks and I want to leave. It‚Äôs like whiplash every few months - is this changing too often? New job on horizon (Employer C) has solid comp and lots of growth - but explaining this to a recruiter seems frustrating.",0,1.0
1007,post,cscareerquestions,1oborb7,1760982652.0,"Am I crazy for considering leaving my current job to join the Navy?
For context, I‚Äôm a 22M recent grad (graduated May 2025) and am working at a F500 insurance company making ~80,000 as a software engineer. I interned at this company during my senior year, and pretty much joined full time right after graduation (I had maybe a week off).

The company is amazing. The work life balance is great, my coworkers and boss are great, and the pay isn‚Äôt bad (especially considering I still live with my parents in a low cost of living area). I‚Äôm nearby most of my friends and have a very healthy life outside of work with multiple hobbies.

Yet I can‚Äôt help but feel like something feels missing. My job is right next to my house where I grew up (10 min commute) and I went to school in state only 30 ish minutes away. I feel like I haven‚Äôt seen or done anything and am missing out. I know I‚Äôm in a situation some would envy, but I just feel‚Ä¶ bored?

I‚Äôve always been interested in the idea of joining the military, but have obviously heard horror stories about it too (hence why I never joined). But just today I was having a casual conversation with the lead engineer and he told me about his experiences in the Navy. All of the fun he had, all of the minor trouble he got into, the places he‚Äôs been, etc‚Ä¶ It honestly sounded like a fun adventure and he said he hasn‚Äôt regretted a second of it. And obviously it didn‚Äôt impact his career negatively as he‚Äôs the lead engineer in our team.

So I guess the TL:DR is, am I crazy for considering leaving my current job to join as an Officer in the Navy/Air Force? What tech skills will I learn and how will it impact me in the future? Obviously I had my lead engineer as a resource, but I want to get a broader set of opinions too that may not be biased by previous experiences. ",28,134.0
1008,post,cscareerquestions,1oboo67,1760982417.0,"What‚Äôs the best way to actually land a Java backend job in today‚Äôs market?
Hey everyone,

I‚Äôm trying to break into (or back into) a **Java backend developer** role and could use some real-world advice from people who‚Äôve done it recently.

**My background:**

* 1 year professional experience in C++
* 8 months professional experience in Java (Spring Boot, Kafka)
* I didn‚Äôt do a *ton* of coding in those roles ‚Äî a few features, pair programming here and there.
* Laid off in July, currently job hunting

**Goal:**

Land a **remote (or hybrid)** Java backend position in the U.S. as soon as possible.

  
**Questions:**

1. What types of projects actually help get callbacks for backend jobs or is that even something I need to be doing?
2. Do companies expect a full microservices project (Docker, Kafka, etc.) or just solid REST APIs?
3. How much frontend (if any) should I show for Java roles?
4. What job boards, networking tips, or LinkedIn strategies are working right now?

Any recent success stories or hiring insights would be super helpful. I‚Äôm looking for what‚Äôs working today to land Java backend roles.

Thanks in advance!

",5,8.0
1009,post,cscareerquestions,1obncbt,1760978725.0,"Working with a smaller company
While looking for my first job out of college, I am in the final round with a pretty small company that‚Äôs growing fast. The job sounds cool and interesting because I‚Äôd have real responsibility and opportunity to work on impactful projects with a team of 2-3 people. It‚Äôs with technology I‚Äôm familiar with, and they‚Äôve also stressed that I‚Äôd start off slow and be able to learn and grow there. 

My only slight concern is that it wouldn‚Äôt necessarily skyrocket my career like a big name company. I know that sounds stupid considering this job market but I‚Äôm getting a few interviews here and there with bigger development teams and I‚Äôm worried I‚Äôll struggle more to find work in 3-5 years. Does company name really matter versus work and responsibility I can showcase?

(I realize I might come off as an asshole so I want to stress I‚Äôve had to fight extremely hard for months and I‚Äôd be lucky and happy with any experience at this point)",2,1.0
1010,post,cscareerquestions,1obkjj6,1760970891.0,"AI Survey
Hi everyone, I am doing research on the adoption of AI in the tech industry. If you have interned in tech or are full time please take two minutes to fill out this form. [https://forms.gle/GpynBFYerBbybnhF6](https://forms.gle/GpynBFYerBbybnhF6) I need to have 200+ responses for my class so it would be very helpful, thank you!",0,7.0
1011,post,cscareerquestions,1obk18e,1760969610.0,"I‚Äôm feeling stuck where I‚Äôm at and not sure I have the skills to land a new job.
I‚Äôve been in the industry for about 6 years now. The first year and a half were at a large company where my role was software engineer.  I wasn‚Äôt really qualified at the time but they were doing a hiring event at my college and I actually started before I graduated (I was in Information systems instead of CS). I  didn‚Äôt love the job so I took the first offer I could to get out and landed at a nontech company, in a sort of software engineer role (we do a lot of adult beverages). I do program some but it‚Äôs really just a WPF application. Most of my team have no idea what I‚Äôm doing including my manager who is a sales lead, not a tech lead. 

I truly think I have no idea what I‚Äôm doing. The first job was great as I learned a TON but since then, I feel like I haven‚Äôt progressed much. I had an interview a few months ago where they asked me simple questions about sql and I had no idea as I haven‚Äôt used it since starting this current job. I don‚Äôt want to be stuck where I‚Äôm at forever but I know that I wouldn‚Äôt be able to land a job somewhere else as I don‚Äôt think I have the skills. I think I can say the only language I really know right now is C# and even my knowledge on that‚Äôs not great. 

I‚Äôm looking for advice on what I can do to advance my career. In my current role/company there really aren‚Äôt many tech related paths I can take. I‚Äôm just a little lost.",1,4.0
1012,post,cscareerquestions,1obk04s,1760969528.0,"Senior level dev, but didn‚Äôt get to earn the title officially before layoffs
Not going to specificy any companies involved.

I am a female web dev with 7+ years experience working on complex applications for high profile clients. Never a job hopper, my second employer supervisor was guaranteeing a title change and promotion once we got to Q1, but they were unaware of the restructuring that would happen.

I had a final round interview where everything went perfect. I did not get the offer unfortunately, and was told I am not senior level, despite answering all technical questions correctly and naturally, and having a history of leading projects and mentoring new hires. I also have high profile references.

I know the job market is super competitive, so maybe that was just their only critique as they decided to choose someone else.

I have 2 interviews today for senior level roles. Anyone have tips for making sure I seem worthy of that title? They are hybrid, and I definitely have some leverage because of that. Not many people want to move or return to office.

TLDR: Senior level skills, didn‚Äôt get official title before company restructure. Asking for advice and tips before 2 senior role interviews today.",7,11.0
1013,post,cscareerquestions,1objx3w,1760969304.0,"Which field is easier to earn in right now if not CS?
Hi guys, I keep hearing everywhere that CS jobs are getting strict, market is cooked, you don't just get hired for doing anything now.
So I was just wondering actually what this means tho.

Like if it's true, are there other fields that are easier to crack with minimum experience and get highly paid like cs once was?
 
From my understanding, it's either that cs was very easy to get paid back then but it's just as strict is other fields now.

Or, the cs field is actually cooked and much harder to crack compared to other fields in proportion to the pay.

So what actually is it? I just needed clarity as to what level of cooked are people talking about since I have interest in this field, but if it's the latter and not the former.. I'd like to consider options.

Thanks for everyone's help and insights.",1,8.0
1014,post,cscareerquestions,1obj462,1760967225.0,"How do you even recover from this
Hey everyone,
I‚Äôm a recent MIAGE engineering graduate from Morocco. I finished a 6-month internship at Omnishore, where I worked on a big insurance platform using .NET 8, Angular 19, SQL Server, and CQRS / Clean Architecture. It was tough, but I learned a lot and thought it would open doors.

After that, I got accepted for a pre-employment internship at Prestige, moved to another city, paid for transport and a gym, even started building a new routine‚Ä¶ and then, out of nowhere, they told me they‚Äôre overstaffed.
Now they‚Äôre offering two options:

Work remotely for free for 3 months until a post is open, or

Come on-site full-time with no clear contract yet.


Honestly, I feel crushed. I‚Äôve already been through this once ‚Äî Omnishore also didn‚Äôt hire me after promising there was a chance. I‚Äôve been trying hard to stay disciplined, rebuild my life, go to the gym, focus on my health and confidence‚Ä¶ but I keep ending up back at zero.

I know I‚Äôm not the only one struggling to find a junior dev job, but I feel completely drained. I‚Äôm trying to stay calm, rebuild, and not lose faith, but it‚Äôs really hard when every opportunity collapses last minute.

If anyone here has been through this ‚Äî how did you keep going?
How do you rebuild your motivation after months of rejection and uncertainty?
Any advice for someone who just wants a stable start and peace of mind?",15,8.0
1015,post,cscareerquestions,1obi616,1760964627.0,"I Pushed AI code on AWS US - East that Caused the Internet to Break. How Do I Hide It
Alot of my AI code passed review because colleagues were just copying and pasting the pull request into AI and asking it to find defects.

The AI weren't finding defects and a N+1 bug caused US East to go down.  How do I hide that it was me.",0,8.0
1016,post,cscareerquestions,1obhom8,1760963238.0,"To those that love to code, how long do we have to wait out this market?
So I'm 31 self-taught web developer, just javascript/node and associated technologies (and maybe that doesn't help me), but a full stack dev. I've been coding for about 4-5 years now and have built some bigger projects. I'll try to keep this short.

I love coding. I have a few hobbies and it's basically coding, playing music, and gaming. I pretty much stick to doing these 3 and I love to learn. For a person like this, you have no choice but to kind of get better at things. I understand the market used to be they'd take anybody with a pulse. I'm definitely a solid junior dev maybe a bit above junior? But...how long do people like me have to wait out this market? How many more years?

In some sense it's like...I'm just gonna keep coding and learning and I figure at some point when this market turns around I'll have some job to fall back on even if its not the exorbitant salaries that previously marked the industry. For me, when I'm in the mood for it, doing leetcode is just sudoku, and any time I get an idea that excites me I typically go build it, but only if the idea excites me. It's stuff like this that makes me know I'm a ""real dev"" - even if I am new.

So I guess my question is just like how long do you guys perceive this market being this way?",25,39.0
1017,post,cscareerquestions,1obhoe3,1760963218.0,"Automation Engineer making $75,000. Am I doing too much for my pay?
Hey everyone,

I could use some perspective from people who‚Äôve been at this a while.

I‚Äôve been at the same US based software company for over 10 years (M/LCOL city). We were acquired and mid-sized now (roughly 150‚Äì200 people) but started small, and I‚Äôve basically grown up here. I‚Äôm the first automation engineer in the company, which I started the role about 5 years ago ‚Äî running Postman/Newman suites, Cypress UI automation, GitLab CI/CD pipelines, QuickSight dashboards, etc. I mentor other juniors, design frameworks, set up reports for management, and handle cross-team coordination.

Lately the company has leaned on me hard ‚Äî I‚Äôve basically absorbed 2‚Äì5x the responsibilities I used to have. I‚Äôm essentially acting as the automation lead, even though the official title is still just ‚ÄúSenior‚Äù.

Despite all that, I‚Äôm only making about $75k USD a year (plus a small bonus), and my last raise was just a cost-of-living adjustment. For context, I live abroad right now, but the pay is still benchmarked to U.S. rates.

I‚Äôm proud of the work that I do, but it‚Äôs starting to feel off. I‚Äôm seeing new hires come in from outside with bigger titles and probably higher pay, while I‚Äôm carrying a lot of the technical and leadership load. The company values those with outside expertise from larger companies a lot I have heard.

I don‚Äôt hate my job ‚Äî I like my director and the work ‚Äî but I‚Äôm starting to feel under-leveraged. I also have some ideas for app-based startups I‚Äôd like to pursue, but I‚Äôd need more capital first, so I‚Äôm thinking in a 6‚Äì24 month window for that.

So I‚Äôm torn between:
	‚Ä¢	staying another year to see if a meaningful promotion or raise actually happens,
	‚Ä¢	scaling back my effort to match the pay, or
	‚Ä¢	starting to look for a higher-level role elsewhere (Lead, Architect, etc.).

If you were in my position ‚Äî 10 + years in, deeply technical, lots of ownership but low pay movement ‚Äî what would you do? I know the market is shit right now, but I feel I could definitely do better, both in the US and in the area I live in.


I‚Äôm not burned out yet, but I can feel it coming if nothing changes.

Appreciate any honest advice from people who‚Äôve been through this.
",2,13.0
1018,post,cscareerquestions,1obdn11,1760945473.0,"How long does it actually take to onboard a new engineer at your company?
Genuinely curious because our team is struggling with this.

We hire decent engineers, but it takes them a lot of time before they ship anything meaningful. Not because they're incompetent, but because:

* Our docs are scattered across Notion, Confluence, old Slack threads
* Nobody knows who owns which service
* Code comments are sparse or outdated
* They waste time asking senior devs ""where is X?"" or ""how does Y work?""

I've been experimenting with a tool that passively watches what senior devs do (files they touch, docs they reference, Slack conversations) and builds a dynamic knowledge graph. When new devs explore the codebase, it proactively suggests: ""Since you're looking at the auth service, here are the 3 docs, 2 PRs, and 1 Slack thread that explain how it works.""

Early tests show new devs get to first PR much faster

But I'm wondering:

1. Is a long ramp time actually normal? Or are we just bad at onboarding?
2. Would something like this actually help, or is slow onboarding just an unavoidable reality?

Would love to hear from other engineering managers or tech leads dealing with this.",0,10.0
1019,post,cscareerquestions,1obbpew,1760938154.0,"Choosing Specialization: AI/Data Science vs Software Development
Choosing Specialization: AI/Data Science vs Software Development

I have a bachelor degree in cs and some work experience with:

Frontend: React, JavaScript

Backend: PHP/Laravel

Databases: SQL & MongoDB

Programming: Python, C++

Some cloud with aws, networking, and basic DevOps


I'm doing a master's degree in cs and need to pick a specialization: AI/Data Science or Software Development. My goal is to work as an AI engineer, but I also want to stay open for software/cloud roles.

My plan: specialize in AI/Data Science, build AI projects while applying software engineering, cloud, and DevOps practices, and fill any gaps (Java, advanced DevOps, QA) via self-study.

Questions:

1. Is AI/Data Science the safer choice given my background?


2. Will this strategy keep me competitive for both AI and software/cloud roles?",3,2.0
1020,post,cscareerquestions,1ob9nfb,1760931543.0,"Advice as a New Grad
Hi! I recently started a job at a big tech company on a infra team as a new grad about 3 months ago. I am starting to get a bit stressed (or overwhelmed) from trying to learn everything. I definitely am getting better at learning our teams services where I am collaborating with other teams on migrations, customer support (other teams at my company), writing a basic design docs for my next project, and code reviews. I still feel like there is so much I don't know and I can't add value back to my team and its very frustrating. I recently had my 90-day performance review and I was told I am doing good so I don't know why I feel so stressed an anxious. At my company it is pretty hard to promote faster than a year and a half to 2 years to SE2 and I honestly don't care about promoting faster (Maybe I do, idk), but I feel like I am taking way too long on tasks. I've had some PRs open in review for like almost 4 weeks now and they still aren't closed. I caused some mini incidents (SEV-5) that I responded to fast and resolved which was a bit stressful, but glad that is over (I know those minor incidents don't matter too much lol). I took 2 days off last week (a long-ish weekend) to visit my GF and kinda unwind, but now that I'm back I feel the stress creeping back again. I don't remember being this worried about work during my internships (maybe because they were a set 3-4 months and I had little to no responsibility). On a side note, my team is great everyone is happy to answer questions and is very understanding of what I don't know.

Has any other new grads and experienced people experienced this?",2,9.0
1021,post,cscareerquestions,1ob9ezh,1760930826.0,"Lost my SWE job after 8 years. Been looking for 10 months and still nothing. Any advice?
I held three different SWE positions at a prominent tech company for the past 8+ years but was unfortunately laid off in January. I‚Äôve been sending out my resume all over the place but I‚Äôm struggling to get a lot of bites.

I‚Äôm a back-end engineer who specializes in C#, .Net and some SQL, however I‚Äôm finding that a lot of the companies I‚Äôve been applying for demand full-stack, but the problem is that I have very little UX experience. I had been meaning to get more into that while I was on my job, but I was never really given an opportunity to learn.

I started a React course a couple of months ago but I‚Äôm having a difficult time maintaining my interest in it. I‚Äôm almost considering abandoning my job search and just focusing on the course just to get it done, but even then I‚Äôll still have fairly minimal experience with React.

The best results I‚Äôve had so far have been individuals from recruiting companies pinging me on LinkedIn. Most of the time this results in me sending a resume to them for a contract job. I‚Äôve had a few seem really promising but then ghost me after I get the resume. 

This last week I got in touch with a contractor who was looking for a position that just so happened to be with my first team at the tech company. They fast-tracked me into an interview that ran for far longer than it should have, and actually ran over what was supposed to be a second interview. The recruiter told me they would reschedule the second interview but I haven‚Äôt heard back from them. The team wants to have someone in the role by the end of this week but now I fear that even they might not be willing to take me back, even though they have my receipts, are probably using my code, and should know what I‚Äôm capable of.

Any advice? I really don‚Äôt want to have to get a masters degree or change careers if I can help it.",90,48.0
1022,post,cscareerquestions,1ob65ty,1760921079.0,"What's currently the best job board/place for junior-mid developers?
All of these job board sites are just either scams or just put on by a company just to show they have openings. Job boards such as indeed, linkedin and even google jobs

I know there are legit jobs on there and your presence on linkedin matters, it's just finding actual open jobs that are actually hiring is really hard

Does anyone have a specific job board/ place they know of that might good for junior to mid developers?",11,14.0
1023,post,cscareerquestions,1ob434j,1760915444.0,"Masters in Computer Science or keep applying.
I'll just be blunt and say that I wasted my undergrad years in college. I have a degree in Computer science, Management, and Communications but I really can't code that well at all. I work in an unrelated job with bad pay (product management) that feels like a dead end. I've been waying options on taking some entry level IT roles or going back for a masters degree. My question is, is that a smart decision? I know people say experience always beats education in this field, but it would give me more opportunities to get internships and would allow me to focus on getting more out of my education.",23,30.0
1024,post,cscareerquestions,1ob3z71,1760915145.0,"How bad is a multi year traditional work gap (3-5 years) if one tried being a founder?
Title pretty much.

Standing at a crossroad to either go back into traditional work after 2.5 years of full-time traveling or trying my way as a founder, building a SaaS and maybe even generate revenue.

Basically if the founder way fails and I don't generate revenue, how hard will it be to find jobs? I'm in my end 20s and horribly scared to not find a job in the future if I go 1-2.5 years more without job but I'd love to build a fully autonomous life.

Also the job market currently is so horrible, that I may as well be forced to try because I couldn't find anything in the last 3 months.

I already have 4 YoE of experience as a Java Web Dev prior to the travels.

Edit: I learned React/Next (was an Angular guy) in those 2.5 years of travel, neovim and lua + did some Python scripting so definitely not completely away from coding. 

Edit2: How fair would be open source contribution additionally? I never did so far but was thinking about it too. ",18,34.0
1025,post,cscareerquestions,1ob1oc5,1760909233.0,"My manager said he ""would rather die than deliver this project late""
Hello all. I'm a software dev with about 6 years of experience. I'm in a bit of a tricky situation and need some advice. I was laid off about a year ago and was super happy to find my current role as a software dev engineer about 4 months ago. My background is mostly backend web applications with some front end work (10%) Upon arrival I found out that I'll be part of a devops team which was not a huge issue for me, I've built CICD pipelines in the past and know the basics of what might be required.

Anyway, about 2 months ago I got handed this high visibility project. Basically it is a massive application monorepo and I'm in charge of the pipeline for this project. I've have been struggling to get any support from the team that manages the application. And the person who has become my ""main contact"" is constantly out of office and I'm starting to notice that the delivery target for this pipeline will get delayed. Whenever I bring up any issues to my manager he immediately dismisses my concerns and his rebuttal is ""oh that's not an issue we can resolve it by xyz"" without actually understanding the concern I'm trying to raise. I suppose I could do a better job of not ""accepting"" his answer and trying to make my point clearer but anyway we are kinda past that.

The other problem is that I've become a defacto project manager. My manager has told me to assign work to other team members and I've had to create a ""second standup"" outside of my main teams standup where we go over the tickets related to this project. My manager has set an aggressive timeline to deliver this project but I'm seeing that we will not be able to deliver it on his timeline and now I'm getting yelled at for delays. I wasn't really expecting to become a project manager for this role, I don't know how to go about dealing my manager who told me in a 1:1 that he would ""rather die than deliver this project late""

Any advice would be appreciated. On the one hand I'm thinking I can use this opportunity to learn about project management etc however I've started doing the ""project manager"" role really close to the deadline so now I'm getting people up to speed etc while we are expecting to go into production asap. But on the other I'm feeling quite overwhelmed and feeling like this was not my expectations for this role.

Thanks for any advice in advance!!",129,69.0
1026,post,cscareerquestions,1ob0fau,1760906243.0,"Hot Take: Engineering is one of the careers with the least amount of stability and job security
Between outsourcing from companies looking to reduce labor costs, the stereotypical agency with ""expertise"" that has never so much as opened a text editor before and just white label contracts every single service externally, big organizations doing pushes then laying off entire departments after or before projects are finished at the whims of leadership - we've seen tons of this from FAANG, the impending downvotes when I describe Indian managers taking over departments and laying off anyone non-Indian and making tons of nepo hires - which we also see in FAANG - all of whom are more than happy to bring the 24/7 work culture and absolutely destroy any semblance of work life balance there once was prior, the prior also applies to anything enterprise or mid-level as the winds change per project and ""KPI-based"" decisions from some consultant that generated a pseudo report to leadership, the constant need to upskill ever year with new frameworks, tech, etc before you get left behind, having to tailor every random resume just to pass ATS and recruiters / firms contracted to hire people with no experience in anything tech, the blatant 1099 vs W2 scenarios with employers abusing lack of SS-8 reporting and investigations into malformed employment standard, etcetera

A lot of people went into engineering thinking it's a more chill job and a golden goose

That was maybe once true but I'd say today it's probably one of the least secure jobs and that's not even including LLM impact. I think most bonafide engineers aren't super worried or impressed by the prior, but leadership is the one laying people off and changing internal gears.

Then of course there is internal politics, general tech ego, and that entire game which has lead to not-uncommon internal blaming and resultant layoffs with someone having to take the heat.

I feel bad for the 2019+ bootcamp grads that spent 5k+ on a camp to enter entry level. It's probably better than blue collar work in terms of exhaustion but the mental strain is equally bad.",285,228.0
1027,post,cscareerquestions,1oaz1ii,1760902999.0,"Should I even switch job now?
New grad almost 1 year into my first job. 

Joined because of the good pay & perks, but I slowly found out my team is a hot mess: no testing, no docs, no staging environment, no ci/cd, a bunch of tech debt and v1/2/3/4/5 to maintain at the same time, stagnant product, team lack of clear direction on what to do next...Very low productivity on everything like oncall, bug fix, project launch, etc, due to all these issues. More importantly, I don't seem to learn much on the job, it's all pretty repetitive work.

I panicked and thought my career growth is gonna be nonexistent,  so I started spraying resume to all the new grad positions blindly several months ago, I was able to get 1-2 offers from some other large company, the pay is on-par with my current company, the work seems more interesting to me, and I signed the offers. 

But now I'm a bit scared when I actually think about job switching.  My manager and my colleagues like me, and my manager is promising a promo in 1-2 years (i know this can be bs), seems like most junior engineers get promoted pretty fast. WLB is ok too.

I chatted with my friends, and it seems like they are all not getting much learning in their job, and it sounds like dealing with a hot mess is a norm in this industry, doesn't that defeat my original purpose for job switching? Given that there's no significant pay bump in these offers and unknown manager/wlb, should I actually just wait at least until 2/3 yoe to promo/jump to the next level? ",28,25.0
1028,post,cscareerquestions,1oaz0iu,1760902932.0,"Building a DevOps homelab and AWS portfolio project. Looking for ideas from people who have done this well
Hey everyone,

I am setting up a DevOps homelab and want to host my own portfolio website on AWS as part of it. The goal is to have something that both shows my skills and helps me learn by doing. I want to treat it like a real production-style setup with CI/CD, infrastructure as code, monitoring, and containerization.

I am trying to think through how to make it more than just a static site. I want it to evolve as I grow, and I want to avoid building something that looks cool but teaches me nothing.

Here are some questions I am exploring and would love input on:

	‚Ä¢	How do you decide what is the right balance between keeping it simple and adding more components for realism?

	‚Ä¢	What parts of a DevOps pipeline or environment are worth showing off in a personal project?

	‚Ä¢	For hands-on learning, is it better to keep everything on AWS or mix in self-hosted systems and a local lab setup?

	‚Ä¢	How do you keep personal projects maintainable when they get complex?

	‚Ä¢	What are some underrated setups or tools that taught you real-world lessons when you built your own homelab?

I would really appreciate hearing from people who have gone through this or have lessons to share. My main goal is to make this project a long-term learning environment that also reflects real DevOps thinking.

Thanks in advance.
",0,2.0
1029,post,cscareerquestions,1oawhh6,1760897072.0,"What specialization paths exist once you've broken into the industry?
Long story short I went form tech support -> low code (webflow+design+jquery lol) -> full stack SWE over my career (28 now) and programming is what I want to pursue long term. 

I feel I am in a decent position now with having a job where I work with NextJS every day, am working on a go/react sideproject as well where I am using websockets and learning about constructing databases etc. 

I want to see what the 'next step' is though and take up something interesting for my next sideproject that has long term possibility of also being a career path. 

My issue though, as a self taught dev (though I want to go low-level as I am genuinely passionate and have studied compsci, just had to leave last year of college due to a family situation), I want to know what are my options to get deeper.

**Things I know exist:**

Go/AWS infra specialization

DevOps specialization

Applied ML (is this an actual field with a decent amount of jobs - it seems fun) 

Cybersec 

Going deeper into web dev 

High performant web app stuff (rust/wasm)

My main goal is that in a year or two, if I ever lose my job, that I am in a strong position to find a new one + ideally to do something I am passionate about, and that seems to be digging deeper rather than working with lots of abstractions as I am now. 

Thanks!",18,7.0
1030,post,cscareerquestions,1oavuzf,1760895626.0,"Is it worth staying at my company and pursing an MS?
Background: I have a little over 1 YOE, working at a defense company since I graduated college. I live in an MCOL area, and my base pay is \~90K. I am expecting a promotion soon (from L1 to L2) which I believe will get me to 110-130K. I am in person full time because my work requires a clearance, which is kind of sucky (I would prefer a hybrid/remote role).

I want to leave my role for a few reasons:

* Salary increase
* Potentially switch to another company with more prestige
* Remote/hybrid opportunities
* The tech stack I'm working with is fairly modern, but I do feel like the project suffers from poor software engineering practices. It lacks structure, has inconsistent code quality, minimal error handling, and an unresponsive, buggy UI. It feels like it was built without clear architecture or professional standards. The codebase is massive, so theres only so much I can do about this.

The problem is that although I want to leave, the current state of the market is really rough and I am considering staying due to the job stability that I have. I am wondering if I should just stick it out with my current company for several more years and get a master's degree while I work full time, which the company will pay for in its entirety. At my company, getting a master's degree leads to a promotion quicker from L2 to L3, and I am thinking if I get a master's degree, especially in the AI/ML space, it will help me in the future when it comes to my career and opening up more jobs. However, doing this will keep me stuck at the company for several years and I won't be able to leave until 1 year after I have obtained the degree.  
  
Given the current state of the market, is it a better idea to stick it out with my company and get a master's degree, or stay screw it and apply to other companies?",3,6.0
1031,post,cscareerquestions,1oau6s0,1760891702.0,"Advice to talk with LLM reliant seniors
I need advice on how to approach seniors who rely too heavily on LLMs. At my workplace, it's split into two groups: one that's super optimistic and lets them write most of the code, and another that‚Äôs more cautious and always stresses the importance of fact-checking because well, they're probabilistic models.

I‚Äôm definitely in the second group. Been gaslighted too many times to trust LLMs like that. I also want to learn, so for me, they're more of a tool to brainstorm or review my own code. The frustrating part is when I review code from the more LLM-optimistic seniors, I keep having to point out mistakes like illogical methods, redundant or overly verbose docs, duplicated or missing or self-fulfilling tests. When I ask about their reasoning, they just point to Copilot like it‚Äôs the answer to everything. It‚Äôs frustrating because they should know better. And after a few times you'd think they got the hint but no.

How do I handle this without creating tension at work? The seniors don't interact much since they‚Äôre on different projects, so I can't really ask others to step in.",3,2.0
1032,post,cscareerquestions,1oatqbe,1760890623.0,"Seeking Advice : Bridging the Gap from CS Grad to Capable Developer
Hi everyone,


I'm a computer science graduate who is currently feeling overwhelmed and behind due to limited hands-on experience and a university course that didn't fully prepare me. I'm highly motivated to catch up, but I'm struggling with where to focus my energy.

The core issue is that I see experienced developers demonstrating deep knowledge across multiple fields (engineering, architecture, and and even security ) while they are also constantly tinkering with side projects, contributing to OSS, and building complex, well-engineered systems. Checking their repos they seem active consistently, including holidays, which I find quite fascinating and I understand their success comes from commitment and love for the craft.

Meanwhile, I feel stuck in a ""CRUD"" application mindset, with the most complex thing I've built being probably a not so great aggregation pipeline.

I know I need to apply knowledge practically to make it stick (I try reading books sometime, but alone they aren't enough), and I want to start on side project, but I can't find anything inspiring. I'm tired of the standard ""inventory manager"" or ""Todo app"" suggestions.


My main 2 obstacles in my mind currently are

- Finding compelling side project ideas that naturally lead me outside of something I can easily oversimply and don't feel the motivation to complicate it.


2- Managing the overwhelming feeling and paralysis that comes from balancing self-improvement with a full-time job. I'm having trouble turning my motivation into consistent action.


How can I strategically start building those more advanced skills? What are some side projects that truly challenge engineering capabilities and aren't just glorified database wrappers? Any advice on maintaining consistency and structure would be hugely appreciated!

If it matters, my main current field is Web, mainly backend development, but I really hope to expand to more things, such as building tools, game dev (mainly for simulations), and maybe even AI (already have the basics I'd say)

Thanks in advance.
 ",0,2.0
1033,post,cscareerquestions,1oaspe2,1760888202.0,"What niches of computer science/software engineering would be good to learn for a mechanical engineer?
I'm a practicing mechanical engineer and I've taken CS50x and CS50P and really enjoyed them.  I'm wondering what I should do after those courses. I would like to do something that can help my career as a mechanical engineer but also give me an opportunity to pivot into tech if I was ever out of a job.

My thoughts are something c++ related since Open Foam (CFD software) uses c++ from what I understand. I have no professional experience with it.

I'm not sure I am interested in web development since I feel like it's far off from mechanical engineering but maybe I'm wrong?

I've also thought maybe some more Python courses on data science but I'm not sure which courses to take, if any.

Are there any other areas in computer science that might overlap with mechanical engineering?",3,1.0
1034,post,cscareerquestions,1oarw3v,1760886241.0,"When did you stop being scared of layoffs?
Was it when you reach a certain number on your retirement accounts? such as 500k? having a 1 year emergency fund? having a certain amount of YOE? I read often times people here are looking forward to get a severance/let go instead of working at their job. So I am curious what this community thinks.  
  
",229,239.0
1035,post,cscareerquestions,1oar0b9,1760884049.0,"I am graduating university in May 2026. I need advice on getting my first job in IT.
My current background in IT/CS: I have worked part-time as a web developer in my campus CS & Engineering department, and participated in an IT internship program at a Fortune 500 multi-industry company.

Academically, I have received several awards and scholarships in STEM in general. I am currently participating in an exchange program abroad (CS & Engineering program).

However, at the end of my internship program, I found out that the company where I was an intern chose not to hire me full-time immediately after graduation. I was also rejected from most positions I had applied for at that company so far (some positions I was still ghosted). Now, I am struggling to find a full-time job after graduation.

I have a wide network of friends and coworkers at some companies I'm interested in joining. However, I am an introvert and found it very difficult to reach out to them. How am I supposed to reach out to my network without feeling shy, like I am desperate and begging them to refer me to their companies? I don't want them to feel like I am using them for something. How can I reach out to them in a professional manner? 

I am also scared of the online assessments (LeetCode/HackerRank/CodeSignal) that companies constantly give me. I have practised coding by myself and have made a lot of programming projects. However, every time I am invited to those ""online assessments"", my mind goes blank, and it's more cruel that I am not allowed to seek help from anyone else or online resources. I have to figure out solutions to every coding problem in roughly an hour by myself without any assistance, and I don't know what companies expect from me. I am OK with doing pair-programming interviews where I can communicate my thoughts to my interviewer; however, in online assessments, I can't. I got hired for my latest internship program without any coding challenges. And in workplaces, I, like everybody else, am allowed to utilise any resources and even generative AI like Copilot to help on doing work, so why am I not allowed to use anything in coding challenges? Overall, I do not know what companies expect me to do in these coding assessments. How can I pass them?

It also takes too much time for me to prepare my CV/resume, apply for jobs, practice my interview skills, network, and practice coding at the same time with my current workload. I don't have time to spend on all of this. And I am scared that I may not have a job immediately after graduation. I just want to earn money by doing what I am passionate about. I truly need advice from everyone. I hope you can help a fellow CS student navigate through all of this. Thanks a lot! I appreciate it!",4,6.0
1036,post,cscareerquestions,1oamty6,1760871801.0,"Should I switch back to SpringBoot for better opportunities or stick to JS/TS & AWS backend?
Should I switch back to SpringBoot for better opportunities or stick to JS/TS & AWS backend?

Hey everyone,

I‚Äôve been feeling a bit stuck lately and could really use some perspective from fellow engineers.

I have around 6 years of experience in software development. I wouldn‚Äôt call myself an extraordinary developer,  just solid, dependable, and still learning every day.

I started my career in Java with Spring Boot, but a few years in, my role shifted toward JavaScript full stack (Node.js + React). For the past 3 years, I‚Äôve been mostly doing backend + cloud (AWS) + some DevOps work.

When I switched to my current company 3 years ago, I got a ~40% hike, but since then, my salary has barely grown. Meanwhile, my friend (who stayed in Spring Boot land) recently made a huge jump, around 250% hike. We both started together, and I know the kind of work he was doing, so it‚Äôs not like he was miles ahead technically. Still, the market clearly values his stack right now.

Now, I know comparison is the thief of joy, and I‚Äôm genuinely happy for him, but it did make me reflect. I‚Äôd like to earn more too, or at least make a meaningful jump (say 150%+).

The catch is: my current project workload is heavy. Every few months we switch to a new product, so I rarely get consistent prep time. That‚Äôs making it harder to gear up for interviews.

So here‚Äôs my dilemma:

Should I switch back to Java/Spring Boot, start brushing up from scratch, rebuild my debugging and tooling familiarity, and hope it opens up more lucrative opportunities?

Or should I stick with the JavaScript + AWS backend world and double down, maybe focus more on system design, architecture, and deeper backend expertise?


I‚Äôm open to tough love too, if my thinking is flawed or if I‚Äôm missing something obvious, please humble me.

Appreciate any honest advice, especially from those who‚Äôve been in a similar boat. üôè",0,4.0
1037,post,cscareerquestions,1oamag1,1760869835.0,"Starting with vibe coding
Hello, I am half way into my software engineering degree and have zero things to show up on my CV. No internship and no student job related to my studies. 

At university we don't do much programming and I feel like i know very little. I also have little interest in programming itself. Maybe it is because of lack of immersion in the field. 

I love to design and i think i prefer frontend over backend. Lately I have been vibe coding some websites. Mostly using AI, fixing some details AI can't understand and that is it. This is really fun for me but probablly pointless? I know nobody can predict the future, but is this approach good start or just losing my time. I am planning to only use AI editors not AI and hopefully gain some knowledge. I would love to hear all the perspectives. Thank you  ",0,13.0
1038,post,cscareerquestions,1oalnlh,1760867396.0,"Does master degree help with transitioning from infra to software engineer?
Hi, currently working as DevOps engineer. Mostly operations and architecture design with AWS and Kubernetes. I feel like this job has a relatively low ceiling and I feel like my job has a lot of reading documentations of new tools and learning how to use it (Terraform, Gitlab CICD, Prometheus, and AWS stuff). I feel like I'm going to be more fulfilled by doing more coding works. However, I'm not from a computer science background and feels that this might be a hurdle for me to move to coding related jobs. I worked as an infra guy for as long as I've been working and never really touched production system's code before. I currently have around 5 years of experience in infrastucture and DevOps.I did learn how to code by myself and did some leetcode problems.

With the market condition and latest concern on AI taking over SWE jobs, I'm thinking of getting a master degree in computer science to be able to work on a more specialized fields like kernel development, designing network cryptography protocol, or work on a more complex network based storage system. 

Redditors who are more experienced in the field, does getting a master degree helps with learning the necessary skills and getting a more specialized job?

Or if you guys have more experience on how you find infrastructure job interesting, I'm also interested in hearing about that.",2,4.0
1039,post,cscareerquestions,1oai457,1760853794.0,"EE vs CS for future student
I honestly have more passion for hardware than SWE work, but I am wondering how both fare in today's job market. I would love to be a SoC or embedded systems engineer, but I'm not sure how feasible that is without going to a top 10 school",2,6.0
1040,post,cscareerquestions,1oah2fm,1760850010.0,"Feels like I'm falling behind?
I have to say a long period of unemployment after grad really makes me feel like I'm falling behind. I know it's hard for international students right now but I still see some of my friends score a deal with Google, Meta, ... and I can't even get a job lol. 

I'm going back to my country soon but I'm not even sure I can compete after wasting 5 month in the US. Also no matter how much leetcode and projects I do I just never feel ready to take on those interview? I'm not sure how smart you got to be to get a good job.  I did a few internship but I never really feel confident to design a professional backend system",24,3.0
1041,post,cscareerquestions,1oagds6,1760847653.0,"What's the future for work that's gonna be the easiest to employ for the next decade
I'm in my 30s I feel like web apps was big, focus either on front or back. Then it became both and now that might also be devops. Now idk whats good in the job market for a regular dev. I mean you could be a unicorn or ai researcher but for the average Joe idk what the focus should be anymore tbh for long term employment ",1,18.0
1042,post,cscareerquestions,1oac7qr,1760834463.0,"Should I switch from CS to IT?
Hey everyone, I‚Äôm a first-year college student about to start my programming classes next semester. Before even beginning college, I was already a bit hesitant about pursuing Computer Science because, honestly, I‚Äôve always struggled with math.

To give some context,  I barely remember any geometry, only know basic algebra, and have zero knowledge of calculus. My math skills probably stop around a 10th-grade level. I was diagnosed with ADHD last year, which explains why I never really paid attention in math growing up.

My main question is: how math-heavy is CS, both in college and in the actual field? I‚Äôve seen people say that the job market for CS is rough right now, which also worries me, though I know things can change by the time I graduate.

I also really enjoy the hands on side of tech fixing computers, setting up systems, troubleshooting, etc. So I‚Äôm wondering if IT might be a better or safer path for me.

I‚Äôm genuinely interested in both fields, but the math side of CS really discourages me. Any advice or personal experiences would help a lot, thank you.",2,14.0
1043,post,cscareerquestions,1oabj8l,1760832457.0,"Should someone thats been tech adjacent for years after graduating with CS degree give up?
My jobs have been in IT, and most recently, ""technical support agent""  , which really seems like application support.

Have got a very nice skillset in azure, aws, docker. Know how to use linux well. Have done tons of python scripting, bash scripting, powershell scripting, etc. 

Everything besides actual software engineering though. Am I cooked from ever getting a software job?",14,19.0
1044,post,cscareerquestions,1oaap42,1760830120.0,"How in your perspective with how the job market right now?
I know this is asked a lot, I just started actually job hunting again seriously and I think my skills are decent enough and my resume/linkedin is decent as well. The thing is most of it doesn't matter if the market is bad.

What's your experience right now and also please say if you're employed or unemployed because it's really looking at it from two different sides",58,97.0
1045,post,cscareerquestions,1oa6rmc,1760820120.0,"[Help/Advice] Final year web development project ideas and tools?
Hi everyone!

I'm in my final year of a *Web Application Development* program (similar to a Software Development degree), and I'm struggling to decide what to do for my final project.

I‚Äôm interested in making a small game using **Phaser 3** or **Godot**, but I also like the idea of doing something related to **web scraping**, since it involves more backend work. The problem is I don‚Äôt want a project that will take 300+ hours to complete.

In my region, the rules have recently changed ‚Äî now we have to work on the project **during the academic year**, not during the internship period, so I‚Äôm a bit lost.

Do you know any websites or resources where I can find examples of final-year web dev projects? Or do you have any ideas that are interesting but still achievable?

Thanks a lot!",1,0.0
1046,post,cscareerquestions,1oa5nx7,1760817532.0,"AI Slop Code: AI is hiding incompetence that used to be obvious
I see a growing amount of (mostly junior) devs are copy-pasting AI code that looks ok but is actually sh\*t. The problem is it's not obviously sh\*t anymore. Mostly Correct syntax, proper formatting, common patterns, so it passes the eye test.

The code has real problems though:

* Overengineering
* Missing edge cases and error handling
* No understanding of our architecture
* Performance issues
* Solves the wrong problem
* Reinventing the wheel / using of new libs

Worst part: they don't understand the code they're committing. Can't debug it, can't maintain it, can't extend it (AI does that as well). Most of our seniors are seeing that pattern and yeah we have PR'S for that, but people seem to produce more crap then ever.

I used to spot lazy work much faster in the past. Now I have to dig deeper in every review to find the hidden problems. AI code is creating MORE work for experienced devs, not less. I mean, I use AI by myself, but I can guide the AI much better to get, what I want.

Anyone else dealing with this? How are you handling it in your teams?",863,214.0
1047,post,cscareerquestions,1oa1ucc,1760808678.0,"BS in Business Analytics, now in ""Buzzword"" AI/BA Master's. What roles are realistic? SWE/MLE/DE/DS out of reach?
Hey r/cscareerquestions,

I'm looking for a reality check and some guidance on what roles I should be targeting. I feel like I was sold a bit of a dream and am now trying to figure out the most realistic path forward.

My Background:

* Education: I graduated in May 2024 with a B.S. in Business Analytics and Information Systems (GPA: 4.0). I'm now in a Master's program for Artificial Intelligence and Business Analytics, expecting to graduate in May 2026.
* ""Buzzword"" Degrees: Honestly, both my bachelor's and my current master's feel like ""buzzword"" degrees. I was told they would open doors to roles like Software Engineer, Machine Learning Engineer, AI Engineer, etc.
* Coursework So Far: My master's coursework has included:
   * One machine learning course (we didn't have to write the code ourselves).
   * One statistics course using R.
   * One course on C# and full-stack web development using the [ASP.NET](http://ASP.NET) Core MVC framework.
   * A course on AWS cloud services.
   * A software testing course.
   * Two SQL courses, one specifically on data warehousing.
* Skills & Projects: I have experience with Python, R, SQL, C#, and JavaScript. I've worked with Pandas, Scikit-learn, and TensorFlow on the data science side. My projects include:
   * Developing a full-stack [ASP.NET](http://ASP.NET) Core MVC web app to track nuclear outages using a RESTful API.
   * Building a fake news detection tool in Python using NLTK and Scikit-learn, where I tested models like SVM and Logistic Regression.
   * Designing and implementing a healthcare data warehouse in Oracle SQL.

My Dilemma:

From reading this sub, it seems like the high-end roles I was told about (SWE, MLE, AI Engineer) are nearly impossible to get without a traditional CS degree, especially at the MS or PhD level. My degrees are from a business school, and I'm worried that pigeonholes me.

My Questions:

1. Is my perception correct? Are roles like SWE, ML Engineer, Data Scientist, or Data Engineer realistically out of reach for me?
2. Should I pivot and focus primarily on Data Analyst or Business Analyst roles, or is it realistic to target Data Engineer and Data Scientist roles as well?
3. If I aim for DA/BA, or even DE/DS roles, what should I be doing right now to be a strong candidate upon graduation? Are there specific skills I'm missing or should double down on (especially for DE/DS)? What kinds of projects would make my resume stand out for these different roles?

Thanks in advance for any advice.",2,2.0
1048,post,cscareerquestions,1oa0lsp,1760805709.0,"How have people gotten married and had a family in a market like this?
(Look at my post history to see a little more context with what I‚Äôve experienced in the market) 

So I got my first job and I really don‚Äôt like it and I honestly don‚Äôt even think I like software engineering but I already graduated and that‚Äôs where my skillset is so I‚Äôm going to stick it out for awhile. But I was wondering, if this is how the market is, how do people with kids make this work?

So if the average tenure of a software dev is 2.5 years, how am I supposed too get married, have kids, and 

1. Stay in one place for 30+ years

2. Be pre-trained with all of the necessary skills to walk into a high paying job that‚Äôs hiring after beating whatever leetcode questions they have, impressing the hiring managers, and beating the competition,

3. Stay there for years, hoping the onboarding is good, the team structure is good, and the expectations are reasonable,

4. Have a CAREER, meaning staying in that specific line of work with promotions and advancements, which means the team structure stays good and expectations remain reasonable even though leadership constantly changes?
How am I supposed to do all of this?

I‚Äôm getting an mba to broaden my options, but i am genuinely confused how people who are married with kids stay in one place and just make it work. 

Do y‚Äôall just constantly take temp jobs and gig work?  Are you constantly switching between technology and random jobs like KFC worker?  Do you sometimes move across the country from your family, pay for your own apartment, and work there and send money home if you can‚Äôt find work? Is there a degree or niche you found that made you constantly employable in your area and if so, how do i find what that skillset would be for my area? 

My fear is not being able to just be a stable adult. Like if one day I have kids they have to deal with daddy being in Nebraska for two years and visiting every 4 months because he couldn‚Äôt find a dev job in his state. What type of life is that? 

I‚Äôve gotten multiple certs and no one seems to care. Getting an MBA and applying for business roles and not getting much of a response.Honestly unsure what to do.

",2,73.0
1049,post,cscareerquestions,1o9zufr,1760803846.0,"UK vs Australia job market for full-stack web developer
I‚Äôm trying to decide between the UK and Australia in terms of job prospects. I know the market‚Äôs tough everywhere right now, but if I had to pick one, which country would give me a better chance of finding a job faster? I‚Äôm not too concerned about salary. I just want to get hired as soon as possible.

My girlfriend is a doctor currently working in the UK. We‚Äôre getting married next year, and she has an option to move to Australia as early as January 2027. She‚Äôs open to staying in the UK or moving to Australia depending on where I‚Äôll have better opportunities.

I‚Äôll be on a dependent visa, so I won‚Äôt need company sponsorship. I have about five years of experience as a full-stack web developer at a consulting company, mostly working with Angular and Spring MVC.

Given my background, which country would likely make it easier for me to land a job quickly?

I am fully aware that the job market is terrible but need some insight from people who are knowledgeable about both these places. Any help will be appreciated. Thank you.

Also, if there are any courses or certifications which I can complete to help me upskill myself, I'd love to hear about them. I want to do everything I can to prepare myself over the course of the next one year.",2,2.0
1050,post,cscareerquestions,1o9zl47,1760803227.0,"I literally dream of Excel spreadsheets!
A couple of months ago I started a new job at a big company. At first it looked like a great opportunity until I realized I only had those few months to get trained of one of the messy files I have ever seen in my decades of career! The logic is complicated, and the files follow the logic but in an informal way with small scatter tables everywhere across dozens of worksheets, cherry picking formulas, etc. It is an Excel nightmare if you know what I mean (despite me being an advanced in Excel). 


Anyways, boss is pissed off from my performance and from my mistakes because I failed to get it yet. I just needed more time but he disagreed and thinks I should have gotten it all by now. Now every deadline is an exam for my performance and he is almost no longer willing to train me further on anything that confusing me. Only documenting my mistakes at this point (I anticipate firing me at any time).


I am already looking for a new job but the market isn‚Äôt great, I have emergency funds and positive net worth but what I can‚Äôt afford is losing my health insurance (I have a family too to support). The stress is so bad that I can‚Äôt enjoy anything anymore in life anymore. Any tips on how to deal with this without quitting till I can find something else or till they fire me?",2,4.0
1051,post,cscareerquestions,1o9tpn3,1760788264.0,"Do you ever leave things undocumented intentionally for the sake of job security?
I was just curious how many people do this. Personally, I refuse to provide exceptionally detailed documentation like what our team on the other side of the world wants because I am worried that they will fire me as soon as they feel like the other team can work independently. Anyone else do this?

Just to be clear, I do document things, but the other team can't figure shit out unless it's super detailed to the point that a non technical person could do it.",14,56.0
1052,post,cscareerquestions,1o9skbf,1760784348.0,"What would you do if u were me
Not studying in US, graduating in 2026, 3.2 cgpa, 4 internships experiences in a bank, a small hedge fund, an e-commerce platform and game studio. 

No RO from the bank due to hiring freeze, have been looking for graduate and internship opportunities but no luck so far. If u were me, will you get a low-paying tech job (easy to find, but possibly outdated stack and uninteresting work) to get some work experience or risk everything and startup or get a master (might be difficult to get into a good program due to low gpa). Maybe a blend of options can work as well, just want some input on this. Thanks ",2,8.0
1053,post,cscareerquestions,1o9rata,1760779573.0,"What do you do in one of those recruiter outreach calls?
It was recently my first time having a recruiter reach out to me for a job opportunity. They've scheduled a 20 minute call with me for Monday to ""get to know eachother"". What should I do in those 20 minutes? Should I treat it like a first interview? Is it too early to ask about pay?",5,6.0
1054,post,cscareerquestions,1o9r9yk,1760779479.0,"What i should prepare for technical support test
So im applied job for technical support role in the web hosting company. What i should prepare for passing the test",2,0.0
1055,post,cscareerquestions,1o9r6il,1760779098.0,"Got a Cybersecurity Internship - 2 Months to Get Up to Speed (Cloud/Distributed Systems Background)
So as the title says - Cybersec is not my forte , I have been more into distributed systems and cloud but I want to get upto speed before my internship Jan 26 (who might convert to full time)  
Im an undergraduate student.  
The JD mentions stuff like -vulnerability assessment, penetration testing,incident response, threat hunting, SOC.  
Any good **hands-on resources** (TryHackMe paths, labs, projects, etc.) you‚Äôd recommend for someone who already knows networking, Linux, and cloud basics but is new to security?  
Also curious ‚Äî how deep should I go into AI/ML + security since they mentioned that in the JD? Is it actually used much in these roles, or more of a buzzword?  
Would love any advice or personal experiences from people who made the jump into security from dev/cloud backgrounds.

Lastly, for anyone working in or transitioning into this field ‚Äî how‚Äôs the **scope and growth** in cybersecurity compared to traditional dev or cloud tracks?  
**Context:** In my interviews, I was asked about topics like the OSI model, TCP handshake, SQL injection, DDoS prevention, OWASP vulnerabilities, and cloud security (S3 bucket policies, rate limiting, etc.) and some web sec Q

",2,0.0
1056,post,cscareerquestions,1o9nq5d,1760766125.0,"Electrical Engineering better than computer engineering degree now?
Seems it offers more flexibility.  You can do computer hardware design or work at a power plant if the world goes to hell.  AI is driving an extreme increase in power generation and energy needs.",94,150.0
1057,post,cscareerquestions,1o9lb6l,1760757933.0,"Does anyone work like 8 months on and 4 months off?
I have always worked at salaried positions with minimal PTO. My kids are at an age where I would like to spend more time with them and travel more. For instance I have a dream of spending a month in Hawaii and a month in Japan seeing all the sights without feeling rushed.  I am curious if anyone has an arrangement where they have significant portions of the year off from work.

I am currently a hybrid manager / architect with 13 years of experience at a Fortune 50 company. Right now every day feels like groundhog day and I am starting to get restless. I am technically competent, can manage people and projects and consistently receive high praise and excellent reviews. I feel like I am finally at a point in my career where I can exercise authority on how and when I work. 

My assumption is this is only possible with contracting which scares me a bit because I am used to the stability of a salaried job. If anyone has any insights or suggestions I would appreciate it. 

",4,10.0
1058,post,cscareerquestions,1o9kzsm,1760756936.0,"Got an out-of-state job invite. They are reimbursing me but how much is okay to spend on travel/lodgings?
Hey, I am a (25F) and the job hunt has been pretty rough as of late. But I finally had a break through recently with a cool job actually in my field. They invited me to interview in person and to get shown around for like 2 days. It is in a different state that would be like an 8-9 hour drive from me. So definitely flying. They told me everything is covered from rental to flight to hotel.

I am in the middle of booking everything now and should I be worried about spending too much? Right now I'm at like $250 for flight, $126 a night for hotel, and then like $200-$300 for rental. They also said meals would be covered by idk how. I know I don't owe the company but I'm not the only one they're flying out so I also don't want to ruin my chances if I overdo it and I'm seen as too much of a hassle to have come in.

Also any tips for things like these? I will be spending an evening with them so are there specific things I should watch out for or remember?

  
Update: Thanks everyone for the help! The best advice was just asking them because I found out that the admin person organizing everything screwed up some details. The only thing I'm even responsible for is flight and apparently they take care of everything else. I should have just asked first but I got nervous trying to get everything in order because my original date for the interview was pushed up sooner. But it's all good now and I feel much more relaxed lol They are taking care of everything ",8,13.0
1059,post,cscareerquestions,1o9j1qu,1760751002.0,"Research (science) roles at NVIDIA - is this compensation range normal?
I have been looking through research positions at big tech (like computational biology, bioinformatics, etc) - typical salary range appears to be really low for jobs that require PhD + prior experience. Like computational biology (genomics) and computational chemistry roles at NVIDIA are listed at $120-200K in the US (SF and Boston areas), which seems to be below SWE new grad levels at these companies. Are research positions fundamentally different from SWE roles?",2,19.0
1060,post,cscareerquestions,1o9imyz,1760749776.0,"Escaping Legacy Tech: Landed 2 AI Offers After 8 Months of Prep (250k+ TC)
For the past 9 years, I‚Äôve been stuck in legacy tech. I built niche monolithic apps with no exposure to distributed systems or system design. Time flew by, and I got pigeonholed in outdated ‚Äúdinosaur‚Äù companies.

Trying to leave my job was incredibly demoralizing. Thousands of job applications and a painfully low callback rate. I was discouraged by this and even more, by my background and lack of modern systems experience.¬†

I posted here asking how long it takes to prep for system design interviews from 0. ¬†Many replies were disheartening, like ‚Äúyou need real on-the-job experience.‚Äù But it turns out‚Ä¶you don‚Äôt‚Äîat least not to pass interviews.¬†

Here‚Äôs what I did while working full-time:

LeetCode (6 months): Focused on the top 150 problems, revisiting and practicing each one 4-5 times. (I failed many, many interviews along the way).

System Design (1.5 months): Started from almost zero and crammed, studying about 15 systems deeply, mainly through videos and practice.

Applications: Sent out over a thousand applications with very low callback. Landed interviews mostly through headhunters. 

Interviews (6 months): Juggled my full-time job while going through processes with 45 companies (failing most of them early on).

It was brutal: endless rejections, self-doubt, and burnout. But I just landed 2 solid offers in AI (around 250k+ TC).

If you‚Äôre in a similar rut, know that it is absolutely doable with consistent effort. You can break free even without the ‚Äúright‚Äù background. AMA if you have questions!",380,101.0
1061,post,cscareerquestions,1o9iju9,1760749515.0,"Under what circumstances is delaying graduation a wise decision?
Since this is a common question and tons of people could benefit from a single answer instead of reading through the multiple posts on here with people that are delaying grad , so under what kind of circumstances is it wiser to delay grad rather thann go straight into the  market?

  
Does it affect job prospects? how so?

  
When is it just not worth it?",0,16.0
1062,post,cscareerquestions,1o9if1x,1760749119.0,"The people with the best careers all have a ""that shouldn't have worked"" story
If you notice all the old HN threads, founder interviews, and current business school advice - they all preach a pattern - almost everyone who ended up somewhere interesting broke some conventional wisdom early on.

One guy cold-emailed a CEO with a working prototype fixing their product's biggest complaint (found via their support forums). Another learned an obscure language because ""that's what the smart people were using"" and ended up being one of 12 people qualified for a role. Someone else spent 6 months building in public what turned into their YC application.

The standard advice: polish your resume, grind LeetCode, apply to 500 jobs - feels like competing where the competition is strongest. Meanwhile, it seems like the interesting opportunities come from doing something orthogonal that most people would call ""a waste of time.""

For those who ended up somewhere unexpected - what unconventional thing did you do that actually worked? What would you tell someone to try that career counselors would hate?

(Ofc ""just network bro"" but am also interested in specific, weird tactics that shouldn't have worked but did)",105,45.0
1063,post,cscareerquestions,1o9h93o,1760745704.0,"Laid off now for exactly 6 months and 16 days. Moving back home.
So I graduated from a city college and started my first job as a backend engineer at Lyft. I got laid off on April 1st 2025, when I had reached about 3.5 YOE of experience, I started my job on October 1st, 2021. I am located in NYC.

My biggest regret was not starting looking for work right away, I took a 3 month break because I was depressed from my first lay off and starting traveling, not knowing a gap increase like that would make it worse. 

I have been preparing for 3 months, have interviewed for a bunch of companies but failed due to very tough calls, and I got a few left now, but interviews just keep getting harder and harder and there is too much variance on what can be asked.   
  
I prepare for leetcode, they ask OOP, I prepare for OOP, they ask a leetcode hard, I prepare for that, they ask me a Java FILE I/O question. Just an example of not knowing enough. 

I have 5 chances left after 4 fails in the past month, and im running out of time and funds, only got 20k left to my name at 28 after paying off all debt. I have the blessing to atleast move back home because I was raised in NY, but it's embarrassing tbh but my parents want me to as they being supportive. 

Wish me luck guys, I genuinely did not expect 6 months lay off, and I was laid off so suddenly and I thought I did good work. Crazy. Please wish ya boy luck.



",380,70.0
1064,post,cscareerquestions,1o9fzp0,1760742208.0,"what countries can a 24 year old with 2 years of exp in full stack get a job sponsorship in?
24 year old with a CS degree and 2 years of exp in full stack I want to move out of my country asap I make about about $2500 because I work remotely but sadly thats not really improving my career at all since I need to work in a company with seniors and get promoted and so on 

but here the salaries locally are about $400-$500 which is shitty so I need something that pays decently even if its half what I make now and I can actually save a part of it and advance my career, it can be in Asia, EU, LATAM anything. ",0,19.0
1065,post,cscareerquestions,1o9drmq,1760736450.0,"Advice on choosing internships
Hi there!! I‚Äôm a junior in CS at a pretty run of the mill university who really enjoys coding but doesn‚Äôt necessarily know what sort of job I want in the future. I‚Äôve been scared about internships for basically all of college, but I‚Äôve had a lot of success this semester and now have 2, possibly 3 companies to choose between. (still have one more round of interviews for company #3)

Company #1 is in the city I already live in so I wouldn‚Äôt have to move, it‚Äôs $30/hr and a tech consulting company which I‚Äôve heard great things about working for. Honestly really leaning towards accepting this offer, the only thing holding me back is that their internship program is a simulated project where you work on a team of other interns to build a project rather than actually working on software that gets used. I‚Äôm curious as to if this matters and if it would hurt my chances of getting a different job if I don‚Äôt get a return offer from this company.

Company #2 is in a different city, in person 3 days a week and virtual 2, about the same pay as number 1, and also a consulting company, but the role of software engineering for specifically AI products and I would work on real projects. The thing is, my team would be virtual so I wouldn‚Äôt actually be able to work with them in person. This is a huge downside for me, as well as the fact that it‚Äôs in another city. (Though only an hour away from where I currently live)

Company #3 I havent gotten an offer from yet and obviously don‚Äôt know yet if I will. Their pay is significantly higher ($45/hour) and in my city and I would be working on real projects. However, this play is notoriously very corporate and competitive and I‚Äôve heard a lot of bad stuff about working there. I‚Äôm not sure if this would apply to having an internship there as well, but it definitely makes me reluctant to want to work there, and ofc I don‚Äôt even know if I‚Äôll get offered the job.

I guess I already know that I want to do company #1, I just want to hear advice from anyone who‚Äôs had a similar internship or if anyone thinks I‚Äôd be making a mistake by working at an internship where it‚Äôs a simulated project as opposed to somewhere where I could show that I contributed to real projects. Again, I don‚Äôt even know if that matters, which is why I‚Äôm asking.

If I do accept company #1, what would you guys recommend doing to help my chances of getting a return offer? I assume that the point of offering this internship is to train interns to then hire to the company, and getting a job after college is what I am most concerned about right now. Additionally, if I do get an offer from company #3, would it be a mistake to turn it down? It‚Äôs a bigger and more well known company than the other 2 and it could be a good resume boost even if I don‚Äôt want to work there post college. 

Any advice is seriously appreciated, thank you guys. 

 ",2,1.0
1066,post,cscareerquestions,1o9crzj,1760734079.0,"The role is less traditional SWE and more Microsoft Power Platform/Power Apps. Is this a red flag?
Interviewing with a company, the job posting made it sound like I'd be working heavily on C# ASP.NET APIs, writing Rest APIs, and doing normal software stuff like my last job.

After interviewing with the hiring manager, he mentioned that it's actually more focused on working with Power Apps (which I do not know or have experience with) but they said my experience as a SWE should be sufficient to get me up to speed with that part of the job. The company itself is not a tech company, but in an entirely different industry/sector. Their tech team is small, and apparently a majority of the time I would be working on would be these Power Apps.

Is this something if I take on, and do for years, would this look bad on my resume? Is this some disparate technology with little overlap to actual SWE work and SWE career growth? Would you take this kind of work for a company which is not tech focused? Moreover, would you move across the country to accept a job like this? I want to feel confident that I won't regret making a large life and career decision based on something that wasn't what I was looking for. I feel like they used normal SWE keywords and kind of bait and switched the role, as the focus will be heavily on these low code platforms which I don't have much experience with.",13,10.0
1067,post,cscareerquestions,1o99x4i,1760727435.0,"Advice regarding comp sci major
Ok so I am currently a senior in High-school right now, but next fall I am going to be enrolled in college. I already have 1.5 years of college done (my basics) so that will give me more time to focus on what I actually want to pursue as a career. Ever since middle school, I have decided I wanted to pursue computer science and since the ending of last year, I have decided I want to major in Artificial Intelligence and Machine Learning and minor in cybersecurity. However, anytime I tell someone I want to major in computer science I get negative comments such as ‚Äúthat‚Äôs unemployment ‚Äú or ‚ÄúAI is going to take over that field‚Äù and that is just making me overthink and second guess what I want to do. I feel like majority of those people simply don‚Äôt know exactly what computer science is though and they have no idea what they are talking about but it still has me concerned. So my question to everyone on here is (if you are taking comp sci right now or particularly those who already have a career in the field) should I still pursue this field? Is it worth it? What exactly do you do? Do I have anything to worry about? ",3,4.0
1068,post,cscareerquestions,1o98c30,1760723791.0,"Linkedin job postings
I've noticed that many software related jobs on Linkedin, posted just a few hours ago already show ""Over 100 people clicked apply"".  
More than 100 people actually applied for the job in just a few hours? Are most of these applicants typically qualified? How does the hiring team manage so many applications?",0,5.0
1069,post,cscareerquestions,1o9872t,1760723472.0,"Help for Google TPS
I applied for an embedded role at google pixel team but getting interviewed for a role with different job description. From the job description I can say role is more about implementing GPS algorithms and gps based location technology systems. I‚Äôm an embedded software engineer and so can anyone suggest practicing what kind of coding questions would be better. As I‚Äôm an embedded software engineer I‚Äôm not good at algorithms but I‚Äôm practicing now. Any help is greatly is appreciated",1,3.0
1070,post,cscareerquestions,1o9830v,1760723210.0,"Has anyone finished a CS degree and then realized they find the field of tech uninteresting?
As a graduate applying for jobs, I have been lying about my interests within roles.

Nothing inside of me stirs when contemplating the different options I have, and nor do I have any drive at all towards them.

I don't really know what happened. I think maybe my idea of this field before I started studying was a lot to different to it in actuality. 

It's very hard to picture yourself elsewhere after 4 years of study (because it's your 4 years deep), but I really can't see myself enjoying working on a computer and doing non-tangible work.",31,37.0
1071,post,cscareerquestions,1o97leg,1760722086.0,"Is it possible to break into this field without a degree in computer science
Like if you did some online courses like CS61 or similar and self studied python through textbooks, would that be sufficient? I know the whole job market is crazy but I have multiple degrees in my current field in a management type of work and I never could find anything so I figure I might as well try to lean into something that has better prospects. 

I need your honest thoughts on this good and bad. ",0,52.0
1072,post,cscareerquestions,1o96f93,1760719453.0,"Feeling stuck after multiple rejections am I missing something?
Hey everyone, I‚Äôve been job hunting for a few months now, and it‚Äôs starting to get really discouraging. I‚Äôve applied to dozens of software engineering roles, tailored my resume for each one, and even worked on personal projects to strengthen my portfolio.

I‚Äôve gotten a few interviews, but every time it ends the same way ‚Äî a polite rejection email or silence. Sometimes I make it pretty deep into the process, then nothing.

I keep asking myself: Am I just not good enough? Has the industry moved past me? Or is this just how competitive things are right now?

If anyone‚Äôs been in the same boat, I‚Äôd really appreciate advice on what actually helps you stand out or get over this hump. Any tips, resources, or strategies are welcome.",7,13.0
1073,post,cscareerquestions,1o94tsz,1760715833.0,"Why is IT (especially software development) always portrayed as a path to burnout on reddit?
Today I on this sub I saw someone say that he has been a programmer for 25 years and another person replied: ""how did you stay sane after so many years?"", that reply got a lot of upvotes. 

But that is not an isolated case, many people on reddit seem to claim that software development destroys your mental health and that kind of stuff.

Do burn out and mental health issues not occur in other professions? Is programming really that much worse than other jobs in that regard?",93,88.0
1074,post,cscareerquestions,1o93kvl,1760713040.0,"With the growth of AI is learning CS even worth it? I'm lowkey scared
I'm in my final year of high school, next year I'm thinking of getting into computer engineering in AI and data science and then plan to become a game designer, but I've recently noticed people creating AI's to create apps and games with the help of AI in minutes, in 4-5 years after I'm done with my bachelor's degree, AI will be far too advanced, tf do I do then?",90,142.0
1075,post,cscareerquestions,1o91zl7,1760709332.0,"Anyone want office hours with a 25 year SWE?
Hey everyone!

I was thinking of just putting up a google meet link every now and then that anyone could join (first come, first serve) and ask questions about getting jobs, how to structure software, interview prep or just design questions on software you might be working on.

Who I am: 25 year SWE, veteran of Fortune 500s, startups and everything in between. I've worked heavily on backend and infrastructure as well as robotics. Lots of different projects and I've been hiring and running interviews for more than half of my career.

If there is interest I can post a link and set something up for this evening.

Cheers!

UPDATE: Wow, lots of interest! Here is the meeting link: Office Hours

Friday, October 17 ¬∑ 6:30 ‚Äì 7:30pm

Time zone: America/New\_York

Google Meet joining info

Video call link: [https://meet.google.com/bvq-meph-sfq](https://meet.google.com/bvq-meph-sfq)

See you guys this evening!",416,129.0
1076,post,cscareerquestions,1o8xz4r,1760697968.0,"Anyone experiencing any changes in the hiring process as a result of the new H1B rules?
I came across this interesting article. I'm in tech but not in the job market, and I'm wondering if you job seekers are noticing any changes as a result of the new H1B rules. https://www.cnbc.com/2025/10/15/trump-h1b-visa-fee-startups-jobs-recruit-hire-workers.html

Edit: replaced the amp link",0,6.0
1077,post,cscareerquestions,1o8r7q4,1760673182.0,"New job, new framework how to kick ass at it fast?
So my first time job hopping into completely new territory. Worked on other frontend frsmeworks the last decade and now I'm hopping into react. Usually there's something familiar when. I job hop but this is my first time jumping into something completely new as a Sr since well I graduated. 

Do you guys have any good advice to shake off the hebie geebies of imposter syndrome before day 1?",6,14.0
1078,post,cscareerquestions,1o8oj8w,1760665292.0,"Why does tech skew so young?
This is odd to me. As someone who swapped into this field later in life, I'm currently outearning everyone in my family (including parents and grandparents) with an entry-level FAANG job. To be earning this amount as a 22y/o fresh out of college would be crazy.

The majority of my coworkers are mid-20s, with some in their 30s. It's extremely rare to see anyone older. Why is that?",541,212.0
1079,post,cscareerquestions,1o8o78q,1760664333.0,"Background Check VS Resignation Notice Period
I accepted a job offer for a big tech company, validated offer with a starting date etc, but my background check  is taking lot more time than I thought. The starting date is in soon to be 2 weeks away and I didn‚Äôt yet resign on my current freelancing contracts. I want to give my clients proper time to transition like 2 weeks. Now I read that it‚Äôs best to wait for the background check to be done before quitting a current job. Though I would like to keep my starting date and respect a 2-weeks notice.

Should I quit my previous gigs already anyway? Should I talk about it to my new employer ?

Note on Background check: it‚Äôs mostly done but still waiting for 2 validations, including one with an ETA close to the starting date",2,6.0
1080,post,cscareerquestions,1o8n3s5,1760661184.0,"Are people at my company sickos or I just don't have what it takes for the job?
Trying to make this short:

I work for a website 8 hrs everyday, sometimes 9 sometimes 10, some nice days 7. It all depends on the workload, and most times there are time constraints to our work.

Now most of it is either frontend or more infrastructure work.

My main issue is, my copmany expects everyone to be constantly ""growing"" and ""making an impact"". Problem is, when I ask if we'll get some time to learn management always says that we should be learning outside of work.

Some people actually do things for the company outside of working hours but I'm just tired man, I'm working all day on features and then I'm asked to keep proggraming either on ""side projects"" or stuff I'm not interested just to keep growing, or else I'd have a bad review.

I don't know if it's just me not having a lot of motivation after work or trying to learn other stuff but I don't  want to spend every waking moment proggraming just because that's what's expected.

Am I wrong for not learning much outside of my job? I know there is infinite knowledge but I'm just tired.",34,16.0
1081,post,cscareerquestions,1o8mu80,1760660422.0,"First-semester CS student at City Tech ‚Äî debating switching to Computer Systems Technology because of the job market. Need advice.
Hey everyone,

I‚Äôm currently in my first semester studying Computer Science at City Tech (CUNY), and honestly, I‚Äôve been feeling pretty lost lately about which direction to go in.

City Tech only offers an Associate‚Äôs in Computer Science, so my plan from the start was to transfer to a four-year program (ideally somewhere like Stony Brook) to finish a full bachelor‚Äôs in CS. But lately, I‚Äôve been thinking a lot about switching my major to Computer Systems Technology (CST) instead, and I can‚Äôt decide what‚Äôs smarter long-term.

The main reason I‚Äôm even considering the switch is the job market. It feels like straight computer science is becoming extremely saturated, and I keep hearing that CST (since it mixes IT, networking, systems administration, and some programming) might open up more immediate and stable job opportunities ‚Äî even at the associate level. At the same time, I don‚Äôt want to make a short-sighted decision that limits me later if I still want to go into software engineering or something more technical.

Here‚Äôs what‚Äôs making me confused:
	‚Ä¢	City Tech‚Äôs CS program ends at the associate level, so I‚Äôd have to transfer if I want to finish a bachelor‚Äôs.
	‚Ä¢	The CST program offers a bachelor‚Äôs, so staying would be easier logistically ‚Äî no transfer stress.
	‚Ä¢	But I‚Äôve heard the CST curriculum is more applied (hardware, networks, databases) and less theoretical (algorithms, discrete math, etc.), and I don‚Äôt know if that will hurt me later on if I want to go deeper into software development or data-related roles.
	‚Ä¢	On the other hand, the job market seems to value practical skills and experience more than pure theory right now, and CST seems to give that earlier.

I‚Äôm just really unsure what the smarter move is. Should I stay in Computer Science, finish my associate‚Äôs, and transfer to a strong CS program like Stony Brook, or should I switch to CST at City Tech and focus on becoming more job-ready sooner?

If anyone‚Äôs been in a similar spot ‚Äî especially if you went to City Tech or a CUNY school ‚Äî I‚Äôd really appreciate your thoughts. How do employers actually view CST vs CS? Would transferring for CS open better long-term doors, or is the more hands-on CST route the better play given how competitive everything‚Äôs gotten?

Any perspective would help. I just don‚Äôt want to make the wrong move early on.

Thanks in advance.
",1,1.0
1082,post,cscareerquestions,1o8mtp9,1760660384.0,"Career Advice: Stay in High-Visibility SRE Role or Switch to Software Engineering for Skill Growth (Debating Between SRE Stability and SWE Growth)
**Introduction**

Hey everyone! I‚Äôm a fairly junior professional who entered the tech industry a little over a year ago. I graduated in 2024 with degrees in **Computer Science and Mathematics**, did a couple of internships, and now work at a **Fortune 500 company** (not FAANG, but still a very well-known name).

**Current Role**

Right now, I‚Äôm on a team that‚Äôs mainly focused on **SRE/Operate** work. I support three large applications (one of them is super critical) and spend most of my time doing **maintenance, monitoring, observability, logs, and production support**.

The upside: I‚Äôve gotten **a lot of visibility** across leadership ‚Äî I regularly interact with my skip‚Äôs manager, higher-ups, and decision-makers.

The downside: I **barely code**, and the skills I‚Äôm building don‚Äôt feel very **transferable** outside of my company, aside from general SRE concepts (SLOs, SLIs, etc.). I also don‚Äôt have a strong SRE mentor or someone I can learn deep reliability engineering from ‚Äî most folks on my team are more on the SWE side with myself and a co-worker (also fairly junior) doing SRE/Operate. For context, I‚Äôve been on this same team since my internship.

**Potential Switch / Future Role**

Recently, I‚Äôve been talking with a **senior manager** who‚Äôs building a **new engineering-focused team** and looking for internal transfers. After chatting with them, it sounds like a great opportunity to **grow my technical skills** and work alongside **experienced software engineers**.

They also mentioned they‚Äôre fine with me being a bit rusty on coding ‚Äî they‚Äôre willing to help me ramp up and get back into it. This new role would offer **a lot more depth** in terms of learning and skill development.

In comparison, my current role gives me **width and visibility**, but not much **depth or engineering skill growth**.

**My Dilemma**

So I‚Äôm kind of stuck deciding between:

* **Staying** in my current role ‚Üí high visibility, stable, decent leadership exposure, but low skill growth and minimal coding.
* **Switching** to the new role ‚Üí less visibility and less predictable security, but strong technical growth and mentorship from other software engineers.

Comp isn‚Äôt an issue ‚Äî both roles pay the same.

**TL;DR:**

Should I stay in a **high-visibility, low-skill growth SRE/Operate** role or move to a **mid-visibility, high- skill growth Software Engineer** role?

Looking for advice from people who‚Äôve been in similar shoes or can generally guide me ‚Äî what‚Äôs the smarter move **long-term**, especially with how fast the **AI and automation** landscape is evolving?",2,11.0
1083,post,cscareerquestions,1o8ml5f,1760659720.0,"quit Zon and move to remote AI startup early career ??
Currently an SWE at the rain forest, 1.5 yoe. Got an offer from a pretty cool AI startup, remote first global company. 

Some context: at Amazon I‚Äôm on an important team, starting to think about promotion, but manager recently moved so that‚Äôs been delayed and harder without him. Probably 6-12 months to L5 (what my L6 said) and salary goes up around 20%. Working 10-6, sometimes 7. Pretty chill wlb and good team. Not many perks. Lots of responsibilities and big projects with large impact.

I definitely have a short term life goal of moving away from London for a few years while I am young. in a year after my promo I can move to the US on L1 visa, idk how feasible it is to get a good transfer internationally tho.

Got an offer from a pretty big startup, it‚Äôs remote first but has offices in a few places, spoke to some engineers and they are working 9-7 so about 2 more per day. Salary around 30% more than Zon, take home post tax is 5.1k, vs 4k atm. Even after promo I‚Äôll be only be on about 4.5k. It‚Äôs an exciting AI company, very interesting fast paced work.

So here‚Äôs the decision-making part‚Ä¶ I‚Äôd make more, moving now, than I would even after a promo here. At the startup, we would report to a tech lead who reports to the CTO; it‚Äôs fast-paced and high ownership (so is Amazon, tbf). I think I‚Äôll have a lot of responsibilities, not treated like an L4, and I think it will supercharge my growth.

My main concerns: is being a digital nomad at 22 weird? Idk how I feel about it for career growth, but I love travelling, so I‚Äôd be excited! But I might be more excited about the USA move (even though it‚Äôs two years away, I‚Äôd be 24). I‚Äôm not sure how easy it is, and I‚Äôm not sure if I‚Äôd have the same opportunity to move if I was in the startup.

What would you guys do? Idk. Anything I‚Äôm not considering? The way I‚Äôm thinking about is - I‚Äôm 22, most of my peers are still unemployed and I am in a very fortunate position. Unsure if I‚Äôm rushing into a job switch too fast.",6,9.0
1084,post,cscareerquestions,1o8m7er,1760658676.0,"I missed the boat on getting promoted this quarter despite positive feedback from teammates, boss, and being told that I am already performing at the next level. How much longer should I give it before searching elsewhere?
I'm a mid-level engineer with 6 yrs of experience. I was expecting to get promoted to Senior this quarter but it did not happen. All my conversations with my boss suggested I was ready for it and my senior coworkers respect me. I had an amazing performance review last quarter and am at the very top of my pay band for the role I'm in. (I'm literally maxed out on base salary in the pay band.)

I suspect the reasons for not being promoted were political and I was declined for promo by my skip-level. I know I need to play the corporate politics game but I am not super close to upper management, and I'm guessing they just weren't aware of my performance.

I could stick around and wait for the next opportunity, but there's no guarantee I'll be promoted. Wondering how long I should give it before searching for other jobs. I've been in this job for less than 2 yrs. 

",49,42.0
1085,post,cscareerquestions,1o8lh2c,1760656738.0,"job hunt successful
Hey y‚Äôall, I want to add a positive data point to the job market discussion here. I graduated with a degree in cs from uc berkeley in 2024 and have just over 1 YOE as a full stack engineer at a small company. I truly started my job search early september and successfully landed an offer at a well-funded sf tech startup in mid october, so just over a month in total. Base is 150k with healthy equity (was able to bump equity a bit through negotiation).¬†

In total, I sent out around 200 apps. 150 were through linkedin or company career sites, and the remaining 50 were through recruiters recommending me to companies on paraform, which had a much higher success rate.¬†

During that month-long job hunt, I did 44 interviews with 20 different companies. I also had 12 recruiter calls. I made it to 3 final rounds and got one offer. Honestly, I‚Äôm very lucky my current company gives me a low enough workload to cram so many interviews. ",125,36.0
1086,post,cscareerquestions,1o8ju66,1760652531.0,"Do overnight shift CS jobs exist?
I‚Äôm expecting a baby soon. I‚Äôm making a career switch and will be job searching soon. My husband‚Äôs current schedule is weekdays 7-3pm. Perfect scenario would be for our schedules to differ so that he can take care of our baby while I am working. 

Are there roles in the comp sci world with non traditional hours? ",69,37.0
1087,post,cscareerquestions,1o8ha0t,1760646383.0,"Not making enough money. Not sure what to do.
Back in the day when I mentioned CS and software dev I would get told how much money it made. After being a full time software dev for a fortune 500 company (not big tech) and making a little shy of 100k (65k take home) i have been struggling with personal finances. My mortgage and housing expenses is 40% of my budget, food is 20%, that doesn't leave much to even get ahead. I have a small 3 bed 1 bath in a poor neighborhood and I'm looking to move for a better school district and my budget is aimed at the worst houses in the market. My wife is disabled and a stay at home mom so I only have 1 income and I'm dealing with health issues myself that makes me not on top of my game.

I studied hard 8 months last year for big tech jobs but when I went to apply I put in apps to all big tech companies I only got Amazon responding and I failed their initial screening due to it being a design problem rather than leetcode. Even tried applying to jobs that weren't big tech and don't get a call back.

I could do a business or at least a micro saas for income but I have too much idea paralysis before starting. I could make anything with software and I make great  full stack software but I don't have a GREAT idea for an undeserved market. 

I could specialize in a field in CS but I am a .NET/go dev with some full stack experience. I could go to a different field like cyber sec or data engineering but I don't know a good list of ones that pay more than software development. 

I am regarded as an up and comer in my organization and work well and hard, but I am underutilized and underpaid. They also don't have a lot of promotion cycles so I don't feel I'm getting promoted as quick as my skills. I should hit senior level within a year or so.

TLDR
Just feeling lost at the moment. I feel starting a business is the only way to get uncapped salary but get stuck with idea paralysis and undeserved markets. I studied hard at a good university and graduated magna cum laude but I just feel stuck like I learned all that for nothing. Not utilizing enough of my skills at work and not getting paid enough and no calls back from jobs. With 60% of my pay going to a small house and food (not to mention medical bills) its tough to get ahead. ",37,98.0
1088,post,cscareerquestions,1o89ji7,1760629215.0,"How to get back into applying for jobs
I have a master's degree in computer science. I have experience as a backend software engineer intern from 2023-24, and for a little over 6 months I've managed to get a part time position at a crappy small networking company that pays a measly $16 an hour, but it's at least good experience to put on a resume, and it's close to my house at least. I completely dropped applying for jobs ever since I got this current position because it genuinely just made me depressed every day, but with full time right around the corner and finding out full time genuinely is just worse in this pay with barely any pay bumps, I want to start looking for better software engineering positions out there. So here's my question: how do I start again?

  
Here's where I am at right now. I already rebuilt my resume, updated my LinkedIn and GitHub to match my current experience, and I have a personal website I already included on my resume and attach on any application. My previous internship had be working on Backend JavaScript most of the time, and my current place utilizes php, python, and CRM development whenever I'm doing programming stuff. I really prefer C# and JavaScript. Admittedly I have not worked on a personal project in a long time, but I intend to work on some C# related projects soon. Where should I be looking for positions? Is it still LinkedIn, or is there a better option? Are there any programming languages that are high in demand right now that I should focus on instead? Should I use a different version of my resume each time I apply for anywhere? I've been out of the game for a while, and I know it's only gotten worse. I'm wondering what my next step should be now that I at least have something worth a damn to put on a resume, or if I should just abandon ship and use my experience for something adjacent. Any help would be appreciated",18,13.0
1089,post,cscareerquestions,1o866r6,1760621379.0,"How to get quicker responses from my team members?
I'm about a year in to my first software development job out of college and enjoy the work that I'm doing but notice that it can sometimes take hours for my collegues to get back to me if I ask them a question on teams.",18,49.0
1090,post,cscareerquestions,1o83oas,1760614312.0,"Nobody Talks About This Phase After You Learn to Code
**When I was a student, I thought once I learned how to code, everything would fall into place: the job, the money, the confidence. But nobody tells you that learning** **to code and getting hired are two completely different battles.**

**I can write decent projects, solve LeetCode problems, and even explain Big O notation yet somehow, the job offers don‚Äôt come as easily as I thought they would. The hardest part isn‚Äôt the coding itself. It‚Äôs staying motivated when rejection emails hit your inbox over and over again.**

**So to everyone grinding right now¬† you‚Äôre not behind. You‚Äôre just in the part of the story that nobody glamorizes: the ‚Äúalmost there‚Äù stage.**

**What‚Äôs the most unexpected lesson you‚Äôve learned in your CS career journey so far?**

",0,6.0
1091,post,cscareerquestions,1o820k5,1760608568.0,"Is it easier to land a job in Data Analysis than in Software Engineering as a fresh CS graduate?
After 3 months of still not getting my first job (pursuing dev jobs), I'm deciding if I should pursue a Data Analyst job. The reason I couldn't get a job in dev roles was because I wasn't knowledgeable in frontend and all job postings are full of fullstack requirements (while confident in backend, I failed every frontend technical exams). 

The reason I've thought of being a Data Analyst was because I only need to study PowerBI and I think I'll have a shot, which is easier than learning frontend from the start like CSS and ReactJS, and even if you mastered it, you won't fit all job postings because some want PHP and some Laravel (everything I said is just my assumption ofc lol). Am I doing a wise choice or is the demand for Data Analysts equally 'low' with SWE?",3,10.0
1092,post,cscareerquestions,1o81noj,1760607209.0,"If we‚Äôre in an AI tech bubble, could someone explain exactly how?

Of course, there are plenty of companies that are basically just wrappers or more ‚Äúagentic‚Äù platform companies, with some focusing on MCP (model context protocol), which might actually be useful. But overall, what really makes this an AI bubble? Is it the tendency to slap ‚ÄúAI‚Äù on every website or product? Or is it the minor, niche improvements over what already exists? I‚Äôm not entirely sure.",0,31.0
1093,post,cscareerquestions,1o813wu,1760605069.0,"How did choose a career?
For some context I finished my CS degree a few months ago and recently joined a small company as a junior backend software engineer. For me at this moment in my life pay is not as important so I chose a small company with an interesting project that will allow me to mess around with new technologies instead of being stuck working on a legacy system in my small box of responsibilities that tends to happen in big companies.
I realised whilst looking for jobs that there so many different paths I could take so I would like advice and how to go about first of all finding out what I enjoy and also learning the most I can.
I am really interested in security but it seems like entry jobs such as pen tester, soc analyst or IT don't really offer all that much regarding experience. Going to dev felt like the most natural progession but even in dev you have software devs, devops, qa, data analysts etc.
I am so lost on what I should follow and what path to take I went with backend because it just seemed like the most natural development from a cs degree and the most interesting one I could find but I've also realised that actually interesting tech jobs that work with all the things I found interesting in my degree such as algorithms, complex multithreading, data structure, machine learning, memory management etc are not really looking for juniors which I understand but I eventually I would love to actually work in a more interesting project.
Do I just have to accept the reality that backend software dev is the most interesting job I will land and just keep at it? Should I stay for sometime until I land something interesting in security for example? Should I switch to devops as soon as I find an opening? I am very lost.",2,6.0
1094,post,cscareerquestions,1o80u9q,1760603977.0,"Master in Data Science ?
I‚Äôm doing a degree in computational social science ( in my school it‚Äôs a mix of programming, economics and sociology with the possibility to get a minor in either computer science or statistics + Data analysis ).
My question is would it be clever to pursue a master in data science ( if I can with that bachelor ) if I want to work with AI ? More specifically in the field of healthcare ? ",2,1.0
1095,post,cscareerquestions,1o7xoxr,1760591731.0,"Career advice
Hey, hope u all are doing well
I came to know that there are two main categories - frontend and backend, so if I learn frontend, I can work in a company or I can freelance building websites and apps, if I learn backend, are my options are only limited to big tech companies who hire for backend roles? 

Which skills should I learn? Frontend or backend 

My priority is to earn majority of income via freelance instead of landing jobs at big tech giants ",1,5.0
1096,post,cscareerquestions,1o7ud0f,1760581081.0,"Stay through the holidays or call it quits?
I‚Äôm in my early career, working as a forward-deployed engineer at a consulting-style company ‚Äî that weird space between dev work and client firefighting.

On paper, it‚Äôs fine: stable job, easy workload, decent title. But the last few months have been chaos. Management‚Äôs scrambling, people are quitting or quietly transferring, and entire projects are collapsing faster than they can be reassigned.

Half the people I used to rely on have left, and now I‚Äôm basically maintaining random fragments of systems that no one else touches. There‚Äôs no mentorship, no technical challenge, and definitely no direction. Every day feels like ‚Äúkeep the lights on‚Äù mode.

The thing is ‚Äî I‚Äôm not overworked. I‚Äôm **understimulated.** The job‚Äôs too easy, the pay‚Äôs on the low side, and the feeling of stagnation is eating me alive. I used to love coding ‚Äî building stuff, solving problems, learning new tech ‚Äî now I just click through Jira tickets and slowly detach a bit more each week.

I‚Äôve thought about quitting a hundred times. I‚Äôve even enrolled in a part-time Master‚Äôs starting next year as a soft reset ‚Äî not because I need the degree, but because I need structure and a sense of progress again.

But with Christmas coming up and everything slowing down, part of me thinks, *‚Äújust coast through the holidays, collect the chill paycheck, maybe even get a promo before you dip.‚Äù*

Then another part of me goes, *‚Äúwhy am I still trying to climb a ladder I don‚Äôt even want to be on?‚Äù*

I know a lot of people here are probably going through their own flavor of career existentialism ‚Äî either can‚Äôt find the perfect job, can‚Äôt get one at all, or are stuck in something that‚Äôs fine on paper but quietly soul-draining. I just want to hear from anyone who‚Äôs in this same weird spot.

How did you break out of the comfort trap early in your career?  
Did you quit cold, coast strategically, go back to study, or just wait until the burnout made the choice for you?",15,18.0
1097,post,cscareerquestions,1o7t38q,1760577479.0,"Getting entry level job
I‚Äôm a fresh graduate with less than a year experience in mobile and frontend internship. It‚Äôs hard to find Java/Spring Boot job in my country, many require minimum exp 2-3 year for entry level job.

What should I do? Should I get a job in different role? For know I‚Äôm still trying to get Java/Spring Boot job since my passion in backend engineering.",0,4.0
1098,post,cscareerquestions,1o7qmsx,1760570754.0,"What is front-end career growth like?
I recently received a new grad offer at a unicorn company, however the role is focused on creating UI design patterns/internal library and other frontend tools related to monitoring and performance optimization. It seems to be a pretty specialized frontend role. 

Can anyone in a front end heavy big tech role speak on what the career growth is like? I am afraid a role like this would limit career growth and employability. Would it be easy to transition to a more full stack role or would I be too pigeonholed to get interviews at other big tech companies?

Alternatively I have a return offer from a big tech for fullstack. But the pay difference is pretty massive so I'm reluctant to take it. ",64,44.0
1099,post,cscareerquestions,1o7qkib,1760570587.0,"New Grad Offer Comparison
Hi I am trying to decide which one would be better for my long-term goals. I want to either work at Prestigious places(like Databricks, OpenAI, Anthropic type big startup) or do my own startup(name value migh help to get noticed by VC maybe?) at some point. For background, I went to both T20-30 school for undergrad and masters(diff school) based in SoCal. I would like to be in the bay because my brother is near there + I want to be in the tech hub for personal growth. 

  
1. Faang adjacent in San Jose (RTO 5)

This was a return offer(technically) from my last internship.

Base 144k Bonus 36K RSU 28K Signing 5k - TC 213k

Pros:

\- More cash

\- Better name value(maybe)

\- Free lunch + Dinner

Cons:

\- Way worse WLB (due to overseas engineers) and culture

\- RTO 5

2. Whatnot (Series E unicorn)

Base 150k RSU \~41k Signing 20k - TC 211K

Pros:

\- Better vibe & culture

\- More ownership of the project

\- Can live home(so no rent but not sure if I will)

\- Faster promotion

Cons:

\- Full remote(scared that I will not grow as much, based on my previous experience)

\- No regular liquid event(equity can technically be paper money)

\- No prestige

",43,78.0
1100,post,cscareerquestions,1o7py2g,1760569008.0,"Have you ever heard of careergrowth dot io? Scam?
Not sure if this is allowed, don't want to advertise for them. Let me know if this is better posted somewhere else.

Being a part of the great tech layoff I have been slogging away with applications. I came across this site with some pretty large promises about how they can help you get not only interviews but actual offers. From what I can tell they have been around for 1 (maybe 2) years.

Their website leaves a lot to be desired. They make some big promises and their intro call is very sales focused on getting you to sign up. Whether they can deliver what they promise, I have no idea.

They have a lot of positive reviews on a reviews site (you can find via google), but they removed the dates and a lot of them read like the same person could have written them. They claim to offer a money back guarantee if they are unsuccessful in getting you a job offer that you want to accept within 120 days.

They seem to be tied to Limitless Growth LLC and another io domain with a similar name. On LinkedIn their CEO doesn't have a picture and the employees don't seem to be clickable. 

Scam? 

",0,4.0
1101,post,cscareerquestions,1o7p81z,1760567244.0,"Remote Contract or FTE On-Site Role?
Hello all,

I recently received two offers. One is a contract role and the other is FTE. Here are the pros/cons to both roles.

Contract: 

* Pays more ($60/hr)
* Fully remote
* 3 months with a ""high possibility"" of extension and a ""potential"" contract to hire (taking it with a grain of salt)
* Opportunity to make holiday pay at the expense of pay rate (EX: $59/hr but will receive 3 paid holidays).

FTE:

* Pays less (Max salary offered is $110k)
* 3 weeks PTO
* Job Stability (?)
* Potentially long commute (live in a big metropolitan area, so can potentially be 1hr+ long commute depending on what time I leave)
* On-site 5 days a week

So yeah. Big dilemma on my end. I'm down the middle on which to go for. I've been working remote for the past few years so the transition to on-site will be difficult, but at the same time I've done it before.",1,8.0
1102,post,cscareerquestions,1o7owiv,1760566426.0,"Would you still work in software if it wasn't for the money?
Are there people here who can retire but still choose to work - if so, what's motivating you?

I love building software, I even volutneer outside of work to build software for others. But I think the corporate is an unhealthy aspect of this field. The constant layoffs, the interview hoops of job interviews. The constant need to be more 'efficient', losing your co workers to restructuring, the lack of PTO. Stack ranking, etc.

If I'm retiring tommorow, I'm travelling abroad for a year, I think I might get a job again since I love coding, but if it's too hard to get a job I think I'll  relax at a beach lol.",149,254.0
1103,post,cscareerquestions,1o7nw3v,1760563925.0,"Do I even have a chance with my experience?
I‚Äôve been a consultant for over 2 years now, it‚Äôs my first job, but I am not happy with it. They mainly want me working on power platform and Sharepoint. There are no promotions either. I would like to leave and find a new job and get my career on track but do I have any chance or am I behind even recent grads? I still know all my coding languages but I haven‚Äôt had the chance to use them, I also communicate with stakeholders and have mastered the power platform: do I have any chance of out competing everyone going for entry level dev roles or maybe a mid level tech ba? Or am I trapped here for more years? What can I do to fix this?",5,3.0
1104,post,cscareerquestions,1o7lt56,1760559160.0,"Asking for a specific language in technical round
All of the jobs I am applying to list at least C++, C#, and Python, usually, bunch of other scripting languages and rust.

During the screen I make it a point to say that C++ is my native language and I do all my whiteboard technical questions using this language. And that I use other languages when the project requires, but I only write code in paper from scratch in C++.

And the reason for this, is there are so many similar operations in other languages that it is easy to confuse minor syntax, functions, or operators, and I specifically study and practice for interviews in C++

Is this a reasonable accommodation to ask for in an interview?

Most of the companies let me select the language I want to use. But I am not sure if this is a universally accepted standard with CS.

If they want to ask me some technical questions specific to other languages, to make sure I know them, I am ok with that, but specifically writing code in person on a piece of paper or a board will require using C++ for me. It is the native language, it is what I studied in school.

I understand if the job description is specifically asking for a python developer or a web developer I would have to use those, but these are not the jobs I am applying, I am targeting specifically jobs where C++ is the main requirement. The problem is none of the job description specifically ask for just one or two things, they ask you for full knowledge of like 10 languages and years of experience and something that only existed for a couple years. And I'm okay with that I'm just not going to be riding the stuff on the whiteboard from scratch in one language when I can do it just as well in another

I am at the point where I am doing second round interviews almost every day and doing coding problems every day between those, and having to study up on python or numpy or pandas or etc syntax for just 1 job that will probably ghost me anyway is just going to completely throw me off and cause unnecessary headaches

",1,13.0
1105,post,cscareerquestions,1o7ktb5,1760556877.0,"I went to my first career fair and it was kinda pointless
Legit none of them are hiring right now it‚Äôs for next year or summer which is fine, but that also means they could easily forget everyone they talked to today. Basically all the booths I went to, I presented myself, talked to them, shared my passion and stated my tech background, heard them out for my questions about the roles, then they say apply online. It almost always ends with scanning a qr code and applying online when things open

Idk man i feel like I wasted time idk how people get jobs like this. It wasn‚Äôt FAANG companies either it was a lot of smaller companies I haven‚Äôt heard about until now like I fr spent almost 2 hours just to be sent online repeatedly for roles that‚Äôll open months in advance üòê. I could‚Äôve just searched up their career sites and saved time so what‚Äôs the point, and how do dudes get jobs like this 

Also about the FAANG point, I said that because usually bigger companies do this but I wanted to emphasize that even the smaller ones still do it. 

Edit: one recruiter did offer his LinkedIn though so idk if that means much. And all the companies took my resume. But that resume is within a whole sea of them so I don‚Äôt get the difference.",211,67.0
1106,post,cscareerquestions,1o7k7n6,1760555519.0,"Dear companies, time to hop on the in person testing train. Google is officially doing in person candidate testing again.
See video: https://www.youtube.com/watch?v=LHkbSNEVcAA

It seems the cheaters have now forced companies to finally bring back in person candidate testing again. I say good. Goodbye to all the cheaters. More companies need to follow. Not just for internships, but all jobs. This online stuff needs to stop. It leads to companies considering way too many people and becoming way too picky. Also, hiring cheaters and causing non cheaters to be punished because standards are way out of line with reality.

People who were saying this couldn't be done are strange to me. It is literally how it was always done prior to covid and pretty much forever.

I think many of the people coming up with the questions for candidates are in for a rude awakening to realize how horrible they were at spotting cheaters. There egos won't let them admit it, but they will probably have to lower the difficulty of questions because cheaters artificially caused the standards to be raised way too high.",994,256.0
1107,post,cscareerquestions,1o7k3hn,1760555250.0,"Leave current sucky job for another one but pay more?
Need advice 

I am at a company that is going through a weird phase. I was on a team with a senior, mid level, and two juniors besides me.

They have all left or have put in their resignation. Only me and one junior are left. Our young senior is pretty inexperienced. Hes great as a person but maybe not good in terms of technical decisions and good practice. 

This new contractor seems like the real deal. He was a tech lead and principal before this. I feel like I could learn a lot from him and really get an understanding of what a real engineer does. Hes only here for 3-6 months tho (likely 6).

I also have a somewhat sucky manager. Not best leader. Most people left because the director of engineering was really horrible. The good thing is he‚Äôs leaving by the end of December.

This role is for backend, which is my interest. 

Now I‚Äôm in a pickle. 

A friend is at another somewhat sucky company but they were hiring. She got me an interview and I was offered the job. Sounds chaotic and also sucky in terms of leadership, but at least she‚Äôs there and also one of the juniors is also going there but on a diff team.

My friend would be on my team and kinda be like my senior. She actually used to work at my current place, she was one of the exodus. 

This new role would be a midlevel role and focus on platform engineering, which I‚Äôve done a bit of but isn‚Äôt exactly in my interest but that‚Äôs the roles focus. 

It would take me from being a junior (it‚Äôs only been 5 months lol), to a midlevel, which I‚Äôm not at all. I just finessed the interviewers. 

It pays about 10k more (in Uk standards that‚Äôs big). 

I‚Äôm very conflicted because I feel like my current company is a mess but it might get better? Meanwhile my friends company is also kind of a mess but I‚Äôd get paid more. Theyre building their team for the first time rn. 

Idk what to do. Part of me wants to wait and see if things get better and learn from this contractor. Another part of me feels like I shouldn‚Äôt wait and just dip. It's hard cause the contractor might not even stay who knows. Meanwhile my friend is great, but she's also basically being the manager to her own team, which doesn't sound normal either. 

Does anyone have advice?",2,1.0
1108,post,cscareerquestions,1o7danf,1760540238.0,"Switch from java to python
My university course is in java. I have a little bit of coding experience (mostly using AI) in python. I have interned, and I will aim at jobs that use python, so my questions is:
How easy is the switch from java to python? and/or Should I keep programming in python on the side? ",0,7.0
1109,post,cscareerquestions,1o7cd6q,1760538135.0,"Struggling with mental health and Failure
In December my manager asked me to quit and that I‚Äôd probably be let go by August. I was taken by surprise and didn‚Äôt understand why. My new manager was promoted and I was placed under him. My old one is skip manager. I was having lunch with my manager and he laughed when I said I was busy. Eventually in a meeting with him, he snaps at me and tells me to ‚Äúthink!‚Äù. I was scared and confused. Keep in mind that I have Autism and severe anxiety. Usually, I get moved to a new project every sprint and have to deliver on time or else. I didn‚Äôt really have anyone available to help if I needed them, just a few minutes of explanation on a good day. I was hesitant since I began asking for ADA 6 months prior. I decided to call again and got my ADA approved for 60 days for Autism. I told my manager that I had ASD and would experience memory lapses under enough anxiety. He told me, ‚Äúyou can over come it.‚Äù He glanced at the paper explaining my condition and didn‚Äôt keep it. 5 days later he puts me on pip and detailing poor code quality. I was shocked. He himself approved those PRs and no one else found issue with it. When I requested pip papers, he gave it to me a month later without the pages of code he showed me. My anxiety skyrocketed to a point where I took extra days off and time to recover from. He never talked to me like the others in the office, and left me out of many team meetings. He puts me on one project where I had to do big data work when my strength was backend. The POC I needed to sign off my work kept changing the solutioning for the data and my ASD brain went into overdrive to make sure I could grasp it. That took 4 weeks. I was struggling with my mental health and updated my mid year a little late. My manager only based my mid year on those 4 weeks only. Shortly after I got very ill and lost a loved one in an accident. My manager told me to compartmentalize. Day 60 into the 90 pip, I ask him for more work since I‚Äôve completed the recent work on time. He told me, ‚ÄúI‚Äôm working hard to find you work.‚Äù He moved our 1:1 meeting to 4:30. Once I show up to the meeting he said that I showed little improvement and had security escort me out the office. I never saw or heard from HR once. No severance, just out on the street. The ADA expired weeks ago and I was going to reapply once I saw my doctor again. I don‚Äôt know what I did wrong. I pushed myself past 100% trying to do as he asked. I compartmentalized and dedicated most of my free time to rest until work the next morning. What did I do wrong? He asked me to quit my black employee resource group, I skipped time with family, and put in an extra few hours on some days to ensure perfect code. I don‚Äôt know what I did to disappoint them. My mind would shutdown from exhaustion, but I was on medication to push me past 100%. I‚Äôm at home now recovering before I seek my next job. It was my first tech job out of college 2 YOE.",19,18.0
1110,post,cscareerquestions,1o7b552,1760535242.0,"Management vs Tech, new job decisions
I‚Äôm currently in a remote tech job and I‚Äôm doing ok, coasting, but not moving up. Also haven‚Äôt received a raise in years. I was offered a tech management job in an industry that is not known for tech. The team sounds very stressed and majority is offshore. It requires in person at the office and it will be stressful. The pay increase is good and I‚Äôm getting older (late 40‚Äôs) so I think I should take it. But my lifestyle and work life balance will definitely change. What should I do? 
",3,8.0
1111,post,cscareerquestions,1o73dig,1760508885.0,"Need Advice. Bad Work Culture.
[This is also a vent]
So I work for this startup. Its pre seed as of now. Based in Cannada and has clients from europe. 

It pays a living wage, I'm still doing it as I haven't found a better job. 

The issue is, my manager expects me to build production quality AI apps w toy level compute infra. He doesnt just let me upgrade to bigger models and GPUs, as he is desperate to save every penny he can. The founder on the other hand is willing to spend on compute and keeps asking me for a working prototype. I cant complain on my manager as they've known each other for several years and are good friends. Also they're way older than me, so there's hardly any friendly vibe. I cant communicate so easily.

They expect me to work overtime, even on weekends. I just dont k wtf im supposed to do. Any advice is appreciated. 

",1,2.0
1112,post,cscareerquestions,1o728ii,1760504825.0,"Is getting a CS degree worth it if I already have an old ICT degree?
I graduated with an Bachelor in ICT a few years ago and have been working in IT since. I have been looking at entry level software dev jobs but they are almost all graduate jobs. 

Would it be better focusing on expanding my portfolio to demonstrate recent knowledge and applying to those jobs anyway or would I be better off getting the full CS degree while also getting eligibility for applying for graduate roles as well?",1,2.0
1113,post,cscareerquestions,1o71qh8,1760503125.0,"Online Assessment for IBM Research AI Engineer, Intern
I‚Äôm a PhD student, what can I expect from the OA? More LC type (hard, medium?) or more research based? Anyone completed this yet? The email said they will have multiple choice questions and coding problems. Preferred language is python.",0,0.0
1114,post,cscareerquestions,1o71ke9,1760502570.0,"Chance of getting a job as a new grad? (Ontario, Canada)
Currently a senior at queen's university, got 2, 4 month internships under my belt(at the big 5 banks), a small startup(200 users and growing), and a decent GPA. Worried about getting a job after grad, even thought about doing an extra year, to improve my resume by adding projects and taking extra courses but not sure if it's worth it. Would appreciate any advice thanks. ",10,32.0
1115,post,cscareerquestions,1o6zfci,1760496019.0,"Got laid off from my first job
I got laid off from my first job about 3 months ago, and it‚Äôs been an emotional rollercoaster since. I went through everything sadness, anxiety, crying at night, questioning my worth  all of it.

What really broke me wasn‚Äôt just losing the job, but realizing that the people I thought were my friends at work‚Ä¶ really weren‚Äôt. We used to have fun discussions, laugh, share personal stuff I genuinely thought we were close. But after I got laid off, it was like I never existed.

I reached out to one person from my old team just to see how things were going there, and she completely ignored my message. That hit me harder than I expected. It made me feel so small, like I was begging for attention or validation  when all I wanted was some human decency.

I‚Äôm still early in my career, just a fresh grad, and this was my first real job. I was one of the top performers on the team too, so getting laid off and then being treated like that felt like a slap in the face.

I know I‚Äôll bounce back eventually, but man‚Ä¶ this experience gave me a real taste of how cold things can get in the professional world.

Has anyone else gone through something similar after being laid off? How did you deal with that feeling of being forgotten so quickly? How you handled their behaviour man.",170,41.0
1116,post,cscareerquestions,1o6xyg7,1760491842.0,"UMD or GT Online for Computer Science/Software Development/Engineering Masters
Basically what the title is. I'm having a hard time finding an entry-level job post-graduation and my mom's friend who works in the industry recommended I get my masters. I live in Maryland so commuting an hour isn't horrible for my masters. The most affordable option for me would be the online Georgia Tech Masters in CS. What would be the best option? UMD programs I'm looking at don't all have in-state tuition so a program might be 50K but I'd get more experience with RA and TA position's which could cut the cost and help if I ever want to go for a PhD.  And..... if I can find a job mid masters that could possibly help with some cost. ",2,3.0
1117,post,cscareerquestions,1o6vjv0,1760485205.0,"Completely losing interest in the career due to AI and AI-pilled people
Within the span of maybe 2 months my corporate job went from ""I'll be here for life"" to ""Time to switch careers?"" Some exec somewhere in the company decided everyone needs to be talking to AI, and they track how often you're talking with it. I ended up on a naughty list for the first time in my career, despite never having performance issues. I explain to my manager and his response is to just ask it meaningless questions. Okay, fine whatever. Then came the ""vibe coding"" initiative. As if we don't have enough inexperience on our teams due to constant layoffs, we're now actively encouraging people to make mistakes and trust AI for the sake of speed. Healthcare company by the way (yikes).

What happened to actually knowing things? When will people realize AI is frequently, confidently wrong? I feel like an insane person shouting on every company survey and in every town hall meeting to get these AI-pilled people to understand the damage they are doing. We have people introducing double-digit numbers of defects on single user stories now, and those people don't get in trouble (meanwhile I'm a bad person because I didn't talk to AI last week, for shame!). 

I have been applying to dozens of jobs, but every job I apply to is now a game of appeasing an AI reading my application. Of course the market just being crummy in general at the moment doesn't help. Most of the job postings are in developing AI tools that won't be around a year or two from now when they inevitably flop. I'm sure there are companies out there that aren't buying into the AI hype or are just too small to necessitate them, but they seem few and far between.

I'm realizing I have such an appreciation for the critical thinking and problem solving aspects of the career, but as it changes I'm falling out of love with what it is becoming. I feel like I'm on The Truman Show when having to listen to these AI-pilled people. What's your approach to dealing with this? I'd love to hear perspectives from my fellow anti-AI/skeptics. I'm not sure if I'm looking for a ""change my mind"" or ""you're not alone"" but I'd love any reassurance or suggestions.",577,172.0
1118,post,cscareerquestions,1o6v0jg,1760483794.0,"Backend devs, what's your strategy for 2025?
**Hey fellow devs,**

Before 2024, the backend interview process was pretty standardized ‚Äî especially for companies following the big tech playbook. It usually started with an online assessment featuring two Leetcode-style questions, followed by a phone interview with one or two more. If that went well, you'd move on to a virtual onsite with a few additional coding rounds, a system design interview, and a behavioral round. You could use any language you preferred.

But lately, things have shifted.

I‚Äôve noticed interviews becoming more domain-specific and less theoretical. For example, I was asked to write a SQL query in Microsoft SQL Server ‚Äî even though I never listed that on my r√©sum√©. In other cases, I‚Äôve been restricted to coding in a specific language, or asked to build a small project during the technical round. Sometimes maybe a take home project.

It feels like some companies are moving away from the traditional big tech methodology. Has anyone else seen this trend?

**What‚Äôs your strategy when companies take a more customized or practical approach to interviewing?** Curious how others are adapting.

  
Are you ignoring them or delaying scheduling to get more prep time?

  
(I asked ChatGPT to polish my writing)",0,8.0
1119,post,cscareerquestions,1o6uz9u,1760483703.0,"Torn between career opportunity and location, would a few years in Big Tech help me relocate later?
Hey everyone, I'm in a bit of a unique situation, and I'd love some advice! I'm about to finish my final year of graduate school, and I just did an internship for a big tech company. When I signed my internship offer letter, my hiring manager said I could work out of the office where my fiance is located for FTE because my org had a hub there (a city in the southeastern US). Unfortunately, with recent restructuring, the org was eliminated, and I've now been placed into a different group for FTE where this is no longer possible. Accepting my FTE offer would put us on opposite coasts. As a new grad, I'm obviously having a hard time getting interviews for jobs in the city I was initially told I could be. Additionally, for very legitimate reasons, my fiance isn't able to relocate away from his city. What is my best path forward here? Will I have a better time in the southeastern job market with a year or two of big tech/west coast experience, or should I instead focus on targeting southeastern cities for networking purposes, even if it's not the exact city where he's located?",2,8.0
1120,post,cscareerquestions,1o6qbv7,1760472698.0,"Should I go back to school? (Need Advice)
Greetings, newer Comp Sci Grad here. Been struggling to land a job since I graduated a couple months ago as Im sure others of you are. I could go on about what I could've done better in college to get a job but Im here now and we move forward. I never wanted to go back to school but student loan payments are coming up and im getting nervous. I don't wanna be idle for too long and I've just been working random jobs here and there to get buy but I wanna put my degree to use. I've been thinking about going back to school and getting my MBA so I could be more on the administrative side of things as coding has never really been my strongsuit anyway. Ive definitely always been better at delegating than executing myself but Im sure im not special there lol. But I just want to be sure that the MBA is worth pairing with my BS or would it just be a waste of time. I'm thinking I could grind that out land an internship and hopefully land a job offer from there. But i wanna hear from more of you experienced comp sci professionals. Should I go back? Would the MBA be worth it? Or if not the MBA what would be a better thing to go back to school for??? Any advice would be appreciated",2,5.0
1121,post,cscareerquestions,1o6pyts,1760471887.0,"Don't apply using r/DevJobLeadsOnReddit, might be a scam
It's a tough market out there, and unfortunately it's also one of the easiest places for honest folks to get duped. Honestly, getting a ""noreply"" is better than having your information sold and being contacted for the hundredth time by some random sales associate (usually speaking broken English with a generic pitch).

That brings me to the subreddit in question. The first post I came across was this one:  
[Software Engineer - HTML/CSS/JS @ Apple | $120K‚Äì$210K | Paid Relocation](https://www.reddit.com/r/DevJobLeadsOnReddit/comments/1o1dgrx/software_engineer_htmlcssjs_apple_120k210k_paid/)

It struck me as incredibly generic and out of touch. I left a comment for future readers because the post felt vague ‚Äî basic HTML/CSS/JS skills, sky-high compensation, and no real details on location, employment type, or expectations. Most legitimate job postings include more concrete requirements. When they don‚Äôt, that‚Äôs usually a red flag.

Out of curiosity, I reached out to one of the mods, who also claims to be the creator of the sub. While the conversation was polite, almost every question I asked was either dodged or brushed off. From what I could gather, they're scraping job listings from various sources, rewriting them into more digestible formats, and then notifying the companies afterward.

When I asked if they had any process to clean up expired or inactive roles, how the employers are notified, or if they could post the sources for these scraped jobs, the conversation was cut short. Shortly after, I received a permanent ban from the subreddit. That, to me, is a bad sign.

I wasn‚Äôt planning to apply through them anyway. I highly doubt Apple is scanning Reddit for job applicants based on comments and quick pitches. But I do care that people might be getting taken advantage of without realizing it.

The job market is already stressful enough. The least we can do is call out shady practices when we see them.

**TL;DR:** The subreddit r/DevJobLeadsOnReddit posted a sketchy-looking Apple job with vague requirements and high pay. I asked the mod some basic questions about sourcing and expired listings, got vague answers, and was banned shortly after. Feels like they're scraping jobs and repackaging them with little transparency. Be cautious, this looks more like a traffic funnel than a legit job board adjacent.",31,6.0
1122,post,cscareerquestions,1o6pu35,1760471597.0,"Relearning to code as a designer: what‚Äôs the smartest path to become independent again?
Hey everyone!

I‚Äôm a designer getting back into programming after a few years away, and I‚Äôm trying to set realistic expectations for myself.

My goal:  
I would like to be able to code and deploy my own projects from scratch : portfolios, landing pages, dashboards, maybe even small e-commerce sites.  
I currently use Framer/Webflow, but I want to be more independent and expand both my creative and technical range.

My background:

* I know HTML/CSS well
* I have JavaScript fundamentals (DOM manipulation, functions, event listeners, etc.).
* I used to do a bit of PHP (mainly with WordPress).
* I‚Äôve done few Python scripts for myself and for my previous job (I completed Angela Yu‚Äôs Udemy Python course years ago).

What I‚Äôve lost (or never really mastered):

* Good coding practices and project structure.
* How to set up a proper development environment.
* I barely remember Git, also I can‚Äôt make sense of most GitHub project architectures right now: there's too many files and code I can't read, how people understand it all?
* APIs in Js, modern JavaScript frameworks (React, Next.js), Node.js, and deployment (Vercel, etc.)

So my question to you all:  
How long do you think it would take to become fully operational again and to build complete, production-ready projects solo? I can have 2 hours/day for this. I started The Odin Project few years ago and I stopped at the beginning asynchronous Javascript.  
And in what order would you suggest I rebuild my skills?

I‚Äôd really love to hear from people who‚Äôve gone through a similar ‚Äúdesigner-to-dev‚Äù path.

Thanks in advance, any roadmap or personal experience would be super helpful!! üôè",2,2.0
1123,post,cscareerquestions,1o6p50b,1760470052.0,"What would the path to being able to get this role at OpenAI look like?
I came across this position at OpenAI for [Research Engineer / Scientist, Interpretability](https://jobs.ashbyhq.com/openai/c44268f1-717b-4da3-9943-2557f7d739f0), and while I'm sure I don't have a chance at it right now, I'm curious what the path to being able to land that type of position would look like. I would love to do this type of work, especially looking into and being able to influence AI/AGI safety.

My background: I have a PhD in aerospace engineering that looked into modeling spacecraft trajectories using machine learning. I moved with my wife for her work to an area that has no aerospace opportunities around (southeast Michigan), and there are virtually no remote opportunities in the aerospace industry, so I've been trying to find a role as an ML engineer instead. I graduated in May 2022, then after 5 months of no luck I ended up taking an IT role at a small company where I had a personal contact because it paid pretty well and bills needed to be paid. This January I was laid off and since then I've been trying to find a position as an ML engineer or more generally as a software engineer. Previously I had 5 internships, 2 of which were ML-based. My PhD and internships primarily used Python and MATLAB, and recently I've been developing a project in C++ to learn that as well.

Theoretically, how would I go from where I'm at with basically 0 relevant YOE to landing a top AI job?",3,16.0
1124,post,cscareerquestions,1o6o8ip,1760468029.0,"Prediction of what tech industry in 2027 could look like
Found this sim of 2027 job industry¬†[https://marbleos.com](https://marbleos.com/?os=osx)",0,20.0
1125,post,cscareerquestions,1o6m6vz,1760463520.0,"How long does the euphoric use of AI last?
My boss, in the course of pursuing an advanced degree, has been exposed to the ""magic"" of AI (mainly Clade, so ""SOTA"") use in the course of completing assignments. Think create a django blog application, with a sqlite db, simple stuff, all greenfield.  We are both very skeptical of AI in general and both have been developers for almost 20 years each, but after a weekend of working with it he seems to have seen the light to the productivity boost. I have hit or miss experiences with AI, but never used it to vibe code. He spoke to a Microsoft Azure rep and they mentioned that AI is great for vibe coding greenfield stuff, but it doesn't work as well with established code. He is now in the process of using AI to take an existing C# .Net 4.8 MVC application to Blazor using Telerik controls to see the viable of using it to churn out new features faster.

From reading this sub and Twitter, there seems to be a period where vibe coding with AI seems like magic where it seems to do everything perfectly the first time until it doesn't. So my question is what are other people's experiences and if and when the tables turned and you settled on using it only when it makes sense? Has anyone had success using it with an established code base where UI is in one project, classes another and then another for services?",35,21.0
1126,post,cscareerquestions,1o6hgjj,1760453095.0,"Trying to find better tech roles abroad or remote, how do you scale and network effectively?
I‚Äôm a software developer with a few years of experience, and I‚Äôm trying to take the next step in my career. Ideally, I‚Äôd like to work on bigger projects, find remote roles, or even opportunities abroad. I‚Äôve been applying, but I feel like I‚Äôm just scratching the surface of what‚Äôs out there.

I want to connect with the right people, join teams that are scaling interesting products, and find roles where I can grow. But I‚Äôm not sure the best way to network effectively or discover opportunities beyond the usual job boards.

Has anyone successfully found international or remote tech roles while scaling their career? How did you approach networking and finding meaningful projects that actually help you grow? Any advice would be really appreciated.",6,4.0
1127,post,cscareerquestions,1o6f5ua,1760447604.0,"Realistically, what will the job market look like in a couple of years?
Currently the job market is clearly oversaturated with a lot of ""wannabes"" - this is visible on reddit, and my real life experience confirm that as well. I'm a wannabe myself. I graduated in 2022 but worked other jobs in the past 3 years.

When I graduated, I had two problems:

1. I have a history of mental health issues. I felt burned out during university and I thought it was because programming and IT weren't for me. Later I realized that I had depression / anxiety in different types of jobs as well.
2. getting a job was HARD. There was an oversaturation even in 2022. Since then it obviously got even worse. Other fields looked more attractive to me because getting a job was easier in those fields.

Today I was lying on the bed and I was reflecting on my life. I was thinking about my past and my future and then I was thinking: ""What if I start coding again?""

**TLDR: I graduated in CSE 3 years ago, but never worked in it (apart from a short internship). Do you think it makes sense for me to start coding again in my free time? Is it realistic to get a dev job in 1-2 years?**",0,49.0
1128,post,cscareerquestions,1o6dr6o,1760443837.0,"Career vs location
Hello all. I‚Äôm looking for advice on whether I should accept a position. I live in an area. It‚Äôs about 40 rough trip to get to the prospective job each day, totaling approximately 130 miles per month. My car takes only premium gas. After budgeting I would roughly take home 1200 extra per month to spend on necessities. Please note, per MONTH. The job would excel my career for the long run. I would get free training. Should I take this job or should I keep looking. Your feedback is greatly appreciated. ",2,9.0
1129,post,cscareerquestions,1o6cr0y,1760440802.0,"The stack a java developer should know
Hi. I'm having trouble job seeking as a java developer with 7 years of experience due to the technologies that companies require now. I have experience with java and spring, databases (SQL and non-sql), event systems like rabbitmq/Kafka, rest/graphql, docker, kubernetes, maven/gradle. These are most of the things I do on a day to day basis. Throw in testing (junit, mockito, testcontainers) and observability/tracing tools like kibana/datadog/grafana.

But when I apply to positions I am asked all of the above and way more. Most jobs are listed as full stack, so they require experience with angular/react. Then they want cloud experience, which is very vague imo. Do they expect you to set up ec2 instances and manage load balancers? They also want DevOps experience, but that doesn't stop at k8s/docker, throw in some helm, terraform, setup clusters from scratch if possible.

At the end of the day most of these positions seem like 3 or 4 people into 1. They want a backend engineer, a frontend one, a DevOps and sometimes even a tester/IT/infra.

And I know those are wishlists but while applying and interviewing, I actually get asked about all these things and even get denied if I don't have experience with them. Is this the new normal? Am I just not versatile enough? The project I work on does not allow me to have experience with all these other things things, and I want to know if you would expect someone to know all of these when working.

And to specify: I'm not applying to startups where I understand its more expected to be a one man team.",26,12.0
1130,post,cscareerquestions,1o6cjp8,1760440166.0,"Should I pursue this degree? (Informatics to major in cs, swe or ai and data science)
Hello everyone, I‚Äôm having a really hard time choosing my major. I‚Äôm very interested in studying informatics (in my country, you start with informatics and later choose a specialization such as computer science, software engineering, AI and data science). However, with all the rapid changes happening in AI right now, I‚Äôm feeling uncertain about my choice. I‚Äôve always loved creating things and even tried making games before. Ideally, I‚Äôd love to go into software engineering since that‚Äôs what I‚Äôm most passionate about, but, as I mentioned, the rise of AI makes everything feel more uncertain. I‚Äôm not sure what the future will look like, especially since I keep hearing about senior engineers being laid off, which is honestly pretty scary.

  
Also, I live in Belguim

",2,4.0
1131,post,cscareerquestions,1o69mwc,1760429526.0,"Conflicted student looking for some help with college, life
I'm a 19 year old studying CS at UC Berkeley and I'm a bit conflicted on if I should still stay on board or if I should ""jump ship"" before it's too late with respect to the current job market. 

For context, I am not as serious about programming as a lot of people on this board seem to be. I chose CS as a major because I did a lot of game programming stuff with my friends and high school and it seemed like something fun enough to make a career out of. Fast forward 4-5 years, and the situation is so bleak, based on my own experiences, that I am beginning to wonder if I should reconsider my options.

  
My conflict is this: I know that CS at Berkeley is a pretty golden opportunity very few people get, but that opportunity doesn't seem to be paying the favors it has promised. I've applied to about 500 or 600 internships and have had a few technical exams, and 0 interviews with real humans. At the same time, I still enjoy programming and think it's a fun activity, so I wonder if by doing another major, I'd be throwing away an opportunity to take CS courses at a school like Berkeley. So the conflict is do I switch into something else while I still have a chance, or do I grin and bear this hellish job market? If I switch out of CS, my backup plan would be applied math because I enjoy it and am strong in it.

  
My backup plan is that I am considering becoming a GED teacher in the state prison system, which is something I have a bit of volunteer experience doing. If I get a Masters it seems like I would be making about what the going rates for software devs are, about 100K or so, which is very good money in my book. I plan to live with my parents anyway after I finish university to take care of them so money isn't super important to me. I'm personally content not getting a job in tech/software and having it as a hobby, though getting one would be sweet, I just don't know if the numbers look all that great.

  
In short, I enjoy programming, but I can't bring myself to enjoy the theory-dense aspects of the courses I'm taking nor the draconian grind that the average programmer unfortunately has to go through to land even a modest job. I worry I am far too casual programmer to dial myself in for this grind. 

  
If anyone could give me any words of wisdom to help resolve this conflict of mine, it would be greatly appreciated. Thanks in advance.",1,4.0
1132,post,cscareerquestions,1o6852p,1760423640.0,"Which is the best full stack dev course With Certification?
Context:I m in my 2 nd year and have just been grinding DSA and CP and naturally I thought the next step is learning web development so instead on my Miniproject I chose to learn Web dev and submit its certificate 

Now I know about Angela Yu and Colt Steele courses on Udemy as well as Freecodecamp but I just want to know which one provides certification as well as is upto date with the currect technology 

Also if possible can y'all suggest if it's even good to go into web development at this time,if not then should I learn any another technology like:

Blockchain dev

Android/iOS dev

ML

AI Engineering 

Devops

UX/UI developer or anything else 

Basically which step should I choose and what best way to learn it with certification of course?",1,0.0
1133,post,cscareerquestions,1o67qlk,1760422123.0,"Do part time software engineer/web dev jobs exist?
I‚Äôm a senior software engineer and I just keep going through cycles of burnout. I really think I need fewer hours and I‚Äôd be happy to take the pay cut. 

Do part time jobs exist? How do you get into contract work?",0,12.0
1134,post,cscareerquestions,1o6746m,1760419866.0,"in 2025 october, Many companiea encourage devs to use AI like Cursor, Copilot to ship code faster.  . What will be the long-term consequences of this?
More buggy app?

hackers laugh while hacking apps easily?

ppl get rich easier since they can build mvp?

Full stack will be standard requirement? 

Less demanding for specialized like pure BE/FE?

Future seniors dev code like today junior/mid level ?

  


  


  
",0,9.0
1135,post,cscareerquestions,1o66pn4,1760418464.0,"For new grads how well has your CS program prepared you for a job?
For my class I have a quick poll about how well CS programs have prepared new grads for employment, and potential issues. It would really help if just a few people filled it out.  
[https://forms.gle/u7wSYbzAMTkFFA417](https://forms.gle/u7wSYbzAMTkFFA417)",2,5.0
1136,post,cscareerquestions,1o66pip,1760418453.0,"Anyone here that got a CS Degree years ago but never got in the industry? What are you doing now?
Sup yall

I essentially got a CS Degree a while ago but never got a job in tech so I just did other stuff to survive lol and now I'm kinda stuck on the minimum wage grind. Came back to see how things were (I'm tired of my wage) and it seems like the industry went to shit and it's super hard to get in now lol.

Anyone else in a similar position? What are your plans? Are you going to keep trying? What did you do instead? 

Any advice for me or am I just toasted?",493,269.0
1137,post,cscareerquestions,1o66gi3,1760417579.0,"Should I still attend this JP Morgan Super Day or be patient?
TL;DR:

2 YoE
JPMC Super Day in 2 days ($110K base, ~25‚Äì30 min commute). Already rescheduled once. Current job pays $78K, 10‚Äì15 min away, and I might be able to move internally to a real dev team soon. Don‚Äôt want to get blacklisted for canceling twice, but not sure it‚Äôs worth the stress or time. What would you do?

‚Äî‚Äî‚Äî

I‚Äôve got a JP Morgan Chase Super Day coming up in 2 days. It‚Äôs a 3-hour interview I already rescheduled once because of family loss.

Here‚Äôs the situation:

JPMC: $110K base, 20‚Äì30 min commute depending on traffic. I‚Äôve heard it can be pretty bureaucratic and grindy depending on the team.

Current job: $78K base, 10‚Äì15 min commute. I like my coworkers (not my manager), but my team barely does real software development; mostly config tweaks, ETL pipelines, and vendor integrations. There‚Äôs a good chance I could move internally soon to a dev-heavy team that aligns with what I actually want to do.

I‚Äôve been spending my off time building side projects and learning Spring Boot, React/TypeScript, and GCP.

So I‚Äôm split. On one hand, I don‚Äôt want to get blacklisted by JPMC for backing out again. On the other, it feels like I‚Äôd just be jumping into a similar situation with more bureaucracy, longer commute, and less prep time.

Part of me thinks I should just skip it and apply somewhere better later if I need to. But part of me feels like I should go just to stay in good standing in case I lose my job soon.

What would you do? Go through with it for experience, or withdraw and move on?

Thanks y‚Äôall!",0,3.0
1138,post,cscareerquestions,1o62rbl,1760406428.0,"Is anyone here able to hold down a CS job in spite of being on anticholinergic drugs?
Ditto for antipsychotics, calcium-channel blockers like Topamax, etc., and other drugs known to impair memory and spatial thinking.

Also, what does your job entail?",3,0.0
1139,post,cscareerquestions,1o604x5,1760399154.0,"Debating what to do now post graduation
21, only did a couple internships at small companies during college, never got any interviews or OAs for elsewhere. Haven't gotten any interviews or OAs for a couple years now. Graduated may '25 from a no name. I've shown my resume to a lot of people, so there isn't much more I could do there. I'm past it.

As my tech journey's now dead on arrival, is it possible to try entering any other field without more education/connections or should I just off myself before being tied to warehouse jobs my whole life? ",23,31.0
1140,post,cscareerquestions,1o5ziv0,1760397494.0,"Gates AI Fellowship vs SWE Full-Time
Hey everyone, i‚Äôm in the final processes for two positions. I‚Äôd really appreciate any advice or wisdom if it comes down to choosing one 

Context:
- New grad, bachelors cs
- Interested in SWE, agents
- Not looking to go back to school 
- Goal is to pivot into a decent, stable job eventually 

Option 1:

Fellowship (fully remote, not sure about travel): https://www.globalsouthopportunities.com/2025/09/14/fellows-6/  - Nov 2025 Start

Pros: prestigious ig? 4000 applicants - 6-9 fellows. 149-163k salary. Not sure about TC

Cons: 1 year contract, not sure about pipeline but apparently strong network


Option 2: 

SWE FT (Capital One TDP) - Feb 2026 Start

Pros: 
- industry exp, stability, probably chill 
- Good resources for swe growth 
- 150k TC give or take 
- Foot in the door (job marketüíÄ)




",35,30.0
1141,post,cscareerquestions,1o5v7o5,1760387257.0,"Career direction advice
Hey, so I've actually got something I'd like advice on.

I recently accepted an offer to move into a position at a defence company. Today I handed my notice in.

My line manager told me he would double my salary to get me to stay. Then immediately after asked me what number I would need to stay. I told him I would need to think about it and we agreed to talk tomorrow.

I make pretty good money already so, obviously even double is a lot of money. However, I'm not a fan of the owners and feel there is little job security there. But, short term I would gain a lot of money quickly. Without lifestyle changes I could easily save/invest an extra ¬£2000 a month.

On the other side is the defence company and the stability that brings. I get to work on cool stuff and have a solid engineering company on my cv. It's much more of a long term investment, however, my salary remains the same (with some great benefits) but my rent essentially doubles and cost of living also increases because I have to relocate for it.

So, boil it down to big money at a start up that's pretty much run on vibes and feelings or less money at a big corp but a much more solid career track?",6,6.0
1142,post,cscareerquestions,1o5uwpd,1760386590.0,"Advice needed-Offer is significantly lower than posted salary
New grad here, I was offered a contract position at a very tiny startup (that does software contracting for other companies). Job posting was 100-120k annual, albeit it was a full time job posting. I was offered MUCH lower. Maybe contractors‚Äô salaries are lower than full time, but what is the reason for this extreme difference? How do I bring this up in my email?

Edit: I really appreciate all the responses and opinions, although they‚Äôre quite mixed. 

I have a final interview coming up at another company, and if offered a position I‚Äôd start in January. 

Because of this it seems like a no brainer to take the offer, but I feel like I should at least address the elephant in the room, I just don‚Äôt know how. 
",96,93.0
1143,post,cscareerquestions,1o5rh8z,1760379173.0,"Should I accept a dev job that relies heavily on AI?
I‚Äôm a new grad interning remotely as a Front-End Dev at a startup where we use Claude Code for nearly everything to move fast in sprints. The offer is to stay full-time.

Upsides are that I‚Äôd finally earn a salary, stop grinding LeetCode, and end the job hunt.
Downsides are that all coding is AI-generated, so I‚Äôd learn less and risk depending too much on AI instead of building my own skills. I‚Äôd still code side projects (web apps, SaaS, full-stack), but the job could be time-consuming.

Has anyone else taken an AI-heavy dev role? Did it hurt your growth or job prospects later, especially if aiming for big tech?

EDIT: Thanks so much for all the responses! Really appreciate it ü´Ç",52,42.0
1144,post,cscareerquestions,1o5que4,1760377845.0,"For engineers job hunting in AI
Hey all,

I keep seeing posts here about sending hundreds of applications with no response, and yeah, the market‚Äôs rough right now. But after talking to a lot of engineers, I‚Äôve noticed that the people¬†still¬†landing solid roles aren‚Äôt necessarily the most qualified, they‚Äôre just being more strategic.

A few patterns I‚Äôve seen working:

* **Warm introductions > cold apps.**¬†Even one shared connection or referral can 10x your chances. If you don‚Äôt have one, find people hiring for your target companies and comment on their public posts (LinkedIn, X, etc.).
* **Targeted portfolios.**¬†Instead of dumping every project you‚Äôve done, highlight 1‚Äì2 that¬†look like¬†what the company builds. Tailor your repo readme or demo to show that alignment.
* **Niche job platforms.**¬†A lot of smaller or AI-focused startups don‚Äôt post on Indeed anymore. They recruit from curated talent networks or specialized communities instead. Worth finding a few that fit your skillset.
* **Show momentum.**¬†Posting your progress (projects, experiments, learnings) publicly is underrated. It signals consistency, something hiring teams actually care about more than perfect resumes.

If you‚Äôve been trying for a while with little traction, maybe skip the ‚Äúapply everywhere‚Äù strategy and focus on visibility + community instead.

For those who¬†did¬†land something recently, what worked best for you?",0,0.0
1145,post,cscareerquestions,1o5p630,1760374289.0,"Should it still worth pursuing tech in 2025?
To give context: I graduated from a top CS school in the west coast with a Bachelors in Human Computer Interaction in 2019, where I learned tech skills (software development, dsa, web dev, UX design, product management.) Now I am super rusty in terms of coding, pretty much need to relearn a lot of it. Since then I have pursued other paths across the film industry and marketing agencies and sales jobs instead for the past couple years. 

Lately with the rise of AI, I‚Äôm inspired again to get back into the tech industry and pursue tech again. I‚Äôm inspired by all the new startups around AI, and I am tired of being the business guy that can‚Äôt build, and want to gain some good technical experience in the tech industry first before I jump into it. I don‚Äôt want to be the guy who vibe codes everything. With all the layoffs that‚Äôs happening, I‚Äôm not sure if it‚Äôs worth it to go back to university to get a Masters in Computer Science. I‚Äôm aware of boot camps and free resources online, but I‚Äôm skeptical if they‚Äôre enough to land me a job or build a sufficient foundation in my understanding of software development. I‚Äôve strayed so far from the tech industry after graduation I haven‚Äôt had any luck in applying to jobs. I know with all the negative press around tech, it‚Äôs not the dream job it used to be. 

I‚Äôm at a point in life where I need a fresh start and I have all the time and enough saved up for a new start. I want to be good enough at coding where I can be the technical founder to a YC startup, and good enough to be qualified as a software developer. The other option is pursuing UX/UI design. 

Any advice and critique is appreciated, thank you so much. 
",0,18.0
1146,post,cscareerquestions,1o5oxod,1760373797.0,"Python -> C#. What's the best plan of attack?
I have been developing in Python for my entire career (~7 years) and now need to pick up C# due to a job change. What is the best way to do this? I have seen some beginner-to-expert C# courses online that say it's possible to breeze through some modules if you have prior programming experience. Should I try something like that? Is there a more focused way of going about learning a new language?",23,21.0
1147,post,cscareerquestions,1o5ognb,1760372779.0,"Do you think in general that devs should have technical mentor/career coach? like SWEs who have been in the industry for at least 10 years?
I recently found  out Principal SWE at Amazon who quited his job after working there 15-20 years.  
Now he do mentoring, helping SWEs  to climb career ladder and give career and general advices.

So those unexperinced devs can follow the right directions...  

Basically learn from someone's mistake and  their experience.



And many high level IC and manger people do mentoring too

As the title says.",39,22.0
1148,post,cscareerquestions,1o5nesq,1760370507.0,"Regarding intervi*ws in other careers
So what, they just have to go to a couple interviews to get a job? How is society even functioning? No 72h case, 2 IQ and 1 EQ test, followed by a live code test? Don't get me wrong, the testing phase of a recruitment is dreadful, but do they just trust that people aren't making shit up? The more I think about it, the less it makes sense.

  
Also, mods should stop delegating interview questions to the dead daily thread imo",0,2.0
1149,post,cscareerquestions,1o5nerk,1760370505.0,"Did they reject me or just ghost me?
I interviewed with a startup tech company. Their main office is in the US, but they said they‚Äôre planning to open a branch in my country.

I had a technical interview for a Site Reliability Engineer position and passed the first stage. A week later, I got invited to a second interview with the Software Engineering Director, where we talked about the responsibilities and key aspects of the role.

After that, they emailed me saying they would contact me again in September (this was back in July) and also shared a document describing the tools and responsibilities for the SRE position.

But just last week, they emailed me again saying that the opening of their local office has been delayed, and that they‚Äôll reach out to me if the position becomes active again.

So‚Ä¶ does that mean I got rejected, or are they just putting things on hold?",0,7.0
1150,post,cscareerquestions,1o5m6ft,1760367834.0,"Got an offer, weighing between staying and leaving
I got an offer for a mid level developer role (same as my current role) via the only way you can find a new job in this market, aka a referral. Passed their tech interview and I got an offer, but the main hangup is the salary. They will likely not offer me any more than I'm already making. So I'm weighing between my current 2 choices, with their own pros and cons. 

Company A (Current company)

- Currently making $115k. They've been decent about raises so far, so I'd expect to reach 120-125 within a year. 

- Java + spring backend, angular frontend. We're a big company, so we have a lot of structure and systems setup. Docker + kubernetes, deployment pipelines, etc. I prefer this stack since it seems like most companies in my area use java + spring. 

- I like my current team and manager. No conflicts, we get along well, and my manager is a tech guy so he understands what I do and how to support me. 

- Benefits are pretty great, huge 401k match and a lot of PTO. 

- The main downside is it's 5 days in office. Yeah, it sucks. It's a 30 min commute so it's not as bad as some others, but it's not great when we used to be 2 days in office before. This is the reason I'm looking elsewhere. 

Company B (Who sent the offer)

- .NET/C# backend, React frontend, and a lot of SQL (we don't use SQL at company A). Mostly Microsoft based tech stack. 

- Would probably offer me 115k, but unlikely to see raises afterwards. 

- 4 days remote, 1 day in office, and the office is 5 minutes away. This is by far the biggest benefit. 

- The team seems good, but they're super small. They're down to 2 devs, the tech lead and another mid level developer. No indication on if they'll hire more. 

- They're not a startup, but they're midsized. Not as structured as company A, but also nowhere near the same amount of red tape. 


What would you do in my position?",56,32.0
1151,post,cscareerquestions,1o5iuhd,1760359910.0,"Moving from FE to FullStack tips
Hello. I'm a Senior Frontend Engineer. I have a lot of knowledge on the backend - it's something I started off with on my learning journey. But I have almost zero enterprise experience with it. I'm having trouble transitioning to a more FullStack position, let me tell you why.

I keep worrying that I don't have sufficient knowledge on security and stability. I don't want to implement something and have it blow up or be a glaring security hole. I feel like I can't handle that responsibility. I also don't have a lot of opportunity to learn from senior backend people and have them review my work as the company landscape isn't very friendly to that (it's complicated...). Essentially, assuming changing jobs is not an option, do you have any advice on how to go in that direction?",3,5.0
1152,post,cscareerquestions,1o5i8ew,1760358250.0,"Just got my first real tech job, nervous as hell and need advice on how to do well & grow fast
Hi,  
I just got my first real job offer (AI/ML role), and I start in about a week. I should be super happy, but honestly, I‚Äôm mostly nervous/anxious. I keep wondering if I‚Äôm even good enough, if I‚Äôll mess things up, or if I‚Äôll fall behind everyone else. I‚Äôve studied a lot, done projects, and know the fundamentals‚Ä¶ but this is my first time in a proper engineering environment. I want to make sure I start strong, learn fast, and become genuinely valuable, not just ‚Äúthe new guy trying to survive.‚Äù For anyone who‚Äôs been through this transition, what advice would you give to someone starting their first job in tech? What do you wish you knew in your first 3‚Äì6 months? How did you overcome imposter syndrome? What habits helped you upskill quickly and not stagnate? Any red flags or mistakes to avoid early on?

Would love any tips. Technical, mindset, or just real-world things nobody tells you before starting. Thanks in advance to whoever replies. I really want to make the most of this opportunity and build a strong foundation for my career.",1,9.0
1153,post,cscareerquestions,1o5g2dk,1760351379.0,"How do I make a transition from Delphi to Java?
Hi everyone!

I am a Spanish 25yo that studied Web Development (what i liked), but my first job was a Delphi job (Multiplatform Development) that I had to take because of money. It's been almost 2 years with this and I am not tired of the programming language, I'm tired of the place I work. They promised me things that are not going to be real and the schedule is terrible, I can't grow as a professional.

I am here. I have applied to a consulting company, in a project that is also Delphi, but has better conditions (same salary, better schedule, work from home). I am in the last phase. My plan is to get a better schedule to get English certificates (B2 and C1). Also get better in general, like psychologicaly and physically because I am burned out. Not of quantity of work, but of being invisible and not having a life.

I was trying to apply to others technologies like Angular + Spring Boot, but it seems like if I don't have those years in experience it doesn't matter, even if I had studied that. In addition, junior offers are a joke, they reject you instantly because you don't have 3+ years of experience. That's why I am applying to a Delphi job, because it seems impossible to get that Java transition. It is poethic that Java taught me through Delphi, and now I can't apply to Java.

**My questions are:** 

\- **Will it be difficult to change career on the future because I will have all my experience in Delphi?** I've done Express, React Native and Python by myself in that same company but I don't want to do mobile development. Not having experience in Docker and Git bc of old projects will affect also?

\- **Does doing personal projects using Angular + Spring Boot will help me get another job?** I have one project idea but because of the burnout, anxiety or call it whatever I can't program after work.

\- I am applying to that stack because it is popular and future proof. I have found that all languages follow the same pattern (the ones that are similar, like Java, C#, Delphi...). I don't like getting married to any in particular.  **Is it a wrong form of thinking? Should I specialize in one stack? Should I specialize in one area?** I feel I can solve any problem with whatever language you give me, but can't decide which area to work. UI/UX design, front-end, backend, DevOps, DBA... That's why I like fullstack.

Thanks in advance for taking your time reading this. Any comment in whatever area will help!",1,2.0
1154,post,cscareerquestions,1o5fi33,1760349354.0,"Formalising work, redistributing power: Lessons from Mexico‚Äôs outsourcing ban
[https://voxdev.org/topic/labour-markets/formalising-work-redistributing-power-lessons-mexicos-outsourcing-ban](https://voxdev.org/topic/labour-markets/formalising-work-redistributing-power-lessons-mexicos-outsourcing-ban)

Interesting a left-wing populist party regulated and restricted outsourcing in Mexico all the way back in 2019 and continued on this path of wage growth, stronger workers rights and restricted outsourcing

I wonder who else had similar ideas? [https://www.wrtv.com/news/politics/bernie-sanders-to-propose-outsourcing-prevention-act-to-keep-jobs-in-us](https://www.wrtv.com/news/politics/bernie-sanders-to-propose-outsourcing-prevention-act-to-keep-jobs-in-us)",3,0.0
1155,post,cscareerquestions,1o5eaf1,1760344743.0,"Is hiring for roles in tech based on meritocracy?
Hey everyone,

I wanted to ask a genuine question.
is hiring for tech roles, especially for freshers, truly based on skills and meritocracy?

I often hear people say companies hire for mindset more than skills, but in my experience, it feels different. In several interviews, I‚Äôve made it all the way to the final technical round. Once, I even asked a tech lead for feedback and he told me I was technically solid for my experience level and had no negative comments.

Yet, despite that, the final decision was a rejection and when I politely followed up asking for feedback to improve, I got no response at all. What made it more sting that they reposted the Job on linkedin. They would rather start the whole process again with a different candidate than offer it already to someone who did good on the interviews???

I‚Äôm genuinely trying to understand what factors really influence hiring decisions beyond technical performance? And how can someone like me grow or align better with what companies are actually looking for?",3,31.0
1156,post,cscareerquestions,1o5dh79,1760341567.0,"Is a systems engineering job right out of college bad for career trajectory?
I have an interview for a systems engineering job at a defense company and I didn‚Äôt really know what it is was when I applied. looking it up it seems like it is a management position. I like software engineering but have not really been getting interviews. If I get this job would it be harder to get a software engineering job later on?",13,3.0
1157,post,cscareerquestions,1o5bfrn,1760333994.0,"Laid off a couple months ago and struggling to find a new role. (US, 6yoe)
I have really been struggling to find a new position lately. I was recently laid off, my contract ended and they didn‚Äôt need me anymore, from my position as a Django developer. Full stack, before that I had a role as a front end Django developer. Have about 6 years of experience. 

I haven‚Äôt even been getting interviews really. It‚Äôs been tough. 

Sent out hundreds of applications, on job boards, company sites, government sites, etc. I‚Äôve had a couple first round interviews but nothing sticks. 

Is the market just bad right now for people with my experience? Or am I just unlucky, or unskilled? 

Thinking about pivoting out, but that feels pretty bad to have to start over with something else. ",36,18.0
1158,post,cscareerquestions,1o59k1e,1760327725.0,"Advice for applying to jobs after medical LoA
Looking for some advice as I feel my situation is a bit unique. Going to be a little vague to avoid identification

Basically, I'm a little over 5 years into my career and have been working at the same company that whole time. After 4 years in team A (including a promotion), I transferred to team B. Almost immediately after, I needed to take a leave of absence for about a year due to some medical issues. Upon returning to work a few months ago, I was moved to team C.

My question is how I should bring this up during interviews? Ideally I feel like this is the sort of thing that shouldn't be brought up at all, as I don't technically have a gap in my resume and I could see there being concerns about whether I'm fully recovered or will need additional leave in the near future. However, given the recency of the leave, I'm worried I'll get to a later stage in the interview and be asked about projects I've been working on, and it will be pretty much impossible to discuss that without being honest about the situation (without going into specifics about the medical issues, of course). In that case, I don't want it to come across as if I was intentionally withholding information earlier in the process.

Regardless of how you feel about lying in an interview ethically speaking, I'm also a terrible liar in general and probably wouldn't be able to successfully do so even if I wanted to (for example, just saying I moved straight from team A -> C a few months ago). So I'm moreso interested in ways I can avoid the topic being brought up, or at the very least ways I can frame it to not seem like a yellow/red flag to interviewers.

Any input here is very much appreciated!",2,0.0
1159,post,cscareerquestions,1o57u4w,1760322586.0,"How to escape underemployment if I do end up underemployed? (Thinking about the future)
For context, I attend a T50 school in the US in my locality, in a major East Coast metro area, and am double-majoring in CS and DS. Some roles I've been applying to include the usual Software Engineer / Developer roles of all stacks, Data Scientist, Data Analyst and the few database-specific roles that pop up, and even QA, Business Analyst, and various IT roles (that I'm probably woefully unqualified for anyways since I have 0 IT experience).

Let's say the absolute worst happens, and no company hires me for any role between now and my graduation this coming May. In that case, I'd be forced to either become a NEET (and maybe even officially register for some form of unemployment), or (slightly less undesirably) end up in some retail or service job - something that doesn't require a CS degree - just to have *some* work.

**What's the likelihood I'd end up in this situation? And if I do, what's the likelihood I'd ever be able to escape?**

Now obviously, that's a pretty terrible fate to end up in long-term. So I think I'd need some form of ""game plan"". I've already worked some of these before as a student, and some of my older coworkers there have been ""failed"" students in non-CS STEM fields.

And since I wouldn't want to work there forever, I'd likely still be applying to ""real"" jobs on the side, and maybe even landing a few interviews if I'm fortunate, but things might not improve, and could even worsen. This current ""employer's market"" might last for a while (I heard for civil engineering it took nearly an entire decade), and unfortunately, it's possible my skills and degree could risk atrophying in the meantime. And this could kill my motivation to do LeetCode / side projects, etc.

And what the hell are you even supposed to tell the hiring team if you do get an interview for a tech position? ""I couldn't find work out of college so I had to work at the local grocery store / restaurant""? How are you going to convince the hiring managers to consider you over some other cracked junior who has *not* needed to resort to menial labor in order to make ends meet or prevent a career gap?

At what point should I simply admit defeat? At what point do I seriously consider reskilling into non-tech roles? (I'm already having trouble with even ""adjacent"" roles like DA and BA.) Which non-tech roles, even? I don't think I'd be able to break into law, medicine, nursing, or most trades, and even if I could, I don't think I'd have the requisite interest.

For the sake of discussion, my definition of ""winning"" would be to have enough money to move out of my parents' house in the suburbs and rent an apartment somewhere major enough for me to have a satisfactory social and romantic life. Doesn't have to be 6 figs, FAANG, or even a SWE role at all. Don't even have to actually do it, just have to *make enough money to do it*, and if the job is really local I could just spend ~1-2 extra years at home and save the earnings to be frugal.

You cannot do this by stocking shelves or flipping burgers for $15/hour. And if I'm forced to care for ailing parents on that salary while their home - the home I grew up in - goes to rot, then oh boy, things are *not* going to be pretty.",14,13.0
1160,comment,cscareerquestions,hnnyaxb,1638926750.0,"Dude, standing around browsing Reddit in a fulfillment center laughing out loud getting double overtime. Thanks for the laugh!",1,
1161,comment,cscareerquestions,lp2zn41,1727384386.0,"If it helps, I stared the DotCom bomb right in the face and enrolled in CS anyway.  You have to make a judgement on whether you think programming need is going to increase, remain constant, or decline.  Objectively, it's a bit harder to predict with the breakthroughs from LLMs, though I think the consensus is pretty clear that ChatGPT is not coming for your jobs any time soon.  The supreme irony is that it's the ultimate productivity tool for those of us with tons of experience because we know when it's full of shit and we know how to ask it to prove its work.  I strongly suspect programmers will be one of the last jobs replaced by AI.

All of us making good money in good programming jobs are necessarily the same batch of people that stuck with it through the DotCom bomb and the Great Recession.  So many people quit or disengaged.  We didn't.  That made our skillsets rare and that's who is getting hired instead of new grads.  A new grad is probably, on average, a liability to a new company mostly because CS does not make programmers in a lot of college programs, it makes Computer Scientists.  That's changed a bit since I graduated, but it's still slow.

There will (hopefully) be a 20 years from now and that's how you should plan.

My advice has always been consistent with respect to programming, you have to genuinely love to do it because the human/business side of it makes it a lot less fun.  If you're doing it because it's a well paying career you are going to burn out.",13,
1162,comment,cscareerquestions,lp6ersv,1727441777.0,"Yeah I‚Äôm not subbed to this feed but every single post that gets recommended here is pretty much all doom and gloom with the sentiment that the market is never going to recover. 

How about some actual advice like doing projects for local businesses/non profits. It doesn‚Äôt even have to be anything complicated, just enough to show that your work can make an impact. I wrote a simple that turns on/off the parking lot lights of a local nonprofit based on an  google calendar (auto populated by the script and can be edited). This way, they don‚Äôt have to constantly make changes cause the schedule is very dynamic (changes every 15 days). You could also make some basic contributions to an open source project. 

I‚Äôve found chatgpt is great for making my resume more professional and recruiter friendly. Best way to fight bots filtering out your resume is to use a bot.

Finally, referrals are and have always been a major key in terms of getting interviews. Find ways to get them. Whether that‚Äôs going on LinkedIn and asking people for one or friends.

This all isn‚Äôt going to automagically get you a job, but it‚Äôs a good start to help you stand out.",2,
1163,comment,cscareerquestions,lp49u93,1727400969.0,"I don't see AI replacing most coding jobs, even in the next 20 years.

That's not because I'm a luddite it's because --- well we have entire billion dollar companies with ""brainless management"" who can't software architecture their way out of a paper bag. And it requires a lot of deep strategic thinking.

That kind of shit is WAAAAAAAAY far off from ChatGPT who doesn't even know how many ""r""s are in the word strawberry.

LLM is a crowd-sourced, crowd-pleasing bot, not a truth-bot. There's a big difference. .... It's like a thing designed to maximize Reddit upvotes. So, it will learn to sound cocksure, authoritative, and confident. But to be right? Well ... how the fuck will it know it's right?",8,
1164,comment,cscareerquestions,lp3h8kd,1727390377.0,"Yeah well accounting and civil are some of the only majors with good job markets right now. Anyone who is entering college for accounting right now though will probably be too late to take advantage of that though. I imagine they'll find the supply for that well to dry up a little bit between AI, outsourcing, and increased enrollment because it is a good field right now.

I don't think civil is really going to saturate for the foreseeable future, but the ceiling for civil is usually kind of low, so that's probably why.",13,
1165,comment,cscareerquestions,locrqa1,1727009343.0,"So there is a former central intelligence guy on YouTube I follow Andrew Bustemante, when he left CIA this is exactly what he did straight bs'd his tech knowledge and background and then learned on the job as he went. I feel like chatgpt o1 makes this even easier as it can give very accurate guidance when you get stuck on something. I've been using to increase my splunk skills and have created some amazing queries and dashboards.",1,
1166,comment,cscareerquestions,lo9oxdh,1726953462.0,"I built a react/spring boot app that had like a couple internal users at one of my jobs where I was a systems analyst. I just slapped on a full stack java dev on my resume and got two software engineer jobs really easily, one of them they hired me straight into being a senior.

The people I worked with who were supposedly java software engineers with 5-10 years of experience barely knew anything. At one job, the tech lead would painfully review every PR for mind numbing details like making sure javadocs were on every single thing, naming conventions were followed to a T, and his favorite, that the file ended with a newline character (which text editors often removed). It ended up I was spending a lot of time helping people just getting their code merged and implementing features, because he was totally absent whenever it came to actually doing anything, he was spending all his time with the best developers making a spring boot wrapper that didn't really do anything just made it harder to know how actually code anything because instead of just looking up spring boot documentation, you had this barely documented junk framework.

All this to say, at most companies, it's just not that hard to do stuff. Now with chatGPT, if you are just pumping out cookie-cutter react/spring boot code, it's very easy as long as someone has a basic understanding of programming",123,
1167,comment,cscareerquestions,lo9qgmn,1726954014.0,"Lying on resumes has been the norm for years I think.

I've gotten a job before just cause my resume looked real and I passed a 30 minute vibe check interview.

When I got laid off from that, my coworker told me that I should curate a false resume to perfectly fit each job description. Now ChatGPT can do that for you in seconds...

I was already cynical about this before I started screening resumes from the hiring side. It was worse than I imagined. Everyone now has 10 years of experience with every technology, works on AI for Google, but is interested in interviewing for our underpaid temp work???

A lot of people also deleted LinkedIn or stopped putting their experience on there, so they don't have to worry about keeping a consistent narrative between fake resumes.

We ended up hiring the people who spoke some English and didn't lie about what city they lived in. We needed the fake resumes cause our policies required a minimum YoE.

There was one person we wanted to hire, but she was having issues trying to balance talking to us and listening to whoever was feeding her answers to the interview questions. She blamed the ""audio issues"" on ""rain"" after telling us she was living in Los Angeles.",60,
1168,comment,cscareerquestions,lobtnzq,1726988119.0,"That sounds like a fake story to me, seems like it was written by an llm, also not sure how having the skills that are trendy now and everybody more or less have can result in super high salary, I would expect the opposite",2,
1169,comment,cscareerquestions,lqt6wnq,1728323421.0,Fulfillment center. Where they send packages off and all that stuff,43,
1170,comment,cscareerquestions,lqt6y6k,1728323434.0,FC = Fulfillment centre = Warehouse = wage cage,20,
1171,comment,cscareerquestions,lxhxwbo,1731793883.0,"Watch out, there might be a chatgpt response on how to build a scalable streaming service coming your way!",2,
1172,comment,cscareerquestions,lxg7ddz,1731773641.0,All those engineers had to do was ask chatgpt! Ezpz,21,
1173,comment,cscareerquestions,lcsdzwy,1720758905.0,Bro tryna train us like chatgpt,1,
1174,comment,cscareerquestions,n9qlj9u,1755707777.0,Junior devs are accountable for their work. LLMs are not.,52,
1175,comment,cscareerquestions,n9sgymf,1755727606.0,"You cannot easily run computerized systems validation on AI tools and training them properly is going to require a shitload of labor.

Companies with serious use cases will have to solve this. It means narrowing down use cases and investing. This will mean making choices.

The bubble will burst but some winners will remain.",2,
1176,comment,cscareerquestions,n9rz2im,1755722097.0,LLM have some genuine worthwhile use cases but they‚Äôre not replacing software engineers,1,
1177,comment,cscareerquestions,n9w3v9v,1755783418.0,"They expect generative AI to be able to eventually remove the need for institutional technical knowledge. Maybe not currently, but eventually.",1,
1178,comment,cscareerquestions,ncq1qh5,1757162308.0,"AI is constantly being used as a buzzword. The correct word is automation, and automation products, and services have been available before ChatGPT and similar became available. What does replacing an employee with AI mean? What‚Äôs the AI they‚Äôre using? They‚Äôre making general assumptions about AI that have come from science fiction, and sound like the /r/futurism subreddit.",1,
1179,comment,cscareerquestions,n9roxrz,1755719128.0,"I think current AI is more powerful than a ""faster google search"".

It is very powerful and speeds up many tasks.  The other day I wanted to make a pie chart that broke down a column of numbers in a spreadsheet by occurrence number of each entry.

To do this by hand, Google said I needed to use a pivot table.  I've never done that.

Instead, I just sent the column of numbers to ChatGPT and asked for the pie chart I wanted.  It did it in seconds.

I do think that the current AI is overhyped, and it's not powerful enough to replace anyone yet.  It's a useful tool, like a spreadsheet, but, like a spreadsheet, it can't actually do anything by itself.

I think most of the layoffs in tech are due to the tax benefit of R&D employees going away, and a cutting back of all the free-for-all hiring that went on the past few years to try and scarf up all the talent.",1,
1180,comment,cscareerquestions,n9satn8,1755725650.0,LLMs are not being pushed by anyone thinking about 10 years in the future. What happens after most of the content being used for training is itself AI-generated?,5,
1181,comment,cscareerquestions,n9rqib6,1755719590.0,"Middle management is probably the place I think AI will hit hardest.

The entire job is to intake internal information communications and reports and distill it for those up the chain and pass tasks back down.  

The worst LLM I have ever used would do a better job organizing a team as a neutral voice in a team group chat than any ego driven middle manager I have ever met.",9,
1182,comment,cscareerquestions,n9vav6h,1755772040.0,"aditionally, what happens after all juniors are fired and after x years LLMs shows unusable, and seniors starts to retire?",2,
1183,comment,cscareerquestions,n9s69j2,1755724242.0,"That model doesn‚Äôt translate to AI though. These companies don‚Äôt have a real moat. The models are interchangeable. If OpenAI raises prices, people will just switch to Claude, Gemini, or Deepseek. And the majority of users won‚Äôt ever pay at all. Unlike Amazon or Netflix, there‚Äôs no unique content or ecosystem keeping people locked in. LLMs are basically commodities: whoever is cheaper (or free) wins. And the kicker? The insane cost of training and inference is not going down. GTP 5 costs more per query than 4. They are fucked.",1,
1184,comment,cscareerquestions,n9qqw5a,1755709297.0,"Sure, but the problem is that these LLM‚Äôs are far more expensive to build and operate. Start ups can run pretty lean and reinvest any income to keep expanding. OpenAI and the like are instead funneling huge amounts of cash into increasingly smaller improvements while hoping that profitable use cases become viable.",3,
1185,comment,cscareerquestions,n9tsddv,1755743989.0,"I also work at a large software company with proprietary technology, and ChatGPT is pretty much limited to suggesting variable names and copying existing files.

It is very little understanding/helpfulness if I need to implement something new.",0,
1186,comment,cscareerquestions,n9ruyu0,1755720899.0,"Also there was an established demand for, you know, taxis and videos. They had real pain points they could address. LLMs are still largely a solution in search of a problem (well, a problem of the scale that will justify the investment)",2,
1187,comment,cscareerquestions,n9qv7re,1755710512.0,"They are aiming for the stars, it might be impossible, but reaching the moon will still be incredibly profitable.

Tesla was promising full self driving by 2017, they still don‚Äôt have it a decade later, but what they‚Äôre a profitable company selling millions of vehicles.

As is, current LLMs do offer significantly value already. They might need to increase prices, decrease investment for improvement, and maybe lower resources for simple tasks, like they did with GPT5, but current LLMs are actually very helpful already.

And that‚Äôs just part of AI, machine learning in general and things like computer vision are here to stay.",1,
1188,comment,cscareerquestions,n9szgz8,1755733852.0,"There's a million different internal AI tools with more every single day, hell there's multiple different internal websites just to track launches of these tools. Some suck, some don't, so someone's experience with ""AI tooling"" can vary extremely significantly based on which one they chose and their usecase. For what its worth, I actually like the Q developer CLI (not the one integrated into VSCode that one is ass)",4,
1189,comment,cscareerquestions,n9qit2c,1755707004.0,"I appreciate that CEOs do a lot that you wouldn't want to entrust to an LLM, but there are some CEOs who really are no better than an LLM already. Looking at you, Muskrat",23,
1190,comment,cscareerquestions,n9s9bo5,1755725179.0,"So you think of the midlevel managers as the Junior Officers like Captains and Lieutenants? Those kinds of jobs in a company are even less likely to be replaced by LLMs IMO. Atleast at the companies I've worked at, the buck stops at them and there's no way we can give that kind of responsibility to a program. I can't possible imagine firing Project Leads and using any tool as a replacement. The project would quickly fail or require a person to actually lead it.

FWIW, I would consider NCO's and Junior Officers to be midlevel for corporations. With Senior Officers and Flag Officers being upper management. The military analogy doesn't perfectly fit which makes sense.",2,
1191,comment,cscareerquestions,n9s872x,1755724830.0,"GPT5 costs more per query than 4. Inference and training costs have not reduced. There is no path to profitability. 

When you say they are aiming for the stars, do you mean the promise of AGI? Because no LLM tech will lead to AGI. We have already reached peak LLM, which is why the massively delayed GTP5 has not offered the gains we saw in the past.

The unsolvable issue of making shit up means LLMs utility is limited. It can be useful, but at no level to justify the hype spend we have seen. Why? Because of venture capital.

VC looks at valuations and growth, not necessarily at profit or revenue. So you don‚Äôt actually have to invest in technology that works, or that even makes a profit, you simply have to have a narrative that is compelling enough to float those valuations. So you see this repetitive and exhausting hype cycle as a feature in this industry.¬†  
[https://www.politico.com/newsletters/digital-future-daily/2023/12/01/5-questions-for-meredith-whittaker-00129677](https://www.politico.com/newsletters/digital-future-daily/2023/12/01/5-questions-for-meredith-whittaker-00129677)",2,
1192,comment,cscareerquestions,n4w3kk6,1753361405.0,"I think he‚Äôs mostly just talking. He‚Äôs hated tech companies for a long time and accused them of being liberal. I‚Äôd love for there to be more protections/incentives from the government.¬†

From the article:

> ""Another major order signed by Trump is aimed at companies that get federal funding to develop AI. These companies will now be required to build AI tools that are politically neutral. Trump made it clear that his government does not support what he called ""woke"" AI models. He accused the previous administration of promoting diversity and inclusion policies that, according to him, slowed down AI progress.‚Äù

Basically, they hope all AI is as biased, artificially influenced, and racist as Grok.¬†",116,
1193,comment,cscareerquestions,n4wr8sq,1753368632.0,"Last line of the article:

>While these changes may not have an immediate impact, they hint at a future where Indian IT professionals and outsourcing firms could face more hurdles **if Trump returns to power**

India Today not hiring the best and brightest. Almost like this article was written with an LLM last updated before November 2024‚Ä¶",6,
1194,comment,cscareerquestions,n4w760b,1753362584.0,"I don't think they care what the President Sex Pest tiny hands is saying.

  
""Trump expressed dislike for the term ""artificial intelligence"" itself, saying he preferred a name that better shows the intelligence and power of the technology. ‚ÄúIt‚Äôs not artificial, it‚Äôs genius,‚Äù he said.""

  
Why just why??",4,
1195,comment,cscareerquestions,n4yo7rc,1753387684.0,">In addition, Trump expressed dislike for the term ""artificial intelligence"" itself, saying he preferred a name that better shows the intelligence and power of the technology. ‚ÄúIt‚Äôs not artificial, it‚Äôs genius,‚Äù he said.

I'm frustrated every day by the stupidity of the LLM tools I use that are so confidently wrong so much of the time. I guess at a certain level of intelligence, it seems like ""genius"".",1,
1196,comment,cscareerquestions,n6mifd5,1754185723.0,It's litterally an Indian news paper. They litterally used ChatGPT which was trained from before Trump was in Power. Which speaks volumes abt Indian offshoring if this is how they do their work,3,
1197,comment,cscareerquestions,n4vvbbz,1753358415.0,"As the bar rises due to AI tools, bootcamps wont get it done. You need to be able to more than build a react app in 2025",2,
1198,comment,cscareerquestions,gg7zy8r,1608267822.0,"I fee like this will be me in a few years. I thin you did the right thing for you. If you love coding and building things, you could start an education youtube channel. Who knows? Best of luck and I hope you find fulfillment in your new chapter.",2,
1199,comment,cscareerquestions,moros5t,1745493007.0,"Look for Medicare Advantage companies, Aetna, Blue Cross Blue Shield, Anthem, etc. There are a ton of region specific companies that seem to always be hiring. 

I'd recommend finding the job opening on job boards then apply directly on their website. Once you apply reach out to recruiters (on LinkedIn) or talent acquisition that works there for some extra visibility.

As we get closer to Annual Enrollment Period (AEP), Oct - Dec, these companies tend to increase hiring so now through summer is a great time to apply.",3,
1200,comment,cscareerquestions,moa5ayp,1745254257.0,"all im gonna say is‚Ä¶ as a swe, best to start specializing in security engineering + cybersecurity + AI/ML with the way that common people trust generative AI so much",2,
1201,comment,cscareerquestions,moa5x72,1745254437.0,"You can trust it all you want, it doesn't simply produce the result you want it to. As of right now if I plug a ticket into chatgpt and give it the necessary context, it just doesn't work at all 95% of the time.",3,
1202,comment,cscareerquestions,lr1k5l6,1728445504.0,Staggering ineptitude is a hallmark of people who force others into the office lol,10,
1203,comment,cscareerquestions,l4vi4c7,1716213034.0,"I am always seeing parroted advice on here to the affect of ""just stop working overtime, stop checking email after work, set boundaries for yourself!"". Which is good advice, and it would be great if it could be applied without worry, but it is being said (usually) by idealistic people who have never actually worked a cs job. The unfortunate consequence for setting those boundaries can be termination (such as in a case like this where the expectation is clearly to not have those boundaries and the existence of those boundaries will be immediately noticeable to everyone involved), something that is not usually mentioned by the commenters. Or if it is, they imply that finding another job is no big deal.

Willful ignorance of the realities of a situation is a hallmark of Reddit's advice, and why you should never ever listen to it.",16,
1204,comment,cscareerquestions,l5c3mls,1716478324.0,"""Willful ignorance of the realities of a situation is a hallmark of Reddit's advice, and why you should never ever listen to it.""

Truth",3,
1205,comment,cscareerquestions,f6axe7n,1572764194.0,"First, let‚Äôs just acknowledge that you come from a different time when, frankly, getting a job was much easier. And I‚Äôm not just talking about tech. My dad, a physician, got his first residency position by literally walking into a hospital and asking to see the head of the dept he was interested in. Last time I visited the valley, I had to explain to him that I couldn‚Äôt just walk into FB office and do the same. In order to get a job in the current market, you do Leetcode. I‚Äôm very passionate about tech. I‚Äôm not passionate about leetcode. Telling me to follow my passion means doing things like this that are dry and grueling. Leetcode barely translates into the work software engineers *actually* do. I guess what I‚Äôm saying is: don‚Äôt hate the players, hate the game.

Also there‚Äôs nothing wrong with chasing money. My parents came from a third world country and poverty is a scary thing. I will do everything in my power to avoid it just like they did. If I don‚Äôt find fulfillment in my work, like you say, then at the very least I could make money to enable my passions outside of work. Because work isn‚Äôt life.",2253,
1206,comment,cscareerquestions,n9194la,1755362965.0,If I wanted to read chatgpt I'll ask chatgpt...,10,
1207,comment,cscareerquestions,n91xsbt,1755370530.0,I know with 100% certainty this post was written by ChatGPT. Here‚Äôs the kicker ‚Äî it‚Äôs obvious.,7,
1208,comment,cscareerquestions,n99z5lm,1755480779.0,"Thanks, ChatGPT!",2,
1209,comment,cscareerquestions,n9lfn4m,1755635884.0,This was definitely ChatGPT,1,
1210,comment,cscareerquestions,ncneomj,1757117411.0,Thanks ChatGPT,1,
1211,comment,cscareerquestions,n9af3g8,1755487116.0,"Miles better! No need to hide yourself behind the LLM. It was also more interesting to read. 

As soon as my brain realizes it's reading AI generated content I just skim the content or stop reading entirely. I'm sure your brain does the same",1,
1212,comment,cscareerquestions,ltdgkbj,1729704424.0,"> using a coding challenge to do your work for you

Yeah, this doesn‚Äôt happen. Someone coming in off the street is not going to write production-ready code for a real code base in 30 minutes. It takes time to set up your local environment. It takes time to learn the code base and how to build it. Deployment takes time. Testing takes time. Code reviews take time. 

Anyone who thinks we‚Äôre getting meaningful, production-ready code in 30 minutes from someone who‚Äôs never seen the code base before is delusional. Such a person is not a good fit for our organization. 

And suppose this is possible. That means if we catch you cheating by using ChatGPT to do our real work, then we‚Äôre telling on ourselves by admitting that we can be replaced by ChatGPT.",65,
1213,comment,cscareerquestions,ltdqm95,1729707449.0,"Okay, but I'm not doing any of that. I don't think my company is, either. If you want to cheat your way into one of the companies that's creating fake jobs, I hope you're very happy with them.

Yes, it sucks that we all rely on companies for healthcare. Maybe we should do something about that, but until we get universal healthcare, there's someone else who needs it just as much, but can also understand how to glue a couple hashmaps together without asking ChatGPT to do it for them.",10,
1214,comment,cscareerquestions,lth6hx3,1729754408.0,"can you write bad reviews for them so future applicants don't go through this BS again? i think its a feature on glassdoor right? i mean if they get a couple thousand interactions per month, something like three bad reviews and their ranking in whatever platform's recommendation system start to drop a lot right?

the one problem with this approach is fake companies generated by chatgpt. but i would imagine this to be hard?

the other problem is when platforms do some punishments to you, who rated others badly. also if platform has your identity or you revealed them it might be bad...",2,
1215,comment,cscareerquestions,ltjm80x,1729790749.0,"I feel like leetcode is ok in that it‚Äôs testing your basic skills like do you know what this for loop is doing and such. But asking leetcode medium or hard is just outrageous. At one point in your job (when you do get it) and you face a problem that is easy but you don‚Äôt know how to do it, your going to wish that you atleast did some leetcode. Not saying it should be required but some people should understand the basics of coding. There are some people out there that have jobs but have no idea on how to code and use chatgpt to solve and wonder why it‚Äôs still not working",1,
1216,comment,cscareerquestions,ltdr9ze,1729707650.0,"In your real job, you encounter problems that are unique. Meanwhile, any interview question from any company that conducts enough interviews, Leetcode or otherwise, is going to be leaked online and be used as training data for LLMs.

No matter what sort of interview you choose, it's never going to resemble real life work, especially once a ""meta"" evolves around that style of interview. IMO, people who hate Leetcode do so because they're bad at that meta. If another style of interview becomes more popular, you'll see just as many other people hate on that style if not more.",0,
1217,comment,cscareerquestions,lten1tb,1729717335.0,The people who would do this aren't concerned with any of that. But I would guess that nowadays they've replaced that with ChatGPT.,1,
1218,comment,cscareerquestions,ltd8pzu,1729702064.0,"No, by every definition that‚Äôs not cheating.  You may want to look at a dictionary again.

Using AI/tools on tests that don‚Äôt allow it is cheating.  By definition.

I do believe that if you need AI tools to get the job, you don‚Äôt deserve the job.  If you cannot be competitive without AI, you‚Äôre just not competitive.

A great dev with AI is still a great dev, but a bad dev only using AI is literally worse than not having a dev at all.",-1,
1219,comment,cscareerquestions,lte8ndf,1729713078.0,"I mean that‚Äôs the hallmark of an abusive relationship. 

Person 1: treat me better please, I would like these things.

Person 2: how dare you I treat you fine, don‚Äôt you see that? Now I‚Äôm going to treat you worse so you realize how bad it could be.",6,
1220,comment,cscareerquestions,n3k70zs,1752719191.0,">Maybe not in years, but in months.    

We've been hearing this for at least 3 years now.  

Very small, well defined tasks is something that AI tools do well.  The more complex or esoteric the task, the worse these tools do.  
Every couple days someone shows an example of these tools doing something trivial and declaring this career dead.  I've worked with Claude enough at this point to know we are no where near ""months away"" from any large upset - except for maybe companies with most gullible of leaders.",178,
1221,comment,cscareerquestions,n3k75ff,1752719237.0,"I mean it sounds like it just took in very easy rote grunt work, with the only advantage over you or any other eng being time. However, these agents cost way more than a person and that's becoming more and more obvious by the day. I'm worried about how this affects juniors don't get me wrong but as someone that knows the types of tasks LLMs suck at we are all safe",28,
1222,comment,cscareerquestions,n3m18u0,1752751498.0,"People here are looking at this from the wrong point of view. Managers will see results like this and assume it‚Äôs possible for every ticket. They‚Äôll hire less. And they‚Äôll pressure the current devs to work faster because they can use AI to finish in minutes what used to take hours. Any dev who tries to argue otherwise will be labeled a Luddite and be on the chopping block. As a result, the codebase will get messier and more vulnerable. 

This is happening now and it could be a while before managers realize their mistake. And in the meantime AI tools will only get better. 

OP is right to be scared",4,
1223,comment,cscareerquestions,n3mnkh9,1752759401.0,"I feel you dude. I was a software developer/data science engineer in a highly specialized scientific domain (pharmacometrics). For the past year though I have done nothing but train models, build AI agents, and begin designing more complex pipelines involving multiple agents.

I see how these things are improving at the LLM level, agent level, and then full pipeline level. Some days I feel ok, but I‚Äôve had two existential crisis in the past week over this. In 2ish years these things will be insane, and it‚Äôll only get worse from there. 

Regulation will delay some sectors, but once the technology can fully automate most intellectual jobs, they‚Äôre going to push for the transition with incredible momentum.

Even if there are delays, new hiring is already beginning to halt. No idea what the next few graduating classes are going to do, and what that‚Äôs going to do to the economy, but I don‚Äôt have a good feeling.

Yeah, I‚Äôm terrified.",2,
1224,comment,cscareerquestions,n3ncmq7,1752766567.0,"My LLMs can't write simple unit tests, or even update existing unit tests. They can't handle enums at all, period. They always hallucinate enumerations that don't exist based on the surrounding code, the imagine what they think the enumeration names *should be*, instead of looking up the enum definitions to read actual enumeration names. And the last time I asked Claude to update a unit test, ""here's the command to run the test, keep going until it passes"", it literally deleted the lines of code that set the test conditions so it was impossible to fail and declared it was done. LLMs aren't replacing me anytime soon.

And more importantly, I have never once received a complete or perfectly accurate JIRA ticket.",2,
1225,comment,cscareerquestions,n3kb5wm,1752720757.0,"I'm curious how it found the ""where"": the repo, the exact area, etc.

And in any/all cases, you will need an engineer to review the PR for the next 10+ years regardless of how fast it's happening.

LLMs are not at all good at solving complex bugs and generally struggle with production-ready code.  Ask an LLM how accurate it is at producing production ready code on small to medium sized tasks.  It's very low.

I would ask that you earnestly report back when it hallucinates on a PR of heftier complexity.  I'd guess that the percent of tasks the Agent is suited for tackling is probably less than 5% of the total tasks that your business will find useful.  I don't see that changing meaningfully over the next few years because complexity is geometric with bugs/issues/performance improvements/features.

There are still decades worth of rewrites of extremely crufty systems that are holding up our entire technical infrastructure and the majority of our Fortune 500 companies.  LLMs will not be better than humans at those wholesale rewrites and migrations, for some time.",3,
1226,comment,cscareerquestions,n3lb796,1752737742.0,"Yup, I'm in infra and we've been using windsurf. (Crazy things happening with the AI Talent Wars btw LOL)

At one point my team members and I were thinking... ""Did we just create our replacement?""

Case in point, one of our tools takes dev team requests in infrastructure and creates a cloud resource with using AI for input/data validation. Basically Claude Sonnet(our LLM of choice) scoured through how our platform was coded and created a workflow that would be not only efficient for us, the infra devops/sre team, but also our app/dev teams.

Two PRs are created(our platform is set up this way for resource creation) and approving them is really the only manual part in all this.

I'm still optimistic(at least about ops stuff) that AI can't replace human creativity until super positioning bits hardware can catch up to the software potentially leading to sentient general AI. 

Yup I think that's when I'll be more concerned.

ps. The general time saved in efficiency from using code crawlers and agents and whatnot is about an 80% time reduction across the board for all teams. But the work doesn't stop, there's always something more/new to do and all these new tools have really done for us is given us the bandwidth to do MORE and COOLER things, so yeah, optimistic still.

It feels like we've suddenly been given electric bicycles when we were originally walking everywhere.(analogically speaking)",1,
1227,comment,cscareerquestions,n3lba2k,1752737786.0,"The currently known limitation for this approach is the context window size. No LLM right now can just ""read and understand"" your whole codebase at once. Even with RAGS.

How small was the project?",1,
1228,comment,cscareerquestions,n3lmdb0,1752744129.0,"And? They can handle simple tasks, and when times come and, ""when"" they fucked up your codebase, who will take responsibility? You? Or the Senior?

This is not even included how LLM AI are lacking in security aspects and many white/grey hats are thriving on it.",1,
1229,comment,cscareerquestions,n3lrpb3,1752747037.0,"I'm not all too worried. I've had AI (ChatGPT) help me with a bit of code references / lookup and when I get code samples, sometimes there are critical pieces of code which are missing. You wouldn't know they're missing unless you know to look for them. I think AI is great as a coding assistant, but it's not going to be a replacement for human expertise. AI can write code, but it has to be owned by a human. And if the human is just vibe coding, they're gonna get in trouble.",1,
1230,comment,cscareerquestions,n3lzw8k,1752750916.0,"After seeing comments on how it performs better for some languages than others, I am wondering, what will happen with these LLM when technology changes (coding language, popular framework) and they dont have a base of historical examples to train with.

Will it fail? Will quickly be brought up to date using stablished knowledge from other languages? Will slow down adoptiin of new technologies?",1,
1231,comment,cscareerquestions,n3m0vu1,1752751344.0,"Welcome to the club my friend!  
Join us in r/singularity, I've seen where this is going 2 years ago ([https://www.reddit.com/r/ChatGPT/comments/13h0tpi/i\_have\_15\_years\_of\_experience\_and\_developing\_a/](https://www.reddit.com/r/ChatGPT/comments/13h0tpi/i_have_15_years_of_experience_and_developing_a/))  
 but people generally only open themselves to the idea when they see it right in front of them.  
We will need to get used to the idea of being retired much earlier than expected and having AI and robots do the work for us while we enjoy our digital reality with our infinite waifus and wireheading. üòç

  
My only concern is the interim transitional period, when AI is good enough to create mass-unemployment of people that did not expect to lose their job while government don't really know how to handle the situation. (Seems like taxing companies for AI work and spreading the love is the only solution)",1,
1232,comment,cscareerquestions,n3m6upy,1752753744.0,"how people who watch AI push PRs in 5 minutes live in the same world as those saying ‚ÄòI tried ChatGPT once, it didn‚Äôt work' üíÄüíÄüíÄüíÄ",1,
1233,comment,cscareerquestions,n3mf4pd,1752756685.0,Was it a specialized agent built on top of a general-purpose AI like ChatGPT or Claude?,1,
1234,comment,cscareerquestions,n3mgzja,1752757296.0,"A lot of people don‚Äôt realize how close AI is to wiping out a majority of jobs because they don‚Äôt see it in use while companies test it out and fine tune it¬†

My last company was a tech startup that build an LLM AI model to interact with our other data tools. We partnered with a big consulting firm and insurance company and uploaded thousands of unstructured documents which the system was able to tag and model and then spit out detailed analytics and reports as well as future prediction modelling. ¬†They weee doing some testing and our accuracy was measured in the low to mid 90s¬†

They were forecasting that within 1-2 years the system would be accurate enough to replace the dozens or hundreds of people currently doing that work¬†

And we were a small startup with 60ish employees. ¬†Google, Apple or one of the tech giants absolutely could crush what we were doing¬†",1,
1235,comment,cscareerquestions,n3mqeb3,1752760264.0,"I'm a big fan of AI. I use it in my daily development and have been looking for ways it can be used to automate more of our tasks and processes.

In my experience, there are some things it does really well, and there are other things that require human intervention so that it works.

I believe it is revolutionary, but I don't think that it will lead to the dystopian future many people claim it will either.  Generative AI is widely available for cheap, and there are plenty of open source solutions and models that compete very well, so regardless of where AI goes, it's not just on the hands of a few corporations. 

I believe it will enable a lot of smaller software businesses to show up and will help us to start solving deeper and more complex problems by getting us up from many of the common problems we often have to solve in software development.",1,
1236,comment,cscareerquestions,n3n9khp,1752765720.0,"LLMs is just search algorithm history repeating itself. 

In fact, LLMs are becoming the search algorithms themselves. 

Three simple letters people 

SEO

We are at the beginning of a very shitty and very lucrative industry that‚Äôs built on manipulating public AI search results. An industry that‚Äôs going to destroy the public confidence in AI results and make them unsuitable for enterprise work. 

lol and this will be after we see a new advertising revenue model added into basically every public AI interface which also skews results. 

Welcome to SEO hell 2.0. 

This will in turn mean companies are going to offer more specialized, nonpublic models and I don‚Äôt doubt they‚Äôll be pretty damn good. 

In fact, I think they‚Äôll be good enough that players like Google and Microsoft will decide to tier those models and restrict them to ultra premium subscription packages that are priced ‚Äúcompetitively‚Äù with their human counterparts. 

Because why the fuck wouldn‚Äôt they?",1,
1237,comment,cscareerquestions,n3nuq3c,1752771571.0,"Our future is writing specs and doing detailed code reviews, probably assisted by AI for both. Learn architecture, common software gotchas to look for and how to interact with the code LLMs.",1,
1238,comment,cscareerquestions,n3ojfcd,1752778420.0,"I work large repo one of the largest in the world. With the right context given (teaching it how to fish), mcps, and initial prompt it can do wonders.

Anyone who says otherwise is probably throwing the entire code base at an llm with a 1 sentence prompt and wondering why the Ilm can do what is asked.",1,
1239,comment,cscareerquestions,n3l0fwp,1752732070.0,"It's also a question of the business side of things even care about developer productivity. If you look at the experienced devs sub, a common talking point is the futility of advocating for better processes and tooling, but the business side of things don't care about that. Big tech might try to convince the MBAs they can replace their workers with it, but like... They could've already done that several times over at some companies with non-AI solutions. Sure, maybe the Googles and Metas have the best engineers in the world and are hyper optimized, but everyone else is not. Most of the AI hype on the business side is just shoehorning in a ChatGPT wrapper into the product out of FOMO.",68,
1240,comment,cscareerquestions,n3kdggc,1752721668.0,"Still wondering is the fabled creature of ""2-5 years"" we've been hearing since 2022 actually even exists or not.

Even GPT3, aka ChatGPT, has been able to take care of small coding tasks on its very release day. Fast-forwarded 3 years 'till today, and it's... still only able to do relatively the same, anything more complex than that, and these models will spit out a lobotomised version of the code.

For doom scrollers, you guys may wanna read up on the saga that is Copilot's PRs history when trying to modify the .NET source code, that should be a good read.",55,
1241,comment,cscareerquestions,n3o106k,1752773264.0,The problem is we don‚Äôt know where we are in the curve of progress for generative AI. A year and a half ago it was like a toy and now it‚Äôs cranking out a ton of workable code for me. If we continue getting the types of leaps we‚Äôve had in the last few months to a year then anything below senior is fucked.,2,
1242,comment,cscareerquestions,n3lwycf,1752749598.0,"A second of attention of a junior engineer costs $0.005. So llms costs the same, and I‚Äôm not even sure this is a coincidence.",5,
1243,comment,cscareerquestions,n3l6yax,1752735411.0,"very true. i am a software developer and i heavily rely on these ai tools to work my way through tasks. having a sound understanding of what i am working on, the technical know hows of the stack, is what makes my life easier. writing code is surely a redundant skills now. 
smartly working with an ai agents along side you and making it do work for you is the new skill which people need to learn.",4,
1244,comment,cscareerquestions,n3kqoso,1752727343.0,"I've been using Deepseek in conjunction with paid chatgpt o3 for a while now. The results are better, the only drawback is it's more limited in image parsing",13,
1245,comment,cscareerquestions,n3wl257,1752880538.0,"My 1st tech team, they spend a decent amount of time detailing next steps for our tickets/tasks for other engineers prior to prioritizing and assigning. 

I‚Äôd bet that AI tools that we currently have can probably do a pretty good job addressing these tickets (the lower effort ones) due to the amount of detail they all have",2,
1246,comment,cscareerquestions,n3lj86j,1752742321.0,"Big tech have regulatory concerns, I work at a company that have a huge internal AI suite. Honestly not sure how it works in practice, as I doubt OpenAi would let even 100k+ employee corps self host their models. 

But we can only use AI tools we have enterprise contracts with for serious stuff, we can faff around on non product stuff with any AI tools to access them. But we can't use them professionally, on legitimate products. On top of that, any clients have to accept us using AIs during development

Generally speaking, the entire industry is not asking for clients permission and I'm looking forward to the first big corp to sue a vendor over this.",21,
1247,comment,cscareerquestions,n3kpzxx,1752727027.0,"It was noticed early on that context size increasing didn‚Äôt necessarily result in greater accuracy. 

The fallacy that I think folks are coming to terms with is that the explosion in advancements of LLMs that we saw over the past few years will not continue at this rate, we‚Äôre already seeing outlines of the ceiling on this thing.",107,
1248,comment,cscareerquestions,n3l0bva,1752732016.0,"AI tools are decent at pumping out green-field projects. Lovable is one example. I haven't tried that with copilot agent (Claude 4) yet, though.",5,
1249,comment,cscareerquestions,n3p69lj,1752784949.0,">If we continue getting the types of leaps we‚Äôve had in the last few months   

What leaps?  I'm sorry, but I just don't call a few percentage points in improvement after billions of dollars and thousands of man hours a ""leap"".  
To me it's like we had a race car that could go 200mph but broke down every lap, and we now have a race car that can go 250mph and breaks down every 3 laps.  Sure, that's improvement, but it's still not anywhere close to as useful as those trying to sell AI tools would like you to believe.  
Don't get me wrong, I'm no AI doomer.  I think it's a powerful tool, and when used in the appropriate circumstances, can be very impressive.  But thinking it's even in the ballpark of replacing devs is a sales pitch, not a prediction.",2,
1250,comment,cscareerquestions,n3kftug,1752722622.0,"It's coming. Seems it's still pretty easy to steer these AI tools into a ditch, though. It still takes a level of competency to articulate properly to the tool. Otherwise, you end up stuck in AI's (very apologetic) ditch. I imagine in another 5 years, these tools will homogenize so much generic code that the entire industry will lose its luster creatively at the lower level in favor of higher level ideas, mostly contributed by humans.",7,
1251,comment,cscareerquestions,n3k2tyg,1752717641.0,"I think you‚Äôre looking at it wrong. In order to compete in the marketplace when everyone is using the same AI tools, they‚Äôll require more humans to do work a machine can‚Äôt do, to innovate.",-4,
1252,comment,cscareerquestions,n3krmpz,1752727772.0,"Sorry to break you the bad news, but big chunks of your codebase was already probably pasted to all kinds of LLM vendors by your junior engineers...",219,
1253,comment,cscareerquestions,n3ktu2k,1752728805.0,"CEO's, even working with INCREDIBLY SENSITIVE DATA are just pushing it now. Everyone is using Claude in their codebases since the last few weeks. I work in a huge open office with like 30+ businesses in America and what OP is saying is what I'm hearing everyday now.

Everyone was hesitant until Claude Code dropped and now I don't know any teams that aren't using it. Its a little annoying because it's like being a fan of an underground band for a couple of years and now everyone is listening to them, haha. I personally don't use Claude, as I'm working in design, video marketing, UX ATM, and I'm using a lot of different AI tools daily, just not Claude. 

I feel like if you're working in an incredibly competitive field, this is very normal. What OP said is what almost every single developer who was anti-AI is saying now.",86,
1254,comment,cscareerquestions,n3kr5nq,1752727557.0,"Yeah that's pretty normal actually, we have dozens of AI tools and AI integrations but which model they're using for a particular thing I'd only know if I happened to see it mentioned somewhere or it was super clear from branding.",60,
1255,comment,cscareerquestions,n3n5w5m,1752764690.0,"I think we engineers tend to think things are simpler than they really are. We assume the human intellect is replicated in LLMs, but I‚Äôd assert it isn‚Äôt.

LLMs use the corpus of the internet to derive patterns and generate text based on the most likely meaning of our inputs. 

I‚Äôd assert there is knowledge or intelligence there, but it‚Äôs not like us just yet. It *seems* like us. It could even be sparks of us. I‚Äôd argue it is still alien to us. I‚Äôm skeptical we can just scale this model up and expect even better versions of this, but I could be wrong. 

Given my possibly poor understanding, it works how I‚Äôd predict it would. 

I think this is why it is better at greenfield tasks or more discrete tasks. There are a thousand tutorials on the internet on how to build something from scratch.

Working in an old codebase with tons of tech debt and no testing has been harder.",14,
1256,comment,cscareerquestions,n3n1ywd,1752763600.0,"It's also currently untethered. Corporations haven't really worked out how to stop it from saying things so enshittification hasn't taken it over yet.

Eventually big tech companies will want to put a muzzle on LLMs. They just don't know how to do that yet except by just filtering out entire replies or inputs.",2,
1257,comment,cscareerquestions,n3lefs5,1752739560.0,"I think ***LLMs*** may be hitting their ceiling but they're really just a tool in the AI toolkit, my eyes are glued to companies building bettet scaffolding for inference like verses ai. Basically systems that can really understand and respond appropriately to new situations.",1,
1258,comment,cscareerquestions,n3levfd,1752739802.0,"Yeah my intuition tells me that larger problems probably require exponentially complex models and compute. A bit like brute forcing comparisons between two data sets‚Ä¶ it‚Äôs not a linear relationship. Far as I understand, that‚Äôs kind of their approach with enhancing LLM‚Äôs‚Ä¶ they‚Äôre brute forcing with some added sugar.

It might be wishful thinking though as I like the idea of AI helping me with mundane stuff but me still being in control.",2,
1259,comment,cscareerquestions,n3mtl9l,1752761216.0,"I don‚Äôt quite agree. Solving complex tasks that need a good understanding of efficient data structures, design patterns etc and how different elements should be structured together comes with experience. LLMs can help write it or help guide you, but you need to understand when it is taking decisions or making actual mistakes, and imho it is even harder to look at code that isn‚Äôt correct but looks like it is than writing the whole thing yourself.

I found that on easy tasks, LLMs can maybe make me 10x faster but on comped tasks (which is what takes up most of my time), LLMs actually make me quite a bit slower",5,
1260,comment,cscareerquestions,n3kx84n,1752730467.0,There are definitely on-site LLMs in some cleared spaces. Everything's local or dark fiber.,27,
1261,comment,cscareerquestions,n3pcipv,1752786739.0,"How does a LLM user ""fine-tune"" the LLM? 

The phrasing ""fine tune"" traditionally refers to a machine learning engineering cracking open the neural network to add/remove neurons, change a few math functions, and run hyperparameter tuning to adjust the model.

Obviously, an LLM user isn't going to be granted access to GPT's neural network code by OpenAI.",2,
1262,comment,cscareerquestions,n3pg569,1752787820.0,"Yep and with overfitting, I‚Äôd say there‚Äôs a clue to what AI really does. And I don‚Äôt say human beings are better thinkers or anything. I say they‚Äôre very different.

The data we give ML algos creates ‚Äúexperience‚Äù and the algorithm ‚Äúlearns‚Äù, but it is deterministic in a much simpler way than our brains (which I‚Äôd also argue is deterministic). It can only execute the code we‚Äôve given it. 

It‚Äôs not better or worse in my opinion. It‚Äôs different. 

I think LLMs are the next stage of those specialized  ML algos, but might be lacking in some ways. Probably ways we don‚Äôt understand and haven‚Äôt anticipated.",2,
1263,comment,cscareerquestions,n3st4zu,1752839153.0,"You joke, but as a programmer the majority of my use cases aren't political so I don't really care either way... my money isn't going to them and I don't provide them much data, mostly asking questions instead of providing code....

so I'm comfortable being a net loss for them hahaha

------

Although, if it were possible to pay I probably would have jumped ship ages ago, just because the results are so much better than chatGPT in most topics. The only case where I preferred openAI was when dealing with Computer Graphics for some reason.",2,
1264,comment,cscareerquestions,n3lwpq4,1752749489.0,Copying to some server in China has been happening way before ChatGPT. You think this is new sir?,2,
1265,comment,cscareerquestions,kti5xs8,1709670204.0,"Yeah heavy Leetcode for a few months helped a lot.  Initially on the job I was using ChatGPT pretty heavily, but as time goes on I'm doing and more and more myself.  I'm working from home quite a bit at the moment which has allowed me to figure things out without people over my shoulder.",12,
1266,comment,cscareerquestions,kx262f8,1711682392.0,I built something for my company which literally gives the resume to chatGPT and asks to extract for a few key details,1,
1267,comment,cscareerquestions,kx01f90,1711654152.0,"I honestly do not know anything about university recruiting, never done it. 

FAANG does not only hire only from FAANG. They also hire from legacy tech companies like Cisco, Oracle, SAP, Dell etc. They also hire from large companies from non tech industries like Wallmart, Goldman Sacs, Pfizer, etc. They also hire from recognizable high growth newer companies like DataDog, Databricks, Coinbase etc. Its hard to get into a FAANG from a complete no name tho.",13,
1268,comment,cscareerquestions,kx065g7,1711655737.0,"But why would you ever look at that shit? It's always gonna be a copy-pasta/ChatGPT ""professional"" sounding nonsense.

Chances are your company is just some random company with a job that pays money, it's not gonna be anybody's life-long dream to work there.

When I see a cover letter required I instantly close the application, waste of time.",3,
1269,comment,cscareerquestions,kx0l7qq,1711660807.0,"
I am not even reading through all the cover letters of all the candidates. I only read the cover letters of the top 10~20 candidates to determine who I actually want to interview. Each cover letter takes like 30 seconds to read, but an interview process can take hours. I would rather read up the cover letter than waste my time on fruitless interviews.

One of the reasons I bother to read the cover letter is because so many of them are copy-pasta bullshit, or ChatGPT generated buzzwords. Although rare, I have rejected a candidate because the cover letter felt too ""generic"".

I should also mention that I work in gaming, so nobody in this industry is just doing it for the paycheck. The cover letters are actually entertaining to read sometimes because it can tell me so much about their gaming journey.",2,
1270,comment,cscareerquestions,kx1i1dt,1711672743.0,"Check out the free short courses on deeplearning.ai

After that, maybe check out: https://course.fast.ai/, which also has a book attached.

*Generative AI on AWS* is new and quite  good - the first few chapters provide a very readable overview before it goes more into details.",10,
1271,comment,cscareerquestions,n5vogkz,1753829087.0,"It looks like the CS would eventually become a niche area with AI landing all the entry level jobs. I would rather spend my money on other majors like health or engineering. Also, whatever openings remain in CS are being heavily offshored. I think CS enrollment will drop sharply.",21,
1272,comment,cscareerquestions,n5z8b2v,1753883254.0,"That was a good time to break in without a CS degree. Doesn't happen now. The problem is [overcrowding](https://archive.is/mPNcX). CS rose to be the #2 major at my university and it falls under ABET so it's no joke.

HR gets over 100 applicants for every entry level position. Hiring decisions have to be defended. Filter by degree, throw them coding questions when 10 max are getting interviewed. Then some CS programs got [watered down](https://web.archive.org/web/20241117232629/https://www.reddit.com/r/theprimeagen/comments/1gqv4vc/teaching_computer_science_in_the_age_of_gippity/) to cater to the high enrollment versus flunk the bottom 25% or anyone with a bad work ethic.

LC thing is still stupid. I never practiced coding in my spare time. My coworkers don't either and have no idea what LC is. I didn't until I came here. 95-99% of Fortune 500 application process isn't knowing obscure algorithms or churning recursion or n log n sorting on the spot.

Yet we have this total FAANG obsession by people who've never been hired and think everyone gets $400k. Applying to the most popular companies to people who don't work in CS. Microsoft in my east coast city offers $150k for very experienced devs. It's slightly above market rate. Then maybe you get laid off in 2 years.",2,
1273,comment,cscareerquestions,n60kzqp,1753897090.0,">  I think CS enrollment will drop sharply.

You don't think coding being easier than its ever been means *less* CS enrollment?  In the long-term, probably, but short-term I think it's going to lead to things getting worse before it gets better.",2,
1274,comment,cscareerquestions,lt6xuih,1729613549.0,"I interview by giving them a task to do with chatGPT/copilot/etc, screensharing with me, and tell them to do a task done in a functional, fast, scalabale, maintainable, well documented, well thought out manner, that they fully understand after talking with their AI. It's encouraged to ask their LLM questions to confirm assumptions, understand, choose direction, etc. 

That way you get to see what questions they ask, which reveals their thought process. You get to see how fast they get unstuck using LLMs or if they have a fundamental misunderstanding and ask the wrong questions and go down a rabbit hole.",158,
1275,comment,cscareerquestions,lt71ywm,1729614820.0,"You should allow and expect candidates to answer your questions in the same way that they will do the job.

If your job can be done by someone with little to no experience typing stuff into an LLM, then I'd be worrying more about your job security than the candidate. 

If your job entails more than that, then learn to ask better questions that actually gauge relevant skills.",34,
1276,comment,cscareerquestions,lt6syn9,1729612035.0,"I‚Äôve seen this too and it was so blatant. They‚Äôd say the same phrase every time ‚Äúlet me think about that‚Äù, visibly type something and move their focus to another part of the screen, then give the word for word ChatGPT generated monologue.",188,
1277,comment,cscareerquestions,lt6utvl,1729612611.0,You‚Äôre gonna get mad downvotes on this because most are dependent on LLMs so much now that they‚Äôre losing the ability to remember fundamentals,63,
1278,comment,cscareerquestions,lt85kei,1729627035.0,"Interviewers: Have your resume open on the side so you can cross reference.

Also interviewers: NO LOOKING AROUND >:C

I get that there definitely have been those who use chatGPT to provide answers to things they might not know off the top of their head, but you‚Äôre interviewing for a role in comp sci. A role in which they will likely have access to references and peers for collaboration. Looking over at a second monitor to cross reference stuff is not something that you should be concerned about. Especially when it doesn‚Äôt end up changing how they portray themselves or work with colleagues.",11,
1279,comment,cscareerquestions,lt6zn8t,1729614108.0,"I've never understood this approach to interviewing. Do you expect your employees to not use google/chatGPT in their daily work? Using these tools to analyze a problem they haven't encountered before can be extremely useful. I understand you don't want someone who offloads all the thinking to AI, so just require them to be open about what they're doing",25,
1280,comment,cscareerquestions,lt84i7k,1729626713.0,"Hey guys, don‚Äôt cheat at work. If OP catches you looking at ChatGPT in the workplace they‚Äôll fire you without notes.

Sounds like these guys dodged a bullet.",6,
1281,comment,cscareerquestions,lt7z0wh,1729625034.0,"Stop asking candidates to code during interviews, and they will be less likely to resort to cheating via ChatGPT.

Ask them questions _that are relevant to the job description_ and that are _about things they'd actually do on the job._ Don't ask them stupid ""gotcha"" questions like asking them to name every single API call in a framework. That's what Google is for.

Tech interviewers/recruiters need to learn their jobs 'cos this ain't the way :)",4,
1282,comment,cscareerquestions,ltbtxfh,1729684940.0,"\>eyes darting from 1 screen to another

How do you know they have 2 screens? More so, Zoom interviews are weird, it's not personal enough that it's comfortable staring at the interviewer's camera 100% of the time. If I'm darting my eyes during an online interview, I'm either reading off my bullet points that I prepared for the interview, or just anxious.

\>lengthy pauses between answers

Again, this is natural, I've done this in just about every interview I've done because I need to think over my answer. I'm putting my future on the line, don't expect me to just wing every answer off the cuff. 

\>insider information about processes used that nobody should know

What does this mean? I could see this meaning they are looking stuff up, but if ""nobody should know"" it, then why is it getting spit-out through ChatGPT?

\>very de-synced audio and video

What? This just sounds like a network issue or the interviewee not having a great setup, I don't understand how this means they're cheating at your interview.",5,
1283,comment,cscareerquestions,lt7njqb,1729621472.0,PSA: Please don‚Äôt ask leetcode style interview questions that are not relevant to the role or expect the candidate to not use ChatGPT while it‚Äôs actually been used everyday on the job.,9,
1284,comment,cscareerquestions,lt6va6a,1729612749.0,"This is because of the hiring practices, candidates are just adapting to the interview game which is mostly an aggregate of questions that are not correlated to the job.
On top of this, the attitude of companies to underpay loyal employees and therefore incentivate job hopping. 

To summarize, on the one hand companies push employees to job hopping in order to keep up with inflation and salary rises, on the other hand the interview process is flawed to say the least. These generative AI tools are showcasing what a clown circus this whole game has become.",17,
1285,comment,cscareerquestions,lt92zog,1729637713.0,ChatGPT is now a reality of every industry and it‚Äôll keep ramping up.,3,
1286,comment,cscareerquestions,ltah92g,1729656268.0,"If you are interviewing with me, please cheat.  You are going to be using ChatGPT, Stack Overflow, GitHub Copilot on the job.  Start now!",3,
1287,comment,cscareerquestions,lt6t2o1,1729612070.0,"I don‚Äôt understand the issue with using ChatGPT during a technical interview. You‚Äôre assessing their ability to functionally solve issues. It doesn‚Äôt exactly matter what tools they use to get the answer as long as they understand what the issue was in the first place and WHY the answer is the answer. 

That being said, both parties should be transparent about what tools are being used and what tools are ‚Äúallowed‚Äù to be used.

Edit: if anything, I‚Äôd prefer that the candidate show me what prompt they‚Äôre entering into ChatGPT. Intelligent promoting is a skill on its own.",49,
1288,comment,cscareerquestions,lt7wjf7,1729624267.0,At this point I'm actually glad people are cheating with LLMs. This just shows how stupid these leetcode-based interviews are. Perhaps companies will finally start taking hiring seriously.,2,
1289,comment,cscareerquestions,lt877yo,1729627540.0,"Hey mods, can you show how organic the upvotes on OP post is? Because this all smells like corporate shilling to me.  
No one is going to be dumb enough to agree with this post and upvote it. EVERY SINGLE PERSON on planet earth are going to have their notes somewhere, before the interview or during.  
We are grown ass adults here, we're not in High School and College. When you're working, you're going to chatGPT, you're going to Google. Who gives a rat ass if someone is looking at notes during interview. The only determining factor that would make it a bad candidate is they are just reading off script verbatim. If you're that terrified, then curate your questions better so that it's not a one off respond. Why are you crying about looking their own notes during an interview? Are you planning on hiring humans or are you looking a terminator in the flesh?  
  
Interview to work =/= Taking test for credential.  
Who are you trying to fool?",2,
1290,comment,cscareerquestions,lt8wkpw,1729635479.0,"One team I interviewed for did it so well. They gave a basic full stack project that we could render and asked to complete certain tasks like debugging a defect or adding features like another API call. 

It was refreshing and they wanted to see my thought process rather than solve some random algorithm or trivia questions about tech. It was the closest simulation and best gauge of talent I‚Äôve had in my interview journey.

They had me share my screen and there was really no way to cheat since the code is spread among different files and whatnot so you couldn‚Äôt chatGPT your way through it.

PSA: don‚Äôt want people to cheat the system? Make a fair evaluation that tests real life skills instead of unrelated trivia.",2,
1291,comment,cscareerquestions,lt95etk,1729638563.0,"Makes me wonder how much you, yourself uses Chatgpt üòèüòÖ. I find when people are very suspicious of others is because they do exactly what it is they are suspicious of. ü§∑‚Äç‚ôÇÔ∏è",2,
1292,comment,cscareerquestions,lta6u9o,1729651835.0,"Interviewer: ""you can use language documentation as reference""


Interviewee: ""Ok"" (looks at python doc)


Interviewer: ""your eyes moved, you must be using a ChatGPT, goodbye""


This might be happening somewhere.",2,
1293,comment,cscareerquestions,ltao4yg,1729659628.0,"Same thing is happening with us. The candidates are clearly using chatgpt during phone screens. If you can't code a simple question without chatgpt, we don't want you. Chatgpt can be used as an assistant but if you need to use it for everything then you don't know how to program. In my personal opinion, you should want to figure these things out on your own, as I thought programming these small questions are a fun challenge. I'll likely get downvoted because this sub thinks the opposite.",2,
1294,comment,cscareerquestions,ltbl0xu,1729680606.0,"It‚Äôs not ‚Äúcheating‚Äù this isn‚Äôt high school.  I don‚Äôt care if you use chatgpt, copilot, stack overflow, mdn or anything else if you can communicate and solve problems effectively you‚Äôre hired.",2,
1295,comment,cscareerquestions,ltbwjyy,1729686075.0,why insider knowledge let you figure its chatgpt?,2,
1296,comment,cscareerquestions,ltbxb2f,1729686390.0,"In the job, aren‚Äôt they going to be using Google, chatGPT, stack over flow anyways? I don‚Äôt see the big issue. If the goal is to produce a good product and the use of chatGPT or Google speeds up the process for a developer to develop their feature for said good product. For me it wouldn‚Äôt be a problem. Companies need to get off their high horse about dev interviews.",2,
1297,comment,cscareerquestions,ltbzlx6,1729687324.0,It's tough for me to judge what cheating is because as someone coding for 25 years I look stuff up on Google/chatgpt all the time.,2,
1298,comment,cscareerquestions,lt7cw79,1729618168.0,"EM here. Recent candidate interviews have been riddle with this. Big tell is them repeating the question back to you aloud. 

I don‚Äôt like asking puzzle questions anyway, when assessing a candidate‚Äôs competency I ask open-ended questions like ‚ÄúAs developers we are always constantly learning and leveling up our skillsets. The languages we use receive upgrades and new features, we explore more advanced APIs of languages we use day to day and we learn new frameworks and techniques as we progress. Tell me about a new language feature or a previously underutilized language API, coding practice or framework you have implemented recently that has improved your workflow.‚Äù The follow questions drill down on WHY that specific answer was useful to them. 

Devs using ChatGPT will often answer ‚ÄúJavaScript recently introduced the assignment operator‚Äù and I follow up by asking about specifically how that helped them. They will say ‚ÄúIt makes my code more readable‚Äù and I ask ‚ÄúDid you have a code readability problem? Where in your code was this a big issue for you? Can you give me an example?‚Äù

ChatGPT will give very simple answers unless prompted with better context. 

If you really want to say ‚Äúclose your eyes while you answer the next question‚Äù and see what they do.",2,
1299,comment,cscareerquestions,lt7ejpy,1729618680.0,">	7/10 have been immediately cut (we aren‚Äôt even writing notes for them at this point)

And you wonder why some candidates may not feel like putting in more effort?

It seems to me that nearly all of your criteria for ‚Äúdefinitely cheating‚Äù has a very normal and innocent explanation. Aside from ‚Äúinsider information about processes used that nobody should know‚Äù but then why would you expect this information to be readily available through an LLM?",2,
1300,comment,cscareerquestions,lt7gj7f,1729619293.0,"This is actually simple but great advice. The comments seem to be filled with people who have either never done an interview so don‚Äôt see how obvious it actually is or people so arrogant that they think they a: won‚Äôt get caught when they do it and b: think that it‚Äôs acceptable to use google/chatgpt because they‚Äôre allowed to use it in real life. 

What they don‚Äôt seem to understand is interviewers have a very short time to make a judgement about you, and if you decide to use ChatGPT to answer their questions, regardless of how smart you think you are , you will never get a job offer. 

Take the advice. It‚Äôs good advice.",2,
1301,comment,cscareerquestions,lt81dn9,1729625757.0,"This is the reasonable answer right here. I don't give a fuck if people use ChatGPT or not, I can tell by just asking them to explain it to me if they understand or not, like it's really simple to tell if someone gets the code or not by just asking them to mentally step through it and explain it to me ü§∑‚Äç‚ôÇÔ∏è

If you have to artificially restrict people because they are ""cheating"" on your little ""interview tricky tricky"" test then thats not somewhere I would like to work anyway.",34,
1302,comment,cscareerquestions,lt8dm3v,1729629477.0,"Yes, exactly this. I haven't interviewed anyone since before ChatGPT was publicly available, but I always (and my coworkers always) explicitly allowed candidates to Google. I would allow ChatGPT too if I were the one making the decision. Why restrict the tools available when they aren't restricted on the job?

And yes, same deal with the ""cheating"" too. A substantial portion of candidates copied the same solution they found online. Besides the fact that I'd seen the solution 20 times before line for line, they also never explain what they're doing or why, which is a good way to fail an interview whether ""cheating"" or not.",2,
1303,comment,cscareerquestions,lt81dvb,1729625758.0,I'm not experienced with using chatgpt but if it's just like using Google and stack overflow to drill down to the solution then I guess this won't be so bad,3,
1304,comment,cscareerquestions,ltlobz2,1729814216.0,"I take a similar approach, as I use our companies private ChatGPT to handle a lot of quick grammar and syntax checking, as well as other loose fact-finding, so want to make sure folks working for me can do similar levels of hybrid analysis.  I also use ChatGPT to help parse resumes and on occasion assist with some structuring of interviews according to time constraints.  I try to have fun with interviews.",2,
1305,comment,cscareerquestions,lt70c7j,1729614320.0,"I‚Äôve had this happen when conducting interviews for senior candidates and I generally ask an open ended design question ‚Äî no coding required. We‚Äôre pretty flexible and stress there is no one right answer, but just want to see if the candidate can roughly conceptualize a system and speak to tradeoffs. Even then we get people who we suspect give regurgitated answers from an LLM on the other screen.",7,
1306,comment,cscareerquestions,ltauqjl,1729663357.0,"Mind you, they are screening entry level candidates.  These are people who likely have a surface level knowledge and little work experience.  tbh that they can google / chatGPT and get the right answer with only a brief pause is actually a pretty good skill at that tier.",3,
1307,comment,cscareerquestions,lt6twj1,1729612326.0,"I really don‚Äôt get why these responses and the OP are being downvoted.

Stuff like chatGPT can be a very useful tool, but you shouldn‚Äôt rely on it! It‚Äôs totally reasonable to ask people to not use chatGPT during an interview. Like let them google stuff, screen share, etc, but if you‚Äôre reliant on an AI tool to perform well in an interview? Cmon.",102,
1308,comment,cscareerquestions,lt9kbi1,1729643753.0,"Yeah, we recently did interviews for a few positions and only had 1 blatant cheater make it to the final round, and the funny thing is in the coding portion the code wasn't quite working and they couldn't figure it out, like writing a perfect class start to finish like they were just reading from something and then not understanding the errors they were getting when the tests weren't working.

I'm guessing either OP is imaging things or they have a bad screening process. We had plenty of people try to cheat with chatGPT in earlier rounds but they can be screened out pretty easily",3,
1309,comment,cscareerquestions,lt7a198,1729617290.0,"I'm seeing this in university, some people can't write the main class in java without chatgpt lol",3,
1310,comment,cscareerquestions,lt8fsiu,1729630118.0,"Depends on the questions. If you‚Äôre copy and pasting proprietary source code into the llm then yeah, we don‚Äôt want you.",5,
1311,comment,cscareerquestions,lt8xa5i,1729635721.0,"> Do you expect your employees to not use google/chatGPT in their daily work? 

But they're generally not asking daily-work questions. They're asking different kind of questions to gauge foundational understanding on things like time complexity and algorithms. They want employees who have a good foundation on the basics so when it comes to implementing a crud app in the latest trendy framework or whatever they know *which questions to ask*.",4,
1312,comment,cscareerquestions,lt6u8wz,1729612431.0,"It absolutely matters. If you're sitting there reading off chatgpt answers word for word, you're demonstrating a level of trust in LLMs that's frankly pretty concerning. This behavior is not demonstrating an understanding of the answer, or even that the candidate thought about whether or not the answer they were given is correct. Furthermore, we all know what interview questions for early career candidates look like. They're generally pretty easy, fundamental concepts that an entry level employee should know without asking fancy auto complete",64,
1313,comment,cscareerquestions,lt6x8m2,1729613360.0,"I am flown into client sites and even before that I had to talk through high level designs interactively in meetings.  What do I look like having to use ChatGPT to answer simple design questions?

That being said, I use NotebookLM to capture notes, transcripts of calls, etc to ask questions about a project we are working on.  No it doesn‚Äôt train on your data and we are GSuite users anyway",6,
1314,comment,cscareerquestions,lt6x112,1729613295.0,So what happens when ChatGPT doesn‚Äôt have the answer? Or Google? You need to show you can problem solve. This whole ‚ÄúI have a right to use ChatGPT!‚Äù is super entitled.,11,
1315,comment,cscareerquestions,lt7m04l,1729620986.0,"That‚Äôs why we allowed them to use whatever resources they wanted to, *as long as they told us what they were using*. We specifically called out ChatGPT, Stack Overflow, Google, IDEs, whatever. 

The problem was the candidate did not tell us she was copying from stack overflow. That‚Äôs the cheating part, and why we immediately disqualified her.",5,
1316,comment,cscareerquestions,lt6zgr4,1729614052.0,"Bring back interviews where you treat people like a human and get to know them.

chatgpt sucks at those",15,
1317,comment,cscareerquestions,lt744qs,1729615481.0,"I mean, traditional engineering engineers get hired all the time without going through some leetcode style gotcha process that is prone to cheating.  Whole thing reeks of a trivia contest and not a good test of aptitude.

For any kind of traditional engineering job, you be qualified on your resume, you meet with people, you talk out stuff, you ask questions about fundamentals... you check for a culture fit, you make a hire.

If it doesn't work out... you fire them.  You move on.

Why can't SD hire like that?

SD has such high turnover anyways, that whole job hopping every 2 years shit during good times, like are people really going to posit that firing a bad developer after 6 months is cost prohibitive compared to your superstar leaving in 2 years for a better job?

My outsider perspective here (chemical engineer, not software... sorry, this sub just fascinates me so I come here) is that interviewers think they're just so damn smart.  These interview processes serve to reinforce their superiority, let them be a petty tyrant of a petty kingdom.

Like OP on this thread just... gives me ""I am very smart..."" vibes.  Plus like, if you had a dude, who could do ALL THE THINGS, and answer ALL YOUR QUESTIONS successfully but with ChatGPT?  Like... isn't using AI to do that the literal wet dream of software development management?  Hire that guy.

I don't get it.",7,
1318,comment,cscareerquestions,lt6yaxs,1729613690.0,"People say ""BUT I WOULD USE CHATGPT AT MY JOB""

Yes, but I already have ChatGPT. If you're just Sigourney Weaver in Galaxy Quest retyping what ChatGPT says, I'll hire ChatGPT.",45,
1319,comment,cscareerquestions,lt75oon,1729615949.0,"If LLMs can do a student's assignment at a sufficient level, they're definitely going to use the same technology once they hit the workforce.  Maybe the assignments should evolve to, ""Create a paper with valid citations that also references as many things from our class discussion as you feel relevant.  Also, cite any AI and grammar tools used.  Write an afterward discussing what you learned in the process of writing this paper.""",2,
1320,comment,cscareerquestions,lt88331,1729627803.0,"A lot of people in our profession are gatekeepers, they do things like this to make themselves feel smarter than they are.

I've interviewed people in the last year knowing they will use ChatGPT and copilot, it's fine, I want them to, we work with complex shit and I don't want them to get stuck. They should exhaust their tools and show they can problem solve in time sensitive scenarios.

We just reduced the test time to 4 hours instead of 2 days (they don't have to finish) and if they can't explain what they've done then it's a no. I don't want someone who blind copy/pastes, it's fine but know what your code does and be able to tweak it.

It's wild to be like ""Oh that guy looked at his second monitor, we can't possibly hire him! Cheater!"". Talk about jumping to conclusions...",4,
1321,comment,cscareerquestions,lt72efq,1729614952.0,"If you google reverse tree algorithm, gemini kicks in and gives the code right on the search page. Pretty soon the line between search and LLM is going to be non-existent. I'm not sure what the right answer will be but letting them use Google is also letting them use gemini ü§∑",4,
1322,comment,cscareerquestions,lt7ku0d,1729620622.0,"Yeah, this doesn't add up at all. They say here that Google is allowed, but then look at how they determined these people were cheating:

>(e.g. chatGPT clearly on a 2nd monitor, eyes were darting from 1 screen to another, lengthy pauses before answers, insider information about processes used that nobody should know, very de-synced audio and video)

Everything but the A/V issues sounds like stuff that could come from Google.",12,
1323,comment,cscareerquestions,lt9yerk,1729648746.0,"Yes, it is the point of the interview to test someone's basic understanding. And then using AI to solve problems doesn't demonstrate that they do. 

I've hired people who didn't do phenomenally well as long as they could explain it and understood what they were doing. 

Slapping a prompt into ChatGPT and giving me the results (right or wrong) doesn't tell me your thoughts process. I don't care if you get it right. I care that you know what you're saying and can walk through requirements gathering and explain your hangups.",2,
1324,comment,cscareerquestions,ltczlzg,1729699294.0,"The problem I‚Äôve had in using LLMs professionally is they so often give completely wrong answers. At this point in their development, I think it‚Äôs *fine* if some people want to use them, but they‚Äôre far from a requirement (and remains to be seen if they‚Äôre even beneficial). 

Probably half the time I use ChatGPT to get help on an issue, or understanding obscure code, or solving some problem, the stuff it spits out is plain *wrong*. Then I end up spending more time verifying its results than it would have taken me to just find the correct answer myself. 

At this point I don‚Äôt trust them enough to rely on them in an interview (I would if asked, though I‚Äôd voice my concerns and it would be a red flag for me). I‚Äôd rather the interviewer gave a mock flawed LLM response and see how the candidate goes about finding the error and working around it‚Ä¶or just leave LLMs out of the interview.",3,
1325,comment,cscareerquestions,lt7g2jn,1729619151.0,"We have developers like that. Can answer all your questions using ChatGPT.


Unfortunately, their ability to read text aloud isn't what an engineer is useful for. These devs are typically our worst performers, because neither can they solve problems that an LLM doesn't solve for them, nor can they evaluate the (often _very_ wrong) LLM answers for correctness. Let alone considering how the LLM code fits into, supports, or leverages the existing code and architecture.



Ten times out of ten I'd reject the types of candidates OP is discussing.",6,
1326,comment,cscareerquestions,lt7u9w9,1729623556.0,More work for the honest ones ü§∑‚Äç‚ôÇÔ∏è can‚Äôt wait to cash in on the code that ChatGPT developers wrote in 5 years,5,
1327,comment,cscareerquestions,lt7fkan,1729618995.0,"I've been in a few debrief panels where cheating came up, both cases where cases where I saw the suspicious behavior myself as well as cases fellow interviewers claimed they thought the candidate was cheating and I didn't pick up such behavior despite actually keeping an eye out for potential signs of cheating.

Some body language can definitely be misunderstood and it really helps to have pre-chatgpt interviewing experience to know what ""normal"" fidgeting looks like, but there's also some behaviors that simply can't be anything other than cheating.

The asking to share screen is really silly because a lot of people have dual monitors. And without even getting into dual monitors, not all hardware setups are going to have the camera front and center like in a standard macbook, so there's plenty of candidates that just look like they're constantly looking to the side, *because that's where their actual monitor is in relation to their camera...*",81,
1328,comment,cscareerquestions,lt8op57,1729632849.0,">Because if you use google for simple interview questions, then the interviewer will not learn anything other than you are quick at googling stuff

Have you ever Googled anything in your life?

You need to understand the subject matter to separate the wheat from the chaff and interpret & apply the results to your situation. 

The comment you replied to even said

>You‚Äôll see how fast they can come up with a solution and then they can explain why they chose that solution.

Google, ChatGPT, and all these other tools don‚Äôt just spit out right answers. 

What matters isn‚Äôt understanding the ‚Äúwhat‚Äù, what matters is understanding the ‚Äúwhy‚Äù and the ‚Äúhow‚Äù. 

If you‚Äôre asking questions that tell you nothing about the candidates if they Google the answers, you‚Äôre not asking particularly useful or relevant questions. 

I‚Äôd expect the job involves not just being able to parrot what Google tells you, but understanding what it means & how to apply it. 

Even if someone can rattle off memorized facts without Google, it doesn‚Äôt mean they‚Äôre capable of that second part.",2,
1329,comment,cscareerquestions,lt74rot,1729615673.0,"Here's my problem: I use ChatGPT to generate an essay of 3000 words. I changed the first part a bit; I now have a 2% chance of it being AI-generated.

AI detectors like Turnitin also have a high chance of false positives; for example, if you quote a line from the source directly, you have a good chance of being accused of AI plagiarism.

Sure, there are options to fix those issues by configuring Turnitin settings, but as you say, why bother when you can give ""Automatic failing grade and looking at expulsion from college in extreme cases"" to everybody? Lazy solutions come from lazy people.

Preventing AI plagiarism isn't as easy as you think it is, and if you're willing to be an asshole and punish innocent people just because you don't want sinners to succeed is a terrible way to manage things.",4,
1330,comment,cscareerquestions,lt6xhkh,1729613438.0,That has nothing to do with ChatGPT,6,
1331,comment,cscareerquestions,lt7hx7e,1729619720.0,"> This is similar to the arguments that were given regarding calculators

It really isn't. You still need to know the fundamentals of math to use a calculator effectively. You don't need the fundamentals of anything to have ChatGPT spit out a paper.",6,
1332,comment,cscareerquestions,lt70ghq,1729614357.0,"I keep seeing this calculator talking point and it‚Äôs just not true. Kids in elementary school do not use calculators when learning basic numbers, addition, and subtraction. Calculators are supplements once you‚Äôve already learned most of what they‚Äôre capable of doing by hand. They‚Äôre then used to skip all that business to do higher level math faster. 

ChatGPT is being used not as a supplement to accelerate what students already know but as a main resource from which to copy/paste without learning.",16,
1333,comment,cscareerquestions,ltal9ko,1729658182.0,"And as someone who has hired many times, I agree with this. If you can generate amazing fucking code with ChatGPT and know how to explain shit, I want you on the team. I want people who know how to use the latest tools, and synthesize with their own knowledge and experiences.

Otherwise, what‚Äôs the difference between hiring someone based on whether they can memorize the capitals of the world vs being able to look it up on Google? 

CS jobs are hard enough. They‚Äôll be found out so quick if you know how to ask questions.",16,
1334,comment,cscareerquestions,n7hq981,1754602213.0,"They talk about replacing us because they don't want to have to employ us.

That's it.  It's a bunch of middle managers thinking that they're qualified to work the line, wanting to increase their pay by reducing their own head counts, and thinking that they'll survive the round of layoffs because they're special and keep the operation moving.

Also, based on how underwhelming ChatGPT5's improvement is, the technology *isn't* getting appreciably better. I suspect that we've already hit the limits of what LLMs can do effectively. They're impressive because they can pass a Turing test, but being able to pass a Turing test doesn't require correctness (and indeed may be limited by correctness: people believe bullshit all the time).",69,
1335,comment,cscareerquestions,n7gxjgr,1754593778.0,"My thoughts are that Claude-4-sonnet is really good and way better than chatgpt 4.

I haven't tried chatgpt 5 yet. I see it's available though, so I'm going to try it for my next story.

I use these models with Cursor AI and am a huge fan. I find coding way more relaxing. Nonetheless, one can't simply be a BA and use it, I still need to be a senior developer IMO to harness it correctly.

EDIT: after trying chatgpt 5, i like claude more. ChatGPT 5 was doing so crazy shit and churning and churning, it can't be trusted.",259,
1336,comment,cscareerquestions,n7gxqcv,1754593835.0,"Every technology has its inherent limitations that are not possible to overcome. The biggest issues for me with LLMs is their inaccuracy and their inability to solve non-trivial (read: something that's not googleable/something that the model hasn't trained on) tasks or even sometimes help in those tasks.

Those stem from the inherent limitations of LLMs as a technology and I don't really think they're possible to completely get over in any way that's feasible financially.",104,
1337,comment,cscareerquestions,n7hmxxf,1754601195.0,"I work at a well known large-ish tech company, and our top AI researcher gave an interesting presentation on the current state of LLMs. 

He described them as having two main parts: the pre-trained part and the ‚Äúthinking‚Äù part. At this point, the pre-trained part is trained quite literally on the entirety of the internet, meaning that we‚Äôre probably close to an upper bound on the benefits we can get from that part.

As he put it, how far LLMs can get in their capabilities depends on how AI companies can innovate on the ‚Äúthinking‚Äù part. Admittedly, I‚Äôm not super knowledgeable in this area, so I wasn‚Äôt totally following, but I think this is where agentic AI comes in (specialized smaller models working together inside a bigger model). 

I think I agree with your assessment. It‚Äôll be interesting to see if these models hit a hard upper bound in their capabilities.",10,
1338,comment,cscareerquestions,n7h12le,1754594820.0,"Why do you think everyone has shifted to ‚Äúagentic‚Äù as the new buzzword? It‚Äôs obvious that a LLM is just a monkey with a typewriter, so now the AI true believers are peddling the idea that if we can just arrange those monkeys in the correct org structure, we‚Äôll get Shakespeare.",63,
1339,comment,cscareerquestions,n7k00pn,1754633430.0,"What we currently call ""AI"" isn't even an artificial intelligence.",7,
1340,comment,cscareerquestions,n7h6tuc,1754596512.0,"There are AI models other than just chatGPT which are actually focused on coding.  Claude Sonnet for example is scarily powerful already.   I still agree that it won‚Äôt completely replace coders any time soon, but it is still a powerful tool already that can drastically speed up coding tasks as long as someone who knows what they are doing is managing it",9,
1341,comment,cscareerquestions,n7k2je1,1754634784.0,"These LLM's have already had their iphone moment, and then just like the iphone despite the hype the company gives at big extravagant public expo's each year about the new releases, each yearly release is just a slightly improved cleaner, sharper UI version of the previous.",2,
1342,comment,cscareerquestions,n7k5g2s,1754636342.0,"Not necessarily. It just means that throwing more information and money at it is reaching minimal gains. 

The bigger effect that these models have is that they will inspire a lot of AI researchers to try non-LLM techniques to achieve general intelligence. I don't believe LLMs will be what takes us to GAI. We need something more brain-like, in my humble opinion.",2,
1343,comment,cscareerquestions,n7key8u,1754641743.0,Chatgpt just won gold at the imo. It's very obviously capable of replacing most software engineers. Openai is just limiting the capability of it's public llm so it can sell it's actual technology to companies for billions¬†,2,
1344,comment,cscareerquestions,n7mqrs4,1754672416.0,GPT =/= all LLMs.,2,
1345,comment,cscareerquestions,n7n1sz2,1754675504.0,"My understanding is that LLMs use a sort of algorithm or statistical analysis/text prediction to guess what the best answer/output is.

However, the issue with this is that their output is restricted to their training data/information on the web.

They cannot truly ""think"". They cannot use critical thinking to come up with the answer.

So they are useful for quickly summarizing the mainstream answer, and if the mainstream thinking on any given question is correct, then AI will output the correct answer.

However, the paradox is that the mainstream thinking is often wrong, especially for more complex questions. So AI will in such cases just parrot the most prevalent answer, regardless of its validity.

Some may say this can be fixed if it is programmed correctly. But wouldn't that defeat the purpose of AI? Wouldn't it then just be parroting its programmers' thoughts? Also, the question becomes who programs it? The programmers will not be experts on all topics. Even if they hire experts from different fields, the question becomes, which specific expert/expert(s) are correct/how were they chosen? This would come back to the judgement of the programmer/organization that is creating the AI, and this judgement itself is flawed/insufficient in terms of choosing the experts. So it is a logical paradox. This is why AI will never be able to match the upper bounds of human critical thinking. Remember, problems primarily exist not because the answer/solution is missing, but because those in charge lack the judgement to know who to listen to/pick.",2,
1346,comment,cscareerquestions,n7hfsaj,1754599103.0,"does anyone really care anymore about LLM versions released anymore? There‚Äôs minor improvements here and there, some LLMs better for this or that‚Ä¶ but like if gpt/gemini/claude/deepseek whatever release anything it‚Äôs all minor nowadays.",3,
1347,comment,cscareerquestions,n7hgdhh,1754599271.0,"Those who use AI to boost productivity will replace those who don't use AI tools. AI itself won't take the jobs.

If you haven't boosted your productivity yet using AI your falling behind.",3,
1348,comment,cscareerquestions,n7hco7z,1754598208.0,"It's what I said years ago. The exponential growth phase of LLMs was over. For the last couples of years we've been at the logarithmic growth phase. Expect more improvements, but at a much slower rate.

Newer models will still significantly change the world. Just not the way AI companies advertise it.

Also r/singularity banned me after constant critique of their AI hype.",2,
1349,comment,cscareerquestions,n7gzkf0,1754594375.0,LLMs definitely reaching limits with current paradigms. Embodiment is next step IMO and most everyone else.,1,
1350,comment,cscareerquestions,n7h3wi3,1754595655.0,"S/AI/LLMs

There is actually useful AI software like OpenCV that shouldn't be lumped in with LLM Shovelware.",1,
1351,comment,cscareerquestions,n7h6g3c,1754596398.0,"I have been a holdout in seeing the benefits but just last week started using ChatGPT for a lot of different tasks. It definitely allowed me to do about 2 weeks of work in 2-3 days. Now that 2 weeks would have involved me being quick on some regular things and much slower where I am rusty on stuff. The AI cut a lot of the re-understanding of the rusty stuff out though. It definitely got a few obvious things completely wrong but that‚Äôs where you jump in and fix it and move on. I was honestly impressed this last time I used it with how well it was working for me.

AI isn‚Äôt going to 1:1 replace developers. It‚Äôll just really enable cutting head count significantly.",1,
1352,comment,cscareerquestions,n7hcaal,1754598097.0,"ChatGPT is still the generalist king, but it's been behind on coding since Claude 3.7 came out. Claude 4 is still better than GPT 5 (from my limited experience with it, kind of hard to say before my company has made it an option for actual production use)",1,
1353,comment,cscareerquestions,n7hnz08,1754601510.0,"llm models are accelerating the capabilities of devs. without it i would have spent more time for my tasks, instead thx to AI my learning speed and solving is increased

I remember when i wasted 1 week to understand this physics proof that i really cant understand how from that line become the next one. People with AI has already an assistant that helps them and improving their life

i think for devs we are still away but for pics and movies, soon we will have porn, movies, mangas all made with AI. many people in that business will lose job",1,
1354,comment,cscareerquestions,n7hwp2w,1754604247.0,"I wrote about exactly this back in May:


https://www.davidhaney.io/llms-will-not-replace-you/",1,
1355,comment,cscareerquestions,n7i6902,1754607421.0,"And chatgpt is like the best there is right now.

Just look at customer service AI chatbots. They are terribly bad they can only point you to premade web pages. They can't actually do the work of a customer service rep.",1,
1356,comment,cscareerquestions,n7i75mz,1754607727.0,"Been saying this for weeks now. I was actually pretty nervous a couple months ago. With how long it took them to release GPT5, you knew it was going to be underwhelming. Officially hit diminishing returns for current LLM tech.",1,
1357,comment,cscareerquestions,n7i7j11,1754607854.0,"This was inevitable. GPT-4 pretty much already swallowed all of the useful data in the world, and throwing more compute at something without more data doesn't tend to improve it much.

Not to mention, LLMs have been on the inefficient part of the logarithmic slope since GPT-2. We're at the point now where we're paying exponentially more for each incremental gain. This was quite foreseeable. And many did foresee it.",1,
1358,comment,cscareerquestions,n7i9k43,1754608527.0,"What do you think people mean when they say that SWEs are going to get replaced? When you hear the term ""Agent"", what comes to mind?

Can you describe a task that a) GPT 5 can't perform reliably and b) cannot be broken down into simpler parts? 

My opinion is that it won't be a model that replaces SWEs, it will be a system of hundreds or even thousands of pieces working together, each with their own strengths tailored to their own responsibilities. Some of those pieces might be software, some might involve a human, some could be an LLM.",1,
1359,comment,cscareerquestions,n7igfwo,1754610883.0,ChatGPT is great for debugging tricky issues.  It works with the user's creative and critical thinking to find answers faster than they could alone.,1,
1360,comment,cscareerquestions,n7iiqxj,1754611702.0,I actually found out today that ChatGPT 5 is not a people pleaser. They probably fixed that (possibly the biggest complaints about previous version). But yeah this is not tech related.,1,
1361,comment,cscareerquestions,n7itbp8,1754615444.0,ChatGPT 5 is heavily monitored and restricted on most topics.,1,
1362,comment,cscareerquestions,n7itm3q,1754615546.0,Any personalization recommendations for ChatGPT 5 that prevents less restrictions,1,
1363,comment,cscareerquestions,n7j0t5i,1754618153.0,"This wasn't a coding model, retard.

OMG They only got 5% better on SWE bench in 6 months. Do the fucking math on that.

Claude Flow just scored 85%. We're rapidly approaching the point where LLMs can solve 100% of real world coding issues.

You don't think models trained on AWS/GCP architecture won't be able to do systems thinking as good as a human?

We're already at the point where LLMs are able to make changes to model scaffolding with significant gains.

It's crazy how fast people get spoiled and adapt to new tech.

I also think the quality of most models is really limited by the humans using them.",1,
1364,comment,cscareerquestions,n7j23hr,1754618623.0,"Current LLMs can't be used to improve themselves, otherwise we'd be on the cusp of GAI / the singularity.",1,
1365,comment,cscareerquestions,n7j3i4a,1754619141.0,"I use ChatGPT a lot and excited about 5, especially in infra configuration and writing code. Super useful tool. Helps me prototype faster and explore different approaches.¬† But it's like a human driving a car, you may know how to actually drive the vehicle mechanically but if you don't know the road code instinctively and where you want to go then you're gonna crash a lot on the way and end up in the wrong place.¬† I'm continually having to correct ChatGPT, often on basic stuff like syntax, query fields, wrong versions of libraries etc and I can mostly spot when it's going down a rabbithole and man it loves going down rabbitholes.",1,
1366,comment,cscareerquestions,n7j3z9x,1754619319.0,"I'm not a developer, systems engineer or anything like that. I manage teams of 15-20 designers that create oil and gas facilities.


I use chatGPT a lot to vibe code away ""the boring parts"". The problem is, those boring parts are typically what I would farm out to entry level designers so they could understand one part of a system while gaining overall design knowledge.


On my projects, those positions are now eliminated saving thousands of hours off my estimates. Now if an entry level resume crosses my desk I don't hire that person. I won't ever need a entry level person again.¬†


AI will replace from the bottom up.",1,
1367,comment,cscareerquestions,n7jeme3,1754623454.0,LLMs have massive applications in coding. But they are unlikely to ever replace engineers,1,
1368,comment,cscareerquestions,n7jk8oy,1754625816.0,"I never really understood the argument that AI Chatbots could take our software jobs. Can they upload a dataset to a distributed storage system? No. They can generate the code to do that if you ask it to, but it won't know that it should do that, and it won't execute the code on your machine. They are just really nifty tools to make our jobs easier. Fancy telescopes with autofocus don't take jobs away from astronomers, they just make their jobs easier. Nail guns don't take jobs from roofers, they just allow roofers to complete a few more roofs per day. 

I'm being a little obtuse though because Generative AI did actually tread on many digital artists, graphic designers, and writers. Software development is just a tougher nut to crack. I do kind of hope progress on Generative AI has peaked for a while. If it has another revolution and it can write coherent books end to end and direct entire movies and competently teach children, society might be in for a shake up.",1,
1369,comment,cscareerquestions,n7jq0pk,1754628424.0,"I created a pygame with chatGPT 5 in about 10 minutes.  Having lots of fun with it!  
[https://github.com/robsuttonjr/python-games](https://github.com/robsuttonjr/python-games)",1,
1370,comment,cscareerquestions,n7jwtej,1754631747.0,"I've never thought LLMs were AGI. AGI **will** replace all humans. But LLMs aren't that.

I will admit that I thought that LLMs were a complete waste of time. Since then, I've used LLMs more and more to the point where it's now a daily driver. No, it'll never replace humans, but it'll enhance each individual's output.",1,
1371,comment,cscareerquestions,n7kexm6,1754641732.0,"Well, i am wondering why anyone would expect that the speed of improvement will stay the same or even increase. It never happened this way in the past. And from the theoretical standpoint: LLMs are information compressors, so it's exponentially hard to remove hallucinations. Not saying they are not super powerful for some tasks.",1,
1372,comment,cscareerquestions,n7kgl8j,1754642694.0,"Every new version of chatgpt is going to be a boring disappointment from now on. Just like when every new iphone comes out.   
It was very cool in the beginning, but now the improvements are small and gradual only.",1,
1373,comment,cscareerquestions,n7kocls,1754647073.0,"To me its a matter of specialisation. LLMs are a great ""generalist"" approach, the best we've had for sure. 

But look at chess for example, or AlphaStar and all the Alpha(game) AIs- they're absolutely amazing and don't take long at all to exceed top level human performance entirely... but only for what they specialise in. This was years before ChatGPT was even realised, and yet ChatGPT NOW can't even beat intermediate chess players (a game where AI beat humanity decades ago...).

Chatbots are great for most things... at a lower level. They could replace junior devs, for sure, and its something I think plenty of places may look at. But until, and unless, they can make one on the level of AlphaSoftwareEngineering or whatever, then yeah, senior devs, and regular engineers have nothing to really worry about (aside from dumb management, but a few years of market correction will fix that anyway). 

That said, I also feel like a lot of people forget just how sudden and how amazing these chatbots really were when they came out, its still crazy the things they can do... We've become used to it, of course, but I don't think I can ever go back to a ""this is impossible and could never happen"" point of view regarding AI. Before GPT came out I would have 100% said such a chatbot was impossible, or something I'd expect to see in the 2100s. 

  
You can never predict the future, especially regarding sudden breakthroughs in AI, IMO.",1,
1374,comment,cscareerquestions,n7kqc67,1754648141.0,"The project I‚Äôve been working on includes some incredibly niche theorems with little documentation. I‚Äôve learned these in and out in the course of the project, so I thought I‚Äôd give it to ChatGPT to see how it did. It failed miserably in several obvious ways.",1,
1375,comment,cscareerquestions,n7kwrf1,1754651297.0,"Adoption of AI tools has nothing to do with how good they are, and everything to do with how much money CEOs think it will save them‚Ä¶",1,
1376,comment,cscareerquestions,n7kxp3n,1754651719.0,"ChatGPT can‚Äôt replace automation engineers. My workplace can only use AI for a support chatbot, but not for engineering.",1,
1377,comment,cscareerquestions,n7le1pc,1754658044.0,"Didn‚Äôt you see all the hype posts ChatGPT solving 9 out of 9 trivia using the OS cursor while ChatGPT 4o solved only 2? Definitely gonna replace software developers!!! 

(Nope)",1,
1378,comment,cscareerquestions,n7ljfge,1754659815.0,"I run a team of quants, and we have a team of engineers and validation analysts. In Mumbai we pay something like $20k USD, in the US our cheapest employee is probably $100k.

Anyway I came here to say - these LLMs seem unlikely to replace good software engineers and quant analysts. 

But there are a lot of bad engineers and analysts, and I think they are screwed. You should see some of the slop that doesn‚Äôt get reported or fixed by our engineers, it‚Äôs significantly worse than a mediocre engineer augmented with an LLM.

Make sure you understand problem solving and your subject matter in a meaningful way and you‚Äôll be fine.",1,
1379,comment,cscareerquestions,n7lm5c6,1754660692.0,"The basic premise this particular aspect of the bubble is build on is ""scale will fix it"".

Just like the models started exhibiting their (admittedly impressive) capabilities at a certain complexity and scale, the hope is that there's another magical barrier to hit at which the current limitations are surpassed (or at least become statistically insignificant).

Whether that will turn out to be true or not is the research question that the world throws massive resources at, regardless of the opportunity cost.

The other and somewhat depressing aspect is that LLMs fail in ways that we (as in, humanity and society as a whole) don't particularly care for: if factual truth and details mattered, we'd be living in a very different world. So it's conceivable that this isn't the barrier to their ""success"" I think it ought to be.",1,
1380,comment,cscareerquestions,n7lq4jx,1754661934.0,I think it's important to note that ChatGPT has only been around for just under 3 years total. That's pretty incredible for a frontier new technology to develop as much as it has within that time.,1,
1381,comment,cscareerquestions,n7ltsm0,1754663020.0,"It won't until there's another zero-to-one occurrence, something that previously didn't exist, starting to exist. Whatever that is, wasn't available until LLMs and the relating architecture debuted.",1,
1382,comment,cscareerquestions,n7mw4ry,1754673915.0,"Individual LLMs may have slowly be improving, but the way they are being used is evolving.

Think of how a group of individuals work together without leadership vs. teams of people with organized leadership, group norms and procedures, people overseeing the activities of others to ensure quality and improve performance.

We are in the infancy of getting agents to work collaboratively with each other, taking on different roles and responsibilities. There will be many gains here.

We have uneducated laborers working on assembly lines constructing sophisticated products because of the organization, engineering and delegation processes that have evolved in manufacturing over time.

We will get to the point where dumb agents can organize into a smart collective taking on greater responsibilities than any single one of them is capable of.",1,
1383,comment,cscareerquestions,n7nbl5i,1754678342.0,"At some point in this race, you'd have to expect that ALL LLMs are going to slow down the rate of improvement.  It's the same as cars, they pretty much peaked in the late 80s~90s and haven't really improved since then in a major way.

They're already at the point where FINE tuning costs more than the pre-training, so it was going to happen.

IDK what happened with GPT 5, I'm seeing reports all over the place, but quite a few were ""less than stellar"".  It does seem like they focused on some specific area too, so IDK if that was accounted for.


-----------

Using what could have been a ""released too soon"" as a sign that AI will never replace programmers is FAR too much of a stretch.  It's like predicting the end of Apple because of the iPhone 5 not being a great improvement over the iPhone4s.

Just look at all the money being dumped into AI.  Do you REALLY think this is the END of AI and programming?",1,
1384,comment,cscareerquestions,n7ndtga,1754679001.0,"As someone that works in AI, it didn't take GPT-5 to demonstrate this. Any trivial understanding of LLMs would make this self-evident. Or any actual use of any LLM by a half-competent engineer.

Now, I wouldn't use OpenAI as the leader, they're simply popular. Many other companies are doing a lot more with less. Still, the way we're building LLMs isn't going to end in an agentic autonomous white collar worker.

Interestingly though I feel that image and generative video models are still making huge gains at a rate that far outpaces LLMs currently. Now they're much more single-use focused, but we still see yearly regular improvements there.

LLMs are still improving though. Only earlier this year with a high end claude model was I able to ask the LLM to fix an issue with a specific .NET class we had identified the issue for. No other LLM could even patch the bug. Claude had fixed the bug and applied 5 other 'safe' but completely unneeded checks into the class. It wasn't able to identify the bug given the whole codebase even when the exact issue was stated, however.",1,
1385,comment,cscareerquestions,n7nf8ih,1754679417.0,"What is scary ... For Management, ChatGPT is Biblical truth in every instance even when you prove otherwise.",1,
1386,comment,cscareerquestions,n7niwf6,1754680498.0,Who uses ChatGPT to code anyway‚Ä¶,1,
1387,comment,cscareerquestions,n7oq66t,1754694073.0,"im pissed that they removed o4-mini-high model and replace with generic chatgpt5, the mini high model were somewhat satisfying when it comes to coding when prompted properly.. and i just dont do ordinary coding, i do geometric, 3d/2d simulation ones as well as graphical (ex engine), GIS-esque type of peojects and i was satisfied with it, although its not perfect with initial answers but when prompted and corrected properly it can arrive to satisfying answer .. but ofcourse im experienced enough to know how i wanted it done

if it can do complex coding, it was easy peasy for standard IS type like websites or mobile app on or do some complex scripts, so far satisfied with how it can be used as my junior coder while i pluged my pseudo code as my prompt and algorithm ideas to it..",1,
1388,comment,cscareerquestions,n7p2w1z,1754698458.0,"I find that AI is bangs out problems that are well-defined and require basic application of data structures and algorithms. So from a problem solving perspective, it operates at a junior undergraduate level in CS. In other words, its applied reasoning skills are well below a junior dev. Its strengths are speed and breadth. It is effective at applying the basics in a huge range of application domains. But the moment an LLM enters a domain where expert knowledge is required, or where it needs to leverage a large proprietary code base, or needs to make contribute small features to a complex pre-existing system, in all these cases it totally blows.",1,
1389,comment,cscareerquestions,n88f4n5,1754971939.0,I just Saw a video of ChatGPT making a Photoshop clone. At this point I dont even know what to believe,1,
1390,comment,cscareerquestions,n8hxbd5,1755104287.0,"Some people thought LLMs were going to scale like the integrated circuit with the capabilities doubling every two years. We can see now that's not going to happen. We may have real AGI in the future, but it's going to require a whole new type of hardware technology to do it.",1,
1391,comment,cscareerquestions,n7jwrt9,1754631723.0,"They can't even pass a Turing test. Ask ChatGPT to explain music theory some time, then drill down. It can't keep it all straight.",11,
1392,comment,cscareerquestions,n7h9uuu,1754597404.0,ChatGPT needs lot of default prompting to make the output concise and serious. Claude does this out of the box. Yes you can get rid of the idiotic emojis and the sycophantic followup task requests.  I think OpenAI is more generous with usage limits in paid tier while I run out of Claude pro usage limits pretty fast.,9,
1393,comment,cscareerquestions,n7hcp3a,1754598215.0," Maybe some other model needs to be explored for LLM's. Chat GPT is also surprisingly bad at chess, to the extent that GM's can easily beat it. But chess AI's are way beyond world champion levels for more than a decade.

 When it comes to programming or doing mathematics, perhaps we need something else. A kind of branching/evolution algorithm that rewards code that comes closer to solving a problem vs code that doesn't. An LLM only regurgitates what a lot of humans already have compiled. That just isn't efficient for certain problems, as you mentioned.",26,
1394,comment,cscareerquestions,n7l6zjv,1754655540.0,"genuine question as someone not in AI research, do you think this limitation is just inherent to our current structure of LLMs? Not as often now, but it used to be that there were papers coming out somewhat regularly with new models for attention and ways to optimize the existing structure. Of course, now all the big companies are throwing ridiculous amounts of money at data centers for increasingly diminishing returns. To move the technology further, do you think the current system would have to be rethought?",3,
1395,comment,cscareerquestions,n7ievvg,1754610341.0,"It's not even true, because if the funding dries up and retraining LLMs becomes uneconomical, their knowledge cut-off will make it unusable for modern codebases & thus in a sense, worse than it was before",2,
1396,comment,cscareerquestions,n7hgbua,1754599257.0,Did you not see the paper that just came our that showed the exact opposite? LLMs make you less efficient.,11,
1397,comment,cscareerquestions,n7iocfe,1754613686.0,"Selfishly, I care way more about the dopamine hit I get from all my Stack Overflow answer up-votes. It's so nice visiting the site and seeing my workarounds for Visual Studio bugs helped other devs.

Too bad LLM's aren't trained with attribution for every fact. Then, if a user upvotes the chatgpt response, chatgpt would go and upvote my stack overflow answer!",8,
1398,comment,cscareerquestions,n7ih4u0,1754611131.0,"The difference is that human engineers can learn, but LLM will continue hallucinate.",8,
1399,comment,cscareerquestions,n7j0xqq,1754618199.0,"Maybe but this was what everyone was saying before Sonnet 3 and then 3.5 came out.
I'm not saying it'll keep taking huge leaps forward but I wouldn't lose faith just yet. Remember chatGPT 5 isn't geared towards coding like Claude is.",8,
1400,comment,cscareerquestions,n7ht3fb,1754603101.0,"It's shockingly bad at chess to the point where an avg casual player can beat it.  I'm about 2000 ELO and played ChatGPT for fun and I'd estimate its ELO to be. somewhere around 800-900.

It'll oscillate between very strong moves and very weak moves. Playing a near perfect opening to then just hanging its queen and blundering the entire game",24,
1401,comment,cscareerquestions,n7hbsrf,1754597959.0,"All AI outputs are hallucination, they're just increasing correlation with reality.¬†


The fact that you can still access older versions of their LLM (and that they're free/cheaper) seems to indicate that newer versions are just additional post processing and workflow refinements rather than an improved model or different logic paradigm.",29,
1402,comment,cscareerquestions,n7i3skx,1754606586.0,"Yea, right now very long context increase the amount of hallucinations by a lot, I've noticed it first hand even in simple conversations, let alone giving my whole codebase to an llm",7,
1403,comment,cscareerquestions,n7hk52a,1754600353.0,"Lol that paper is bs, in no way would having access to LLMs make you less efficient.",5,
1404,comment,cscareerquestions,n7iss2w,1754615248.0,"Yeah, this was actually one of the really disappointing things for me. Even from the standpoint of treating an LLM like an eager but fallible little helper, who will go find all the relevant bits from a Google search & write up a coherent document joining all the info & exclude irrelevant cruft... it failed at that for exploring chess openings or patterns. Not even playing a game mind you, just giving a text explanation for different lines

Like I wanted to have it go into the actual thought processes behind why certain moves follow others & such. If you read the wikibooks chess opening theory on the Sicilian it does that pretty well, that is,m in terms of the logic behind when you defend certain things, bring out certain things at the time you do, branch points where you get to make a decision. I was hoping it could distill that info from the internet for arbitrary lines. But it couldn't even keep track of the lines themselves or valid moves properly

Mind you this is stuff that's actually REALLY HARD to extract good info from on Google on your own, at least in my experience. there's so much similar info, things that might mention a line in passing but not delve into it, etc. Should be perfect for this use case. I guess the long lines of move notation don't play well with how it tokenizes things? Or maybe too much info is locked behind paid content or YouTube videos instead of actually written out in books or in public",5,
1405,comment,cscareerquestions,n7kjw14,1754644587.0,"yes, it will slow you down, because you have to carefully read LLM-generated code which is immensely slower and more cognitive loaded task than writing the code yourself because:

1. such code looks extremely convincining but the devil hides in the details and there are usually horrible things, you basically throw away 60-70% of generated code and the solution is usually synthetic between human code and AI code
2. LLMs has imperative to generate the tokens, so it produces unneccessary complexity and really long code listings, it literally has no reason to be laconic and straight to the point as senior SWE, models are not trained for that
3. LLMs are really bad at following design patterns and code writing culture in the provided codebase, so you have to correct how it organizes the code

the only thing that surely increased my productivity is more smart intellisense autocompletion provided by local 32B model, all the agentic stuff from paid models is unapplicable to real world tasks I tried to solve with it, I'm really not sure of what are all these people doing saying that Claude slashes JIRA tickets for them, in my experience, it wasn't able to solve anything by itself even when I pointed it at example

  
so far, productivity has only increased for those who simply push LLM-generated code to prod without proofreading it and it's usually a disaster",8,
1406,comment,cscareerquestions,n8ihvc3,1755110121.0,"Yea this is a fascinating experiment you tried there.  I've imagined that ChatGPT could help me with my understanding of openings but never actually played around with it.

These types of things where ChatGPT is spitting out pure nonsense in areas that I'm familiar with make me take it with a huge grain of salt in any other area.

I pretty much never take anything it says at face-value without verifying it from some other source.  Basically like what teachers told us we had to do with wikipedia growing up in the 2000s lol",2,
1407,comment,cscareerquestions,jwx5ho0,1692483749.0,"Just have chatGPT write it.

Their emails are probably auto generated, so it's fair.",2,
1408,comment,cscareerquestions,jwzrfhw,1692537069.0,Send them the ChatGPT response and let them waste time over it.,2,
1409,comment,cscareerquestions,jx1h7cm,1692562248.0,The trick is to repsond wie some code from chatgpt.the trick is to give them as much work as possible not Vice Versa.,2,
1410,comment,cscareerquestions,jww4ylb,1692470070.0,"Great job, you just had chatgpt answer a question when there wasn‚Äôt even a question given.

Maybe stick with accounting instead of cs?",0,
1411,comment,cscareerquestions,m3vacrn,1735217760.0,"Great. Another 30,000 Indians trained to ise chatgpt to cheat in interviews",2,
1412,comment,cscareerquestions,m3tzvkd,1735187663.0,And international student enrollment is going through the roof as well.. so tough few years ahead for Americans,39,
1413,comment,cscareerquestions,izexwdj,1670518043.0,"Just use ChatGPT. Say ""this is my resume"" and ""this is the job description"" now ""rewrite my resume to fit the job description"". Works pretty well. Also for cover letters.",32,
1414,comment,cscareerquestions,h7rvhxg,1628143031.0,"/u/downrightcriminal has a great list of bad practices.

I worked more in the embedded / physical product space on safety critical C++ based products so what I saw not entirely the same, but comparable.  

&#x200B;

* Code Reviews being being more formality than a true review of the code  So one of the primary ways to teach people how to write clean code was basically lost.
* No real up front code design / review it's just write code and make the feature work so we can Code Review it.
* No abstractions in the code base and lots of use global that are touched everywhere.
* Very superficial testing
* Very unstable tests where people just keep running it until it passes because there is some timing issue that causes it incorrectly fail. 
* No continuous testing. 
* Poor automated linters
* No use of design patterns and when used the rules to the pattern were broken in the name of ease.
* Always rolling in house code when a library could be used.  Somebody actually wrote their own cryptographic library for our messaging using Elliptic-curve Diffie‚ÄìHellman.  It was slow as shit and not usable.
* When external libraries were used it's was just this first one I found ""should work"" and no real investigation understanding all of the options before picking one.

A lot of the bad practices were caused due to bad management.  Management didn't trust the SWEs and thought everybody was a slacker who over estimated everything, but yet was always surprised that we were late to when they thought task would be done. 

Basically all management cared about was checking the box that we met the bare minimum FDA guidelines and the productid worked.   There was no real ""care"" taken to make a high quality product.

What really opened my eyes was reading books by Uncle Bob like ""Clean Code"".  While I don't treat ideas like SOLID as gospel there were 100's of approaches to code that just made sense to me.  

The whole idea of how methods should should work on one abstraction level or classes was the encapsulation of 1 idea was something I was never shown.  I was working with SWEs, some considered ""senior"" with 8+ YOE, that saw a few large methods as ""clean code"" and easier to understand.  

Small methods that do 1 thing that you use as building blocks for more complicated functionality was seen as too confusing because they ""had to jump around to methods too much"".  So over time their ideas of clean code was becoming more and more different than what I saw was clean code.",26,
1415,comment,cscareerquestions,n0y7w7a,1751467137.0,"The fact it‚Äôs been 3 years and layoffs are still happening left and right and the job market is still terrible I legitimately don‚Äôt see this getting better for at least another 3-4 years, if at all. 

CS enrollments are still at near all time highs, companies are hiring people outside of the US, etc.",147,
1416,comment,cscareerquestions,n0z3w57,1751476167.0,"It's not just Indian CEOs wanting to hire Indians. Companies thay are led by non Indians are still hiring tons of Indians. Go to any Amazon office in Seattle that is engineering heavy its dominated by Indians. Almost like India has a huge population and as a result because of massive CS graduation numbers, they produce a lot of good devs and they are good enough to clear the bar. Same with Apple, Wallmart tech, Cisco, Atlassian, Meta, all have lots of Indians even though the CEO is white. All these companies have massive growing operations in India too.¬†",13,
1417,comment,cscareerquestions,n101cxs,1751485970.0,">AI improving rapidly is the next black swan event

Is it actually improving rapidly though?

From my understanding things like ChatGPT were technologically possible for a long time but just not packaged into something easy enough for the regular person to use.

I've tried using AI to write code but it's wrong so often I just went back to Google Search",7,
1418,comment,cscareerquestions,n0zgeyr,1751479687.0,"It doesn't have to fully replace jobs. It doesn't even have to come close to doing that. It just has to give enough lift to allow 1 developer to do the job of 3-5 developers and you've completely devastated the existing workforce. And as someone who uses LLMs for programming daily, it's already about there. 

It doesn't get things right on the first try and absolutely needs human intervention, but if I need to write 200 lines of code it can still get it 80%+ of the way there which makes me wildly more efficient and effectively does replace a junior dev by doing the easy parts for me. 

I agree we're a long way off from the dev process being 100% autonomous, but the destruction of the job market is going to happen way before we get to that point.",2,
1419,comment,cscareerquestions,lqk9pm7,1728184331.0,"Isn't mostly Amazon mostly automated. They have a lot of systems that track productivity like even in the warehouse as well as the programming activities and productivity like they could just train llm. You know end then probably implement time management and scheduling specifically built for each different site or like office and the AI can basically do a checkup on people 

Cuz let's be real here managers they do a meeting and they want to check up and communicate like well. What if they can just do that by email and a message and the worker just has to reply that way and it's going to be hassle free because there's no emotions. Don't need to worry about creeps and the AI basically is going to facilitate. Definitely a better environment honestly cuz some of those managers and managers in general. Some of them are you know I don't want to say rude or Power tripping or racist or anything like that 

I think we're already at a point where AI can replace. Not everyone knows that actually but.... 5 cents ü§ó",1,
1420,comment,cscareerquestions,lqougev,1728254000.0,"When I was there, one of our first indoctrination videos were about it always ‚Äúbeing Day 1‚Äù.  Amazon has been about Day 2 for at least four years.  They are well in there MS/Ballmer years.

There AI efforts are a joke and so far behind the other companies it‚Äôs ridiculous.  Look at services like Comprehend, Kendra, Translate, Lex etc.  it‚Äôs embarrassing how far they are behind",1,
1421,comment,cscareerquestions,lmr328h,1726142083.0,This made me chuckle. A recruiter just asked how many years of experience I have with the chatgpt api even though it‚Äôs only been out for like a year and 10 months.,96,
1422,comment,cscareerquestions,lmvjemr,1726198327.0,"If I am ever asked this question, I'm telling them that ChatGPT actually uses Cleverbot's API and my experience is 16 years.",3,
1423,comment,cscareerquestions,lmtyl4b,1726176758.0,"Probably not even a recruiter, just ChatGPT",2,
1424,comment,cscareerquestions,ib5l61h,1654357139.0,"I don‚Äôt think those companies can keep up with ev demand for at least the next ten years compared to Tesla. Tesla also a good ten years ahead in terms of artificial intelligence and that‚Äôll be their bread and butter money maker while the other companies sit on the side and watch Tesla power through all the barriers of tech. It‚Äôs crazy stuff and I‚Äôm not saying it‚Äôs all elons master plan, but rather Tesla making the right moves at the right time. They‚Äôre just too ahead of the curve for others to outpace them",-13,
1425,comment,cscareerquestions,moxg9vu,1745563033.0,"Won‚Äôt PMs be less layoff prone than developers with AI? As a developer, I‚Äôve started using the AI tools like Q CLI and Cline and the amount of productivity boost these agentic tools give has gotten significantly better over the past few years. I fear for the future",2,
1426,comment,cscareerquestions,mozhfbf,1745594978.0,"I'm praying the lawyers working on the cases against LLM's using copyrighted material in their training sets win. If we get some real court wins, any company that used these LLM's to commit code into their repo's will also be affected. That will deter future companies of using these LLM's.",2,
1427,comment,cscareerquestions,h73k0tv,1627661909.0,"Reddit has long been a hot spot for conversation on the internet. About 57 million people visit the site every day to chat about topics as varied as makeup, video games and pointers for power washing driveways.

In recent years, Reddit‚Äôs array of chats also have been a free teaching aid for companies like Google, OpenAI and Microsoft. Those companies are using Reddit‚Äôs conversations in the development of giant artificial intelligence systems that many in Silicon Valley think are on their way to becoming the tech industry‚Äôs next big thing.

Now Reddit wants to be paid for it. The company said on Tuesday that it planned to begin charging companies for access to its application programming interface, or A.P.I., the method through which outside entities can download and process the social network‚Äôs vast selection of person-to-person conversations.",1,
1428,comment,cscareerquestions,l28aooi,1714649684.0,"This is mostly because Sundar is screwing up so hard. He's not been giving good growth for the company. Compare him to Satya Nadella, who's doing so well. He literally made Microsoft the most valuable company by market cap. He's been investing in bing, and integrating gpt in it, and has increased the users in that. He also made game pass, and made Xbox a lot more competitive. And Microsoft isn't moving jobs to India or Mexico, or doing as many layoffs for now. 

Meanwhile Sundar is not doing well. Gemini isn't as good as ChatGPT, and the latest release of Gemini was very bad, and google search seems to have gotten worse, Google is into doing some very shady tech with Israel for example. So he's busy laying off people to please the shareholders, and keep his job, since he needs to show them the numbers and metrics.",192,
1429,comment,cscareerquestions,l2elq3l,1714747837.0,Search.chatgpt.com has been registered. Kiss goodbye those sweet ad money,1,
1430,comment,cscareerquestions,l290yx0,1714660660.0,"I tried Gemini, it's comparable to other LLMs.",4,
1431,comment,cscareerquestions,l29ipy9,1714666776.0,Sorry if this is a dumb question but isn't there something to be said about IBM Wattson's global achievements in the early 2010s?  It was able to compete against jeopardy contestants before chatgpt.,12,
1432,comment,cscareerquestions,l28nnto,1714655632.0,"It's funny because googling stuff went from just typing in what you were searching for to knowing what key terms to throw in to get the search results you wanted. I feel like in the future, people are going to need to do some ChatGPT prompt engineering type stuff to get search results that are actually useful.",21,
1433,comment,cscareerquestions,l29zakg,1714672337.0,"Gemini is by far the best at creative writing of any AI. I keep being impressed by it because it comes up with things I never thought of. Meanwhile ChatGPT4 and Claude are tedious and stale.  

I'm not sure if this is due to the LLM itself or the way it has been preprompted though. I suspect it is the later. OpenAI basically lobotomized their chatGPT with stuff that it is no longer allowed to do.",3,
1434,comment,cscareerquestions,l2a7pgx,1714675105.0,"This is precisely the problem with google. SEO has ruined Google search. It's worse than it's ever been. And now comes a tech that lets people search with natural language, and get results that are comprehensible instead of sifting through a sea of ads. ChatGPT actually represents the first existential threat to Google's hegemony since their inception.

Don't get me wrong, over time, ChatGPT will probably sell out for more ads, and SEO will probably get replaced with AIEO, and nothing will fundamentally change for us. But it could change everything for Google.",9,
1435,comment,cscareerquestions,jfx4nek,1681270293.0,"I work for a insurance company (HMO) in the department that would have eventually would get your escalated enrollment. You are correct, it is often as stupid as that. Even worse because their is sometimes a third party between the company and the insurance carrier, so a extra chance for someone to drop the ball.",11,
1436,comment,cscareerquestions,l8gvzpo,1718305685.0,"I have this dim guess that it might be an attempt to stop AI-driven cheating? Like the idea being that the candidate has to clap *immediately*, which has a short enough turnaround that even if they're feeding the audio into a live transcription into an LLM they don't clap in time?

That, or it's a very strange attempt to handle candidate psychology.

Either way, lol.",157,
1437,comment,cscareerquestions,l8j654a,1718337647.0,"That's what I was thinking. I interviewed someone for a contractor position a couple weeks ago, and he was clearly reading from ChatGPT when answering questions (typing furiously in the background, then giving a textbook response, and each clarifying question was met with another pause and furious typing), and he wrote the code for the tiny programming problem I gave from top to bottom with no pauses whatsoever, and no mistakes on the naive unit tests...but he didn't foresee the edge cases I had prepared. When I asked why a particular input didn't work, he typed off the screen for a while, with his IDE still viewing the failed unit test, until after a few bouts of typing where I couldn't see, he switched back to the code and fixed a line without even having time to reread it to look for the mistake. He fixed the second broken edge case the same way, and couldn't explain why he made the change he did. He also couldn't predict the output correctly even when I had him step through the code. 

A clap-along sounds like a clever solution for at least part of the problem.",31,
1438,comment,cscareerquestions,l8hzb6h,1718319336.0,"Entry-level and junior positions get a LOT of unqualified applicants. People who attended a bootcamp and have done no self-teaching outside of it, people who haven‚Äôt even done that and know literally nothing expecting the company to train them, people from other countries who can‚Äôt even legally work in the US or require sponsorship, and even CS grads who slipped through the cracks with D‚Äôs or from schools that basically give you the degree as long as you pay and show up.

I‚Äôve encountered wildly incompetent juniors from all backgrounds‚Ä¶ CS, bootcamps, self-taught, you name it. The reverse is also true.

The clapping is probably to inhibit the use of ChatGPT.",4,
1439,comment,cscareerquestions,l8hh8bg,1718312699.0,Can't furiously type the question into ChatGPT for the answer if you have to clap when you hear the answer... that's a very unique way to stop that behavior üòÇ,14,
1440,comment,cscareerquestions,l8ndqzy,1718404339.0,"Yeah, we had a candidate that, at the time I didn't consider, I thought it odd he looked on the other screen a lot but figured maybe some notes or something?- but the other interviewer pointed out he spent so long trying to clarify the question. Get it to a sentence or so, before finally answering.

My colleague said he suspected he was using chatgpt - and my waffly question asking (I'm thinking them up in response to their experience, previous answers etc) wasn't working well with writing a nice succinct prompt.

Difficult to prove or even know for sure though, unless it's painfully obvious like in your case.",3,
1441,comment,cscareerquestions,l8k22v5,1718358120.0,Ok stallman,1,
1442,comment,cscareerquestions,ho2prjg,1639192633.0,Must be a Steve Ballmer wannabe,14,
1443,comment,cscareerquestions,ho53gm6,1639243020.0,It also depends on how long ago someone interviewed with MS.  For a very long time Windows was king and others OSes were totally verboten.  Post Gates and Ballmer that finally started changing more quickly.,2,
1444,comment,cscareerquestions,hz0pvje,1646203141.0,"This content has been removed by me, the owner, due to Reddit's API changes. As I can no longer access this service with Relay for Reddit, I do not want my content contributing to LLM's for Reddit's benefit. If you need to get it touch -- tippo00mehl [at] gmail [dot] com -- mass edited with https://redact.dev/",2,
1445,comment,cscareerquestions,jafrido,1677638915.0,Is ‚Äúuse ChatGPT for cover letters‚Äù unethical?,154,
1446,comment,cscareerquestions,jagu33s,1677663632.0,Classic ballmer peak one. Love it,5,
1447,comment,cscareerquestions,mmhf04u,1744331040.0,This is a ChatGPT post. Same thing was posted a couple weeks ago by the same user.,5,
1448,comment,cscareerquestions,mmdx1f9,1744292335.0,What in the name of ChatGPT is this post...,0,
1449,comment,cscareerquestions,mt5vzd6,1747678780.0,"Times like this, I like to remind people of the wild life and times of the the petroleum engineering graduates over the years.

Another high paying in demand niche engineering field, until you suddenly don't need them, and the median graduate is unemployed.

Petroleum engineering during the first ""bust"" in the early 80's, saw doubling/tripling of masters/PHD students as bachelor enrollments fell.  The most recent bust in 2015/2016 was LESS of a surge, but still none-the-less graduate school was a common safe haven for new graduates.

""Masters Degree"" or just ""graduate school"" in general is the first step in decreasing enrollment overall IMO.  Functionally, for CS, think of it as ""dropping out of the workforce"" for 2-3 years.

For many disciplines that ultimately saw drops in enrollment in response to economic conditions, flight to ""graduate school"" was a first step.

> With the influx of master degree holders I can see software engineering becomes more and more specialized into niches and movement outside of your niche closing without further education. Do you agree?

THIS ROUND, I do not, I see this as just a ""delay"" tactic for people in school to avoid reality for 2-3 more years on someone else's of future you's dime.  Schools are HAPPY to offer this... just look at how popular the GT / OSU programs have been.  People don't NEED those degrees unless they're in some already marginalized situation, and even then they function as a way to reset a candidate to ""parity"" with a bachelor student who DID study CS.

For the scenario you describe to happen, which of course is absolutely a scenario that has happened to some more niche engineering disciplines like biomedical, and to some extent civil/aeronautical, the HIRING crew has to get co-opted by graduate degrees.  MY perceptions is those specific disciplines have moved to being Masters preferred precisely BECAUSE they're lower volume, had crashes in employment, and eventually the hiring manager pool gets polluted by masters degrees.  Suddenly ""masters"" is the new normal.

I've found the #1 indicator of ""does a hiring manager care or think you need a masters degree"" to be ""whether the hiring manager themselves has a graduate degree.""  Software development thus far is so vast, that I have a hard time seeing the hiring manager pool getting polluted by graduate degrees that quickly.

This may be what happens ONE DAY, but I don' think this cycle is that day.",9,
1450,comment,cscareerquestions,mt6v53c,1747689263.0,How much of this is companies not being able to trust college grads given how easy it is to simply get a degree using LLMs. I feel like all these companies don‚Äôt want to risk bringing in ppl who they can‚Äôt trust because the degree itself is worthless now,1,
1451,comment,cscareerquestions,mu205ar,1748114234.0,"All that is telling me is that they are indirectly creating more jobs for engineers once their apps accumulate any decent complexity.

Even if most of the coding work can be done with LLMs - most C-suite people are not going to want to spend their days prompting away to build their apps, they‚Äôre going to want someone that actually understands what they‚Äôre doing, especially if money is on the line.",2,
1452,comment,cscareerquestions,mtntwkg,1747924418.0,"Run your resume (and the job description/ad) through ChatGPT to get it to re-word your resume to pass likely ATS filters. Proofread and touch it up for cosmetics, then try it out.

I went from total crickets/radio silence (in response to over a hundred individualized applications) to regular touchbacks, phone screens, interviews, and finally several offers.  This was the end of last year.

ATS is a pain, and is likely the cause of your applications just vanishing into the Void.",2,
1453,comment,cscareerquestions,n5c9r2y,1753569601.0,"It would be cool if this can be stickied. The huge layoffs took place mainly because of Section 174. 

TLDR: Trumps last term proposed a tax change for tech companies in 2017 which was set to take place in 2022, which made tax deductions on software engineering much more complex and difficult.

More details as per ChatGPT:

the changes to Section 174 were not introduced as a standalone proposal, but rather as part of the broader Tax Cuts and Jobs Act (TCJA) of 2017.

üèõÔ∏è Who introduced the TCJA?

The Tax Cuts and Jobs Act was:
	
‚Ä¢	Introduced by Republicans in the House of Representatives.
	
‚Ä¢	Sponsored by Rep. Kevin Brady (R-TX), who was Chair of the House Ways and Means Committee at the time.
	
‚Ä¢	Backed by the Trump administration as a major tax reform effort.

üìú What Section 13206 of the TCJA said:

This specific section of the bill modified Section 174 of the Internal Revenue Code. It included the clause that:

‚ÄúSpecified research or experimental expenditures shall be capitalized and amortized ratably over a 5-year period‚Ä¶‚Äù

It also explicitly eliminated immediate expensing of R&D and software development costs ‚Äî which is what‚Äôs causing the issue now.

‚∏ª

ü§î Why did they do it?
	
‚Ä¢	It was a revenue-raising provision ‚Äî it delayed tax deductions, which made the TCJA ‚Äúlook‚Äù cheaper on paper over the 10-year budget window.
	
‚Ä¢	It was deliberately delayed to 2022 to avoid hurting the short-term economy and to push the political fallout into the future.",1,
1454,comment,cscareerquestions,n5cbd8x,1753570191.0,anyone have any stats for the Machine Learning / Artificial Intelligence  field? I feel the top minds there are all first generation immigrants at least judging by that Zuck all star AI team or whatever.,3,
1455,comment,cscareerquestions,n5njdo8,1753727206.0,"Okay and what has he actually done to make that happen beyond some fake promises? 

Trump took donations from all the big tech CEOs. They own him. This is how American government works my friend and Trump does it too. Big tech will hire whoever they want and they will buy our politicians if they have to so they can keep doing it. 

Artificial Intelligence might be some sort of exception due to the potential implications for national security. But that‚Äôs a small portion of the hiring for the entire tech industry. 

I would love to eat crow on this if I‚Äôm wrong. But I‚Äôll believe it when it happens, and it‚Äôs more than just talk.",1,
1456,comment,cscareerquestions,ihzwxdq,1659020083.0,"Artificial intelligence for HR software does more harm than good.

There are too many variables and most times your best effort is to mimic the bias of the recruiters. You look at the past and see who has been hired and try to understand what are the relevant characteristics for that company.

Example of a complexity problem: someone who has experience in marketing but it‚Äôs migrating to software and doing a bunch of courses. AI reads their resume and deems it not a good fit for a software position (even if junior). It doesn‚Äôt matter if you had a brilliant career in marketing with lots of achievements. It makes people stuck with their past experiences. There are many other complexities hard to automate and it‚Äôs mostly unfair.",58,
1457,comment,cscareerquestions,ii10n1k,1659035362.0,"Allmost all commercial products and services are flawed and contain bugs, false certificates, and were rushed to ship.

Integration testing is a nightmare in most companies.

Documentation is utterly lacking in many companies.

99% of data science and 'data analysis ' is some kid scripting in Excel.

99% of 'AI' startups is hiring cheap labor to pick patterns

Many companies have a critical piece of software that no one knows how to maintain and they dare not touch.",4,
1458,comment,cscareerquestions,lr6n7z6,1728522663.0,"Well said.  Has always been the case.  Miserable days starting out.

That said though - if I were hiring today, I'd be looking for just a humble junior with lots of energy.  AI tools can more than cover 80% of the skill gap - they'd be entirely capable of doing far more work than I ever have just by facilitating the tools and keeping a curious mind.  Though this is only possible currently because I'm a senior who could bail them out if they get stuck.  I don't expect that situation to last for long though - if nothing else, calling up a senior for spot consulting, but more likely the AI simply catches up for most practical situations.

JUNIORS:  bad news: the market is fucked, but it always has been.  good news?: with AI tools you're at least 10x more powerful and productive than what we used to be.  Go build shit.

Actually: I'll do you one better.  Juniors, if you have any tech situations in need of senior advice, PM me and I'll give it a lowkey take.  Will remove this message if I get overwhelmed though lol.  No economic/business connection hookup advice tho - I aint got that either.",3,
1459,comment,cscareerquestions,lr7dnej,1728533851.0,"I dont think this is the case. In the state of LLM  or other code generators, in order to use it to solve a problem or a solution, you must first intimately know the problem and effectively what solution you want implemented and why. Without this  knowledge thinking a junior could use chatgpt to implement a solution in our codebase is almost laughable. Too many complexities they dont understand, they dont know how to frame the problem and sure dont know why they choose one solution over the other.",2,
1460,comment,cscareerquestions,lr6qiyu,1728523947.0,"That‚Äôs kind of a hallmark of coding bootcamps, they teach you popular coding paradigms but rarely go into enough detail that they can effectively debug. Then charge state tuition for barely enough to even be junior.",45,
1461,comment,cscareerquestions,lr628m8,1728514769.0,"A big factor is they‚Äôre insecure about their own skills. The less supply of candidates, the easier it is for them to keep their job. These are the same people that think ChatGPT is going to replace software engineers by the way.

Not to mention, if it‚Äôs taking juniors over a year to ramp up then their hiring process is broken and they‚Äôre picking terrible candidates, or they have 0 idea how to mentor, teach, and give meaningful work that‚Äôs digestible. Not everyone who graduates with a CS degree is cut out to be a software engineer and that‚Äôs okay. This job is not easy despite what people will tell you, but if someone is making it through your hiring pipeline and they take a year to come online, that‚Äôs a you problem.",15,
1462,comment,cscareerquestions,lr5pjtl,1728510199.0,"What is less work and more fun for a senior engineer?¬†

¬†- writing detailed prompts, providing context, and uploading other docs sufficient for a LLM to implement; reviewing generated code with well formatted feedback to further train the LLM, etc. never gets to write code anymore¬†

¬†- crafting tickets for a junior engineer to do it, and filling in the most interesting bits of the code while the junior does the drudgery?

¬†Which is cheaper for the company?",18,
